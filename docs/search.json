[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Introduction to Statistics and Data Science",
    "section": "",
    "text": "Welcome to the website for Middlebury College’s Fall 2024 STAT 201. On this website you will find the course syllabus, schedule, and assignments. The website is frequently updated during the semester, so please make a habit of refreshing the page. The icons at the top right will link to the supplemental textbook and Canvas."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#course-details",
    "href": "index.html#course-details",
    "title": "Advanced Introduction to Statistics and Data Science",
    "section": "Course details",
    "text": "Course details\nInstructor: Prof. Becky Tang (she/her)\n\nOffice: WNS 214\nEmail: btang@middlebury.edu\n\nMeeting times and location:\n\nSection AZ: MW 8:15-9:30am in Warner 100, R 8:15-9:30am in Warner 104\nSection BY: MW 9:45-11:00am in Warner 100, R 9:45-11:00am in Warner 101\n\nOffice hours (both sections welcome):\n\nProf. Tang: Monday 2-4pm, Friday 9-10am\n\nTA hours in Quantitative Center in Armstrong Library:\n\nTA Claire Yang: Sunday 7-9, Thursday 7-9\nTA Justin Corke: Thursday 7-9\n\nThe syllabus (most recent update: 9/8/24) outlines our course policies and a rough schedule. Once classes begin, you should follow the schedule and due dates found on this website.\n\nNote the textbooks are optional in this class. However, several practice and homework problems are pulled from the OpenIntro textbook.\n\n\n\nggplot2 cheat sheet"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Jump to:\n\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\nWeek 7\nWeek 8\nWeek 9\nWeek 10\nWeek 11\nWeek 12\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nIn-class material\nCoding practice\nAssignments\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\nM 09/09\n\nTopic: Welcome! (pdf here)\n\nDirections for printing at bottom\n\n\n\n\nBring your completed quiz to next class\nFill out this survey by next class.\n\n\n\n\nW 09/11\n\nTopic: Study design (pdf here)\n\nPractice problems\n\n\n\n\nWednesday problems on Problem Set 1\n\n\n\n\nR 09/12\n\nTopic: Software installation (no need to print slides!)\nR and .Rmd basics\n\nDownload the following .Rmd template and drag to your new STAT 201 folder:  Rmd template \nRecording here! Password: 3g^SS69r\n\n\n\nIntro to R and R Markdown\n Coding practice \n\n\nThursday problems on Problem Set 1\n\n\n\n\n\n\n\nIn-class material\nCoding practice\nAssignments\n\n\n2\nM 09/16\n\nTopic: Numerical data (part 1) (pdf here)\n\nLive code and associated template:  Rmd \nPractice problems\n\n\n\n\nMonday problems on Problem Set 2\nSunflower field\n\n\n\n\nW 09/18\n\nTopic: Numerical data (part 2) (pdf here)\n\nLive code\nPractice problems\n\n\n\nBasic plots and summary statistics in R\n Coding practice \n\n\nWednesday problems on Problem Set 2\n\n\n\n\nR 09/19\n\nTopic: ggplot for numerical variables (pdf here)\n\nLive code\n\n\n\nIntroduction to ggplot\n Coding practice \n\n\nThursday problems on Problem Set 2\nAssociated R problems\n Problem Set 2 \n\n\n\n\n\n\n\nIn-class material\nCoding practice\nAssignments\n\n\n3\nM 09/23\n\nTopic: categorical data (pdf here)\n\n\nVisualizing categorical data\n Coding practice \n\n\nMonday problems on Problem Set 3\n\n\n\n\nW 09/25\n\nTopic: Introduction to data wrangling (pdf here)\n\nLive code example\n\n\n\nPractice with wrangling\n Coding practice \n\nBegin working through this R lab:\n Problem Set 3 \n\n\n\nR 09/26\n\nTopic: Data wrangling (cont.)\n\nLive code example\n\nGroup exercise\n .Rmd \n\n\nContinue working on the above lab.\n\n\n\n\n\n\nIn-class material\nCoding practice\nAssignments\n\n\n4\nM 09/30\nTopic: Probability basics (pdf here)\n\nIntroduce Midterm 1\nPractice problems\n\n\n\nMonday problems on Problem Set 4\n\nNote: this week’s problem set is due Saturday 10/05 midnight!\n\n\n\nW 10/02\nTopic: Conditional probability (pdf here)\n\nPractice problems\n\n\n\nWednesday problems on Problem Set 4\n\nNote: this week’s problem set is due Saturday 10/05 midnight!\n\n\n\nR 10/03\nTopic: Simpson’s paradox (pdf here)\n\nPractice with wrangling: probabilities\n Coding practice \n\n\n\n\n\n\n\n\nIn-class material\nCoding practice\nAssignments\n\n\n5\nM 10/07\nTopic: Introduction to Bootstrap (pdf here)\n\nLive code\n\n\nNo problem set this week. Study for midterm!\n\n\n\nW 10/09\nTopic: Bootstrap confidence intervals (pdf here)\n\nPlease complete this mid-semester survey\n\n\nBootstrap CIs (due Monday midnight)\n Coding practice \n\n\n\n\n\nR 10/10\nMidterm 1!\n\n\n\n\n\n\n\n\nIn-class material\nCoding practice\nAssignments\n\n\n6\nM 10/14\n\nTopic: Introduction to Hypothesis Testing (HT) with simulation (pdf here)\n\nPractice problems\n\n\n\nBootstrap CIs from last Thursday due tonight\n\n\nMonday problems in Problem Set 5\n\n\n\n\nW 10/16\n\nTopic: HTs with randomization (pdf here)\n\nLive code for HT for one proportion\nLive code for HT for difference in proportions\nPractice problems\n\n\n\n\nGet started on problems 1-3 in the .Rmd problems.\n Problem Set 5 \n\n\n\n\nR 10/18\n\nTopic: HTs with bootstrapping (pdf here)\n\nPractice problems (CI and HT concepts)\n\n\n\n\nContinue working on Problem Set 5\n\n\n\n\n\n\n\nIn-class material\nCoding practice\nAssignments\n\n\n7\nM 10/21\nTopic: Normal distribution (worksheet)\n\nPractice problems (some in worksheet)\n\n\n\nMonday problems in Problem Set 6\n\n\n\n\nW 10/23\nTopic: Central Limit Theorem + CIs with mathematical models (pdf here)\n\nPractice problems\n\n\n\nWednesday problems in Problem Set 6\n\n\n\n\nR 10/24\nTopic: CIs for means using CLT (pdf here)\n\nLive code\n\n\n\nThursday problems in Problem Set 6\n Problem Set 6 \n\n\n\n\n\n\n\nIn-class material\nCoding practice\nAssignments\n\n\n8\nM 10/28\nTopic: Introduce final project, HTs with mathematical models (pdf here)\n\nPractice problems\n\n\n\nMonday problems in Problem Set 7\nFinal project groups!\n\n\n\n\nW 10/30\nTopic: HTs (cont.) (diff. in proportions and single mean) (pdf here)\n\nPractice problems\n\n\n\nWednesday problems in Problem Set 7\n\n\n\nSo spooky!\nR 10/31\nTopic: HTs (cont.) (paired difference, diff. in means) (pdf here)\n\nSection A: live code\nSection B: live code\nProject work day\n\n\n\nWork on proposal for data collection (due Monday 11/4 midnight)\nThursday problems in Problem Set 7\n\n\n\n\n\n\n\nIn-class material\nCoding practice\nAssignments\n\n\n9\nM 11/4\n\nReview of inference with CLT\n\nPractice problems\n\nLive code for pivoting\n\n\nPivoting coding practice (due tonight midnight)\n Coding practice \n\n\nProposal for data collection due tonight!\nProblem set 7 due Wednesday\n\n\n\nDessert social 3-4:30pm in WNS 101\nW 11/6\n\nTopic: Introduction to simple linear regression (pdf here)\n\nPDF with figures\n\n\n\n\nWednesday problems in Problem Set 8 (due next Thursday)\n\n\n\n\nR 11/7\n\nTopic: SLR interpretation (pdf here)\n\nPractice problems\n\n\n\n\nThursday problems in Problem Set 8 (due next Thursday)\n\n\n\n\n\n\n\nIn-class material\nCoding practice\nAssignments\n\n\n10\nM 11/11\n\nTopic: Inference in SLR (pdf here)\nFitting SLR in R\n\nLive code for coding SLR in R\n\nIntroduce Midterm 2\n\n\n\nMonday problems in Problem Set 8 (due Thursday)\nNote: questions 4-7 in the .Rmd will probably have to wait until Wednesday class. Sorry!\n Problem Set 8 \n\n\n\n\nW 11/13\n\nTopic: Simulation-based CIs for SLR (pdf here)\n\nLive code\n\nLive code for pretty output using broom and factors\n\n\nFitting SLR coding practice\n Coding practice \n\n\nHomework 8 due tomorrow!\n\n\n\n\nR 11/14\n\nTopic: Simulation-based HTs for SLR (pdf here)\n Template \n\nLive code\n\n\n\n\nHomework 8 due tonight\nStart studying for Midterm 2\n\n\n\n\n\n\n\nIn-class material\nCoding practice\nAssignments\n\n\n11\nM 11/18\n\nTopic: Introduction to MLR (pdf here)\n\n\nStudy for midterm\n\n\n\nW 11/20\nMidterm 2: Part 1\n\n\n\n\n\nR 11/21\nMidterm 2: Part 2\n\n\n\n\n\n\n\nThanksgiving Break!\n\n\n\n\n\n\n\n\n\nIn-class material\nCoding practice\nAssignments\n\n\n12\nM 12/02\nProject work day!\n.Rmd template for final project:\n Template \n\nWork on final project!\n\nRead through the project description page\n\n\n\n\nW 12/04\nProject work day!\n\n\nWork on rough draft\nMidterm revisions\n\n\n\n\nR 12/05\nProject work day!\n\n\nRough draft of two sections of due Sunday 11:59pm\n\n\n\n13\nW 12/11\nSection AZ: Project presentations 7-10pm\n\nSlides due by 6pm\n\n\n\nR 12/12\nSection BY: Project presentations 9am-12pm\n\nSlides due by 8am\n\n\n\nSu 12/15\nReport, data + dictionary, and reflection due at 11:59pm\n\n\n\n\n\nTo print PDF of the slides with multiple slides on a page:\n\nDownload the PDF and open the document\nPrint the document\n\nIn the Print options, go to Layout then choose your preferred number of Pages per Sheet\nHit Print"
  },
  {
    "objectID": "slides/slides-01-study-design.html#population-and-samples",
    "href": "slides/slides-01-study-design.html#population-and-samples",
    "title": "Study design",
    "section": "Population and samples",
    "text": "Population and samples\n\nData do not come from thin air! Data have to be collected in some way.\nThis usually takes the form of sampling a subset of individuals from a target group of interest\n\nThe target group of interest is called the population\nThe subset of individuals from whom we actually collect data is the sample\n\nA case is a fancy term for saying one observational unit\n\nWhat are the target populations in the following research questions? What would an individual case/unit be?\n\nWhat is the average height of trees on Middlebury College campus?\nWhat proportion of current Middlebury professors attended a liberal arts college?\nOver the last five years, what is the average time to complete a degree for Middlebury College students?"
  },
  {
    "objectID": "slides/slides-01-study-design.html#example",
    "href": "slides/slides-01-study-design.html#example",
    "title": "Study design",
    "section": "Example",
    "text": "Example\nThe U.S. Census Bureau is responsible for producing data about the American people and economy.\n\nDecennial census\nAmerican community survey"
  },
  {
    "objectID": "slides/slides-01-study-design.html#parameters-and-statistics",
    "href": "slides/slides-01-study-design.html#parameters-and-statistics",
    "title": "Study design",
    "section": "Parameters and statistics",
    "text": "Parameters and statistics\n\nOften times, answering the research question simplifies to understanding a numerical summary.\n\nA numerical summary calculated from (or considered for calculation from) the entire population is called a (population) parameter\nIn contrast, a numerical summary calculated from the sample is called a (sample) statistic\n\nWhy do we differentiate? It’s always good to remember that we are trying to answer questions about the population!"
  },
  {
    "objectID": "slides/slides-01-study-design.html#population-and-samples-cont.",
    "href": "slides/slides-01-study-design.html#population-and-samples-cont.",
    "title": "Study design",
    "section": "Population and samples (cont.)",
    "text": "Population and samples (cont.)\n\nTypically, the size of the sample is way smaller than the population. Why?\n\nIn the lucky event that we are able to collect data for every individual in the population, the sample is referred to as a census\n\nExample: the U.S. Census Bureau is responsible for producing data about the American people and economy. They collect data with different schemes and frequency:\n\nDecennial census\nAmerican community survey"
  },
  {
    "objectID": "slides/slides-01-study-design.html#a-good-sample",
    "href": "slides/slides-01-study-design.html#a-good-sample",
    "title": "Study design",
    "section": "A “good” sample",
    "text": "A “good” sample\n\nThe way we sample data from a population can directly influence the quality of that sample.\n\nWhat are desirable characteristics of a sample?\n\n\nRepresentative: the sample roughly “looks like” the population, i.e. the individuals in the sample offer a good representation of the population at large\nGeneralizable: any results based on the sample can generalize to the population, i.e. we can make “good guesses” about a population parameter using a sample statistic\nUnbiased: every individual in the sample had an equal chance of being sampled"
  },
  {
    "objectID": "slides/slides-01-study-design.html#bias-in-a-sample",
    "href": "slides/slides-01-study-design.html#bias-in-a-sample",
    "title": "Study design",
    "section": "Bias in a sample",
    "text": "Bias in a sample\nBias in a sample can arise due to many causes. Here are a few:\n\nSelection bias\n\nOften arises when convenience sample is taken\n\nExclusion/undercoverage bias\nNon-response bias\nResponse bias\n\n\n\nSelection bias: systematic tendency in procedure that causes some members of population to be more likely to be included than others\nUndercoverage bias: part of population is excluded from sample -> not representative\nNon-response bias: bias introduced when people who don’t respond differ for some reason that affects the target quantity of interest\nResponse bias: systematic favoring of certain outcomes that occurs when people don’t answer truthfully (lying or not being truthful)"
  },
  {
    "objectID": "slides/slides-01-study-design.html#simple-random-sampling",
    "href": "slides/slides-01-study-design.html#simple-random-sampling",
    "title": "Study design",
    "section": "Simple random sampling",
    "text": "Simple random sampling\n\nThe most intuitive and basic form of random sampling!\nEach individual is chosen entirely by chance from the population, each member of the population has an equal chance of being sampled.\n\nKnowing that an individual was sampled does not provide useful information about which other cases are included\nAny given fixed-size subset of the population is equally likely to be chosen.\n\n\nConsider again the research question: What proportion of current Middlebury professors attended a liberal arts college?\nHow might I obtain a sample random sample of 25 professors?"
  },
  {
    "objectID": "slides/slides-01-study-design.html#stratified-sampling",
    "href": "slides/slides-01-study-design.html#stratified-sampling",
    "title": "Study design",
    "section": "Stratified sampling",
    "text": "Stratified sampling\n\nDepending on the population, we might need to use different random sampling methods to ensure the sample is representative.\nAssume that the population is/can be broken up into several different, distinct sub-populations or strata\n\nThe division should “make sense”\n\nRather than randomly sampling from the population as whole, we take a random sample from each stratum\n\nHow many from each stratum? Typically use a sampling fraction that is proportional to entire population!\nE.g. if population of trees on Middlebury campus are 80% deciduous and 20% coniferous and we want to sample \\(n = 10\\) trees total, we should randomly sample ___ deciduous and ___ coniferous trees\n\nWhat are some pros/cons?\n\n\n\nMay be useful to ensure we have samples from rare groups, and helps to improve precision of the sample\nMost useful when stratifying variables are simple to work with, easy to observe, closely related to topic of survey\nCan be expensive to travel –> motivates the next type"
  },
  {
    "objectID": "slides/slides-01-study-design.html#cluster-sampling",
    "href": "slides/slides-01-study-design.html#cluster-sampling",
    "title": "Study design",
    "section": "Cluster sampling",
    "text": "Cluster sampling\n\nDivide total population into \\(M\\) distinct groups or clusters of roughly equal size\nPerform a simple random sample on the \\(M\\) clusters, than sample all individuals within each of the randomly selected clusters\n\nDiscuss the following:\n\nWould you prefer the individuals within a cluster to be homogeneous (similar) or heterogeneous (varied)? Why?\nWould you prefer that cluster A and cluster B be relatively similar or different in terms of their sub-populations?\nWhat is the difference between stratified and cluster sampling?\n\n\n\n\n\nCreates pockets of sampled units –> reducing costs. Also useful when we cannot obtain a list of ALL units in the population, but can easily obtain a list of all units within a cluster\nLess efficient that SRS –> better to survey large number of small clusters than vice versa\n\nNeighboring units tend to be more similar –> sampling one being cluster would not represent a whole spectrum of variation present in the entire population\n\nDifficult to control final sample size. If you want a particular size n (e.g. 100), the other two methods are easier"
  },
  {
    "objectID": "slides/slides-01-study-design.html#multistage-sampling",
    "href": "slides/slides-01-study-design.html#multistage-sampling",
    "title": "Study design",
    "section": "Multistage sampling",
    "text": "Multistage sampling\n\nBuilds on the cluster sampling method, but rather than sampling all individuals within the selected clusters, only collect a random sample within each selected cluster\nThough seemingly more complicated, why might we prefer multistage sampling over cluster sampling?"
  },
  {
    "objectID": "slides/slides-01-study-design.html#experimental-design",
    "href": "slides/slides-01-study-design.html#experimental-design",
    "title": "Study design",
    "section": "Experimental design",
    "text": "Experimental design\n\nExperiments are studies where the researcher assigns treatments to cases\n\nNote: experiments are often conducted in medical settings, hence the word “treatment”\n\nAre treatments considered explanatory or response variables?\n\n\nWhen the researcher randomly assigns the treatments, we have a randomized experiment\n\nRandomized experiments are critical when trying to assess the causal effect of the explanatory variable on the response variable\nNote: random assignment \\(\\ne\\) random sampling\n\n\n\nRunning example: quizzes and final exam. Want to know if having quizzes throughout semester boosts performance on final exam. Randomly assign half of class to do quizzes, and the other half to not. Then compare their final exam scores."
  },
  {
    "objectID": "slides/slides-01-study-design.html#confounding-variables",
    "href": "slides/slides-01-study-design.html#confounding-variables",
    "title": "Study design",
    "section": "Confounding variables",
    "text": "Confounding variables\n\nUnderstanding a causal relationship is made difficult by confounding variables: variables that are associated with both the explanatory and response variable of interest\n\n\nConfounders are bad!! Why?\n\n\nExample: consider a study that seeks to examine the effect of coffee consumption on heart disease.\n\nFrom each person, we only collect information on the average amount of coffee they consume per day and whether or not they have heart disease.\nWe find a positive association: more coffee \\(\\rightarrow\\) higher risk of heart disease\nPossible confounder: smoker status. Smokers tend to drink more coffee and tend to have higher rates of heart disease than non-smokers.\nSo the increase in heart disease may be due to smoker status rather than caffeine intake\n\n\n\nConfounders prevent us from attributing the cause to the (desired) explanatory variable!"
  },
  {
    "objectID": "slides/slides-01-study-design.html#principles-of-experimental-design",
    "href": "slides/slides-01-study-design.html#principles-of-experimental-design",
    "title": "Study design",
    "section": "Principles of experimental design",
    "text": "Principles of experimental design\n\nRandomization: randomly assign patients to treatments\n\nHelps account for variables that cannot be controlled and possible confounding variables\n\nControlling for differences in the treatment: ensure that everyone follows the same protocol exactly\nReplication: the more cases we observe, the more confidence we have in the effect of the explanatory on the response\n\nAchieved by collecting a sufficiently large sample in a single study, or repeating the entire study more than once\n\n\n\nThese first three principles are crucial! The following is principle is desirable, but more complicated."
  },
  {
    "objectID": "slides/slides-01-study-design.html#principles-of-experimental-design-cont.",
    "href": "slides/slides-01-study-design.html#principles-of-experimental-design-cont.",
    "title": "Study design",
    "section": "Principles of experimental design (cont.)",
    "text": "Principles of experimental design (cont.)\n\nBlocking: suppose we know ahead of time that there is/are variable(s) that could influence the response besides just the explanatory. We assign patients to their respective blocks, and than randomly assign treatments within blocks.\n\nHelps to decrease unexplained variability by accounting for nuisance variables: variables that affect the response variable but are not of interest for answering the scientific question\nCan lead to greater interpretation of results"
  },
  {
    "objectID": "slides/slides-01-study-design.html#reducing-bias-in-human-experiments",
    "href": "slides/slides-01-study-design.html#reducing-bias-in-human-experiments",
    "title": "Study design",
    "section": "Reducing bias in human experiments",
    "text": "Reducing bias in human experiments\n\nWe should make the experiment a blind experiment by not allowing participants to know which group they’ve been assigned to\n\nGive a fake treatment known as a placebo to those in the control group (e.g. a sugar pill that looks exactly like the actual treatment pill)\nOftentimes, a placebo results in a slight but real improvement in patients. This is known as the placebo effect\n\nDoctors and researchers involved in the study should also be blinded so they do not give preferential treatment or care to patients in certain groups.\n\nDouble-blind experiments: both the patients and the doctors/researchers who interact with patients are unaware of who is or is not receiving the treatment\n\nQuestion of ethics"
  },
  {
    "objectID": "slides/slides-01-study-design.html#treatment-vs.-control",
    "href": "slides/slides-01-study-design.html#treatment-vs.-control",
    "title": "Study design",
    "section": "Treatment vs. control",
    "text": "Treatment vs. control\n\nRandomized experiments are the gold standard for data collection, but biases can still occur!\nWhen we want to learn if the explanatory variable causes some effect in the response variable.\n\nWe have a control group which establishes a baseline, and typically receives “zero amount” of the explanatory variable.\nWe also have a treatment group which receives some “non-zero amount” of the explanatory variable\n\nExample: suppose we want to test the effect of a drug that is developed to help people fall asleep.\n\nTreatment group: receives 50mg of the drug in pill form\nControl group: does not receive the drug at all\n\nWhat is a potential issue?"
  },
  {
    "objectID": "slides/slides-01-study-design.html#observational-studies-1",
    "href": "slides/slides-01-study-design.html#observational-studies-1",
    "title": "Study design",
    "section": "Observational studies",
    "text": "Observational studies\n\nStudies where no treatment is explictly applied\nNothing is manipulated; researchers simply record/observe without intervening\nTypically cannot obtain causal conclusions using data from observational studies\n\nThere are too many confounding variables at play in observational studies\n\nBut we can use these studies to identify associations or form hypotheses for future experiments!"
  },
  {
    "objectID": "slides/slides-01-study-design.html#observational-studies",
    "href": "slides/slides-01-study-design.html#observational-studies",
    "title": "Study design",
    "section": "Observational studies",
    "text": "Observational studies\n\nStudies where no treatment is explicitly applied\nNothing is manipulated; researchers simply record/observe without intervening\nTypically cannot obtain causal conclusions using data from observational studies\n\nThere are too many confounding variables at play in observational studies\n\nBut we can use these studies to identify associations or form hypotheses for future experiments!\n\n\nQuiz/exam example could be an observational study if we gave students the choice to do an optional quiz."
  },
  {
    "objectID": "practice/practice-01-study-design.html",
    "href": "practice/practice-01-study-design.html",
    "title": "Study design",
    "section": "",
    "text": "Please work on the practice problems in your group. At least one of the following problems will be assigned to the weekly problem set. Unless otherwise stated, problems come from the IMS textbook.\n\nTime to implement the sampling methods we learned! Keep track of your work on this problem; we will return to it in the next few classes.\nWe have a farmer who grows sunflowers for making sunflower oil. Her field is arranged in a grid pattern, with 12 rows and 12 columns as shown below. Water is important for crops, so irrigation ditches have been installed along the top and bottom of the field. It is expected that plants closer to a water source will perform better than those further away from water.\nThe farmer would like to estimate the number of healthy plants in the field, along with a few other characteristics about the sunflowers. It would be unfeasible to conduct a census, so we should choose to sample a subset of the grid cells. Suppose we’d like to sample \\(n = 12\\) grid cells total.\n\nUsing words (sentences or bulleted list are fine) and perhaps labeling the figure below, describe exactly how you would obtain a sample of 12 grid cells using simple random sampling. I should be able to read your work and know what to do without any questions! Think about how you will perform the random sampling.\nThen implement the method that you’ve written down, and either using the figure below or drawing your own 12x12 field, shade in the squares that correspond to your sample.\n\nUsing words (sentences or bulleted list are fine) and perhaps labeling the figure below, describe exactly how you would obtain a sample of 12 grid cells using stratified sampling where the strata are rows. I should be able to read your work and know what to do without any questions!\nThen implement the method that you’ve written down, and either using the figure below or drawing your own 12x12 field, shade in the squares that correspond to your sample.\n\nUsing words (sentences or bulleted list are fine) and perhaps labeling the figure below, describe exactly how you would obtain a sample of 12 grid cells using stratified sampling where the strata are columns. I should be able to read your work and know what to do without any questions!\nThen implement the method that you’ve written down, and either using the figure below or drawing your own 12x12 field, shade in the squares that correspond to your sample.\n\nUsing words (sentences or bulleted list are fine) and perhaps labeling the figure below, describe exactly how you would obtain a sample of 12 grid cells using cluster sampling where we have 24 clusters total. I should be able to read your work and know what to do without any questions!\nThen implement the method that you’ve written down, and either using the figure below or drawing your own 12x12 field, shade in the squares that correspond to your sample.\n\nUsing words (sentences or bulleted list are fine) and perhaps labeling the figure below, describe exactly how you would obtain a sample of 12 grid cells using multistage sampling. I should be able to read your work and know what to do without any questions!\nThen implement the method that you’ve written down, and either using the figure below or drawing your own 12x12 field, shade in the squares that correspond to your sample.\n\n\nWe discussed several methods of sampling today. Now consider a new sampling scheme that is commonly used in the social sciences when sampling from a “hidden” population. A scenario might be that we want to conduct a poll among homeless people, but the research team may only have contact details for a few homeless people.\nDenote the initial contacts as Group 1. The sampling proceeds in the following stages:\n\nStage 1: ask the people in Group 1 to take the survey, and then provide contact details for other people from the population who might want to participate. Denote these new contacts as Group 2.\nStage 2: ask the people in Group 2 to take the survey, and then provide contact details for other people from the population who might want to participate. Denote these new contacts as Group 3.\nProceed in a similar manner until the researchers have sufficient data.\n\n\n\nWhat are some advantages to this method of sampling?\nWhat are some disadvantages? Think about statistically and also ethically.\n\n(2.5.20) To assess the effectiveness of taking large doses of vitamin C in reducing the duration of the common cold, researchers recruited 400 healthy volunteers from staff and students at a university. A quarter of the patients were assigned a placebo, and the rest were evenly divided between 1g Vitamin C, 3g Vitamin C, or 3g Vitamin C plus additives to be taken at onset of a cold for the following two days. All tablets had identical appearance and packaging. The nurses who handed the prescribed pills to the patients knew which patient received which treatment, but the researchers assessing the patients when they were sick did not. No statistically discernible differences were observed in any measure of cold duration or severity between the four groups, and the placebo group had the shortest duration of symptoms.\n\nWas this an experiment or an observational study? Why?\nWhat are the explanatory and response variables in this study?\nWere the patients blinded to their treatment?\nWas this study double-blind?\nParticipants are ultimately able to choose whether to use the pills prescribed to them. We might expect that not all of them will adhere and take their pills. Does this introduce a confounding variable to the study? Explain your reasoning."
  },
  {
    "objectID": "slides/slides-03-numerical-vis.html#data",
    "href": "slides/slides-03-numerical-vis.html#data",
    "title": "Numerical data",
    "section": "Data",
    "text": "Data\nWhich of the following variables are numerical?"
  },
  {
    "objectID": "slides/slides-03-numerical-vis.html#scatterplots",
    "href": "slides/slides-03-numerical-vis.html#scatterplots",
    "title": "Numerical data",
    "section": "Scatterplots",
    "text": "Scatterplots\nScatterplots are bivariate visualizations that prove a case-by-case view of the data for two numerical variables\n\nEach point represents the observed pair of values of variables 1 and 2 for each case in the dataset"
  },
  {
    "objectID": "slides/slides-03-numerical-vis.html#scatterplots-cont.",
    "href": "slides/slides-03-numerical-vis.html#scatterplots-cont.",
    "title": "Numerical data",
    "section": "Scatterplots (cont.)",
    "text": "Scatterplots (cont.)\n\nHow do we determine which variable to put on each axis?\nWhat do scatterplots reveal about the data, and how are they useful?\nHow might we improve on this visualization?"
  },
  {
    "objectID": "slides/slides-03-numerical-vis.html#dot-plots",
    "href": "slides/slides-03-numerical-vis.html#dot-plots",
    "title": "Numerical data",
    "section": "Dot plots",
    "text": "Dot plots\n\nDot plots are a basic visualization that show the distribution of a single variable. They are univariate (one-variable) scatterplots.\nIn the following, we have a dot plot of BMI rounded to the nearest integer."
  },
  {
    "objectID": "slides/slides-03-numerical-vis.html#binning",
    "href": "slides/slides-03-numerical-vis.html#binning",
    "title": "Numerical data",
    "section": "Binning",
    "text": "Binning\n\nDot plot display the exact value for each observation. Become hard to read when the variable of interest has a wide set of values\nWe will sacrifice a bit of precision for convenience by binning: we will consider segmenting the variable into equal-sized bins and visualize the value of each observation using its corresponding bin\nFor example, the bmi variable has observed values of \\(15.96\\) through \\(49.6\\). Consider the following bins of size 5:\n\n[14, 18), [18, 22), [22, 26), …, [48, 52)\n\nWe tabulate/count up the number of observations that fall into each bin.\n\n\n\n\n# A tibble: 8 × 2\n  bmi_bin  count\n  <chr>    <int>\n1 [15, 19)     5\n2 [19, 23)    12\n3 [23, 27)    35\n4 [27, 31)    58\n5 [31, 35)    41\n6 [35, 39)    35\n7 [39, 43)    13\n8 [49, 52)     1"
  },
  {
    "objectID": "slides/slides-03-numerical-vis.html#histograms",
    "href": "slides/slides-03-numerical-vis.html#histograms",
    "title": "Numerical data",
    "section": "Histograms",
    "text": "Histograms\nHistograms are visualizations that display the binned counts as bars for each bin."
  },
  {
    "objectID": "slides/slides-03-numerical-vis.html#histograms-cont.",
    "href": "slides/slides-03-numerical-vis.html#histograms-cont.",
    "title": "Numerical data",
    "section": "Histograms (cont.)",
    "text": "Histograms (cont.)\n\nHistograms provide a view of the density of the data; the values the data take on as well as how often\nVery helpful for understanding the shape of the data distribution\n\nDistributions are either symmetric or skewed in a one direction\nDistributions with long tails to the left are called left-skewed, whereas distributions with long tails to the right are right-skewed\n\nAlso helpful for identifying modes which are prominent peaks in the distribution\n\nDistribution may be unimodal (one peak), bimodal (two peaks), or multimodal (more than two peaks)\nPeaks need not be same height in histogram"
  },
  {
    "objectID": "slides/slides-03-numerical-vis.html#histograms-cont.-1",
    "href": "slides/slides-03-numerical-vis.html#histograms-cont.-1",
    "title": "Numerical data",
    "section": "Histograms (cont.)",
    "text": "Histograms (cont.)\n\nHow would you describe the shape and modality in the following two histograms?"
  },
  {
    "objectID": "slides/slides-03-numerical-vis.html#mean",
    "href": "slides/slides-03-numerical-vis.html#mean",
    "title": "Numerical data",
    "section": "Mean",
    "text": "Mean\n\nBy far the most common way to measure the center of the distribution of numerical data is using the mean, also called the average\nWe use the term sample mean when referring to the mean of observed/sampled data, which is typically denoted as \\(\\bar{x}\\)\n\n\\(x\\) is a placeholder for the variable of interest (e.g. BMI, charges)\nThe bar communicates that we are looking at the average\n\nThe (sample) mean is the sum over all the values of the variable, divided by total number of observations \\(n\\):\n\n\n\\[\\bar{x} = \\frac{x_{1} + x_{2} + \\ldots x_{n}}{n} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i}\\]"
  },
  {
    "objectID": "slides/slides-03-numerical-vis.html#mean-cont.",
    "href": "slides/slides-03-numerical-vis.html#mean-cont.",
    "title": "Numerical data",
    "section": "Mean (cont.)",
    "text": "Mean (cont.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe sample mean \\(\\bar{x}\\) is an example of a sample statistic\nThe mean over the entire population is an example of a population parameter. The population mean is often denoted \\(\\mu\\) (Greek letter mu)\nThe sample mean \\(\\bar{x}\\) is often used as an estimate for \\(\\mu\\)"
  },
  {
    "objectID": "slides/slides-03-numerical-vis.html#live-code",
    "href": "slides/slides-03-numerical-vis.html#live-code",
    "title": "Numerical data",
    "section": "Live code",
    "text": "Live code\n\nBase R\nggplot"
  },
  {
    "objectID": "slides/slides-03-numerical-vis.html#example",
    "href": "slides/slides-03-numerical-vis.html#example",
    "title": "Numerical data",
    "section": "Example",
    "text": "Example\nSuppose we have obtained the following data on diastolic blood pressure from patients at Porter Hospital:\n\\[\n\\boldsymbol{x} =76, 64, 62, 81, 70, 72, 81, 63, 67, 77\n\\]\n\nWrite out how you would calculate \\(\\bar{x}\\)\nThen we will use R to calculate the sample mean diastolic blood pressure!"
  },
  {
    "objectID": "slides/slides-03-numerical-vis.html#means-depend-on-proportions",
    "href": "slides/slides-03-numerical-vis.html#means-depend-on-proportions",
    "title": "Numerical data",
    "section": "Means depend on proportions",
    "text": "Means depend on proportions\n\n\nWhat is the average of the following values: \\(1, 4, 4\\)?\n\n\n\\(\\bar{x} = \\frac{1+4+4}{3} = 1\\left(\\frac{1}{3} \\right) + 4\\left( \\frac{2}{3}\\right) = \\frac{9}{3} = 3\\)\n\n\nIf instead there were 10 1’s and 20 4’s, would the average be the same?\n\n\nYes! \\(\\bar{x} = 1\\left(\\frac{10}{30}\\right) + 4 \\left(\\frac{20}{30} \\right) = \\frac{90}{30} = 3\\)"
  },
  {
    "objectID": "slides/slides-03-numerical-vis.html#variability",
    "href": "slides/slides-03-numerical-vis.html#variability",
    "title": "Numerical data",
    "section": "Variability",
    "text": "Variability\n\nWe learned that the mean is a way to describe the center or “average value” of a numerical variable\nHowever, at the heart of statistics is also the variability or spread of the distribution of the variable\nWe will work with variance and standard deviation, which describe how spread out data are from their mean"
  },
  {
    "objectID": "slides/slides-03-numerical-vis.html#deviation",
    "href": "slides/slides-03-numerical-vis.html#deviation",
    "title": "Numerical data",
    "section": "Deviation",
    "text": "Deviation\nWe return to the blood pressure data:\n\\[\n\\boldsymbol{x} = 76, 64, 62, 81, 70, 72, 81, 63, 67, 77 \\qquad \\qquad \\qquad \\bar{x} = 71.3\n\\]\n\nWe start with deviation, which is the distance of or difference between an observation from the (sample) mean\n\nHow might we write this using statistical notation?\n\n\n\n\n\n    x deviation\n1  76       4.7\n2  64      -7.3\n3  62      -9.3\n4  81       9.7\n5  70      -1.3\n6  72       0.7\n7  81       9.7\n8  63      -8.3\n9  67      -4.3\n10 77       5.7"
  },
  {
    "objectID": "slides/slides-03-numerical-vis.html#variance-and-standard-deviation",
    "href": "slides/slides-03-numerical-vis.html#variance-and-standard-deviation",
    "title": "Numerical data",
    "section": "Variance and standard deviation",
    "text": "Variance and standard deviation\n\nThe sample variance \\(s^2\\) squares the deviations and takes an average:\n\\[\ns^2 = \\frac{\\sum_{i=1}^{n} (x_{i} - \\bar{x})^2}{n-1}\n\\]\n\nLet’s talk about this notation and intuition behind this formula. In particular, there are at least two things to note\n\nThe sample standard deviation \\(s\\) is simply the square root of the sample variance (\\(s = \\sqrt{s^2}\\))\nCalculate the sample variance and standard deviation for the blood pressure data\n\nBy hand\nUsing R"
  },
  {
    "objectID": "slides/slides-03-numerical-vis.html#variance-and-standard-deviation-cont.",
    "href": "slides/slides-03-numerical-vis.html#variance-and-standard-deviation-cont.",
    "title": "Numerical data",
    "section": "Variance and standard deviation (cont.)",
    "text": "Variance and standard deviation (cont.)\n\nLike the mean, the population values for variance and standard deviation are denoted with Greek letters:\n\n\\(\\sigma\\) for population standard deviation (sigma)\n\\(\\sigma^2\\) for population variance\n\n\nIf the calculation of standard deviation is a more complicated quantity than the variance, why do we bother with standard deviation?"
  },
  {
    "objectID": "practice_probs/practice-03-numerical-data-pt1.html",
    "href": "practice_probs/practice-03-numerical-data-pt1.html",
    "title": "Numerical data (part 1)",
    "section": "",
    "text": "Please work on the practice problems in your group. At least one of the following problems will be assigned to the weekly problem set.\n\nIndicate which of the plots show (a) a positive association, (b) negative association, or (c) no association. Also determine if the positive and negative associations are linear or nonlinear. Each part may refer to more than on plot.\n\nCalculate the average/sample mean in each of the following scenarios:\n\nSuppose that we have some data where 20% of the data are 1’s, 50% are 2’s, and 30% are 3’s. What is the sample mean?\nA school has two classes, one with 10 students and one with 100 students. What is the average class size?\nA school as two classes, one with 10 students and one with 100 students. What is the average size of the class that a student is enrolled in?\n\nA list has 10 entries, and each entry is either a 1, 2, or 3. What must the list be if the average is 3? Can the average be 4?\n21 people in a room have an average of 5 feet 6 inches. A 22nd person enters the room. How tall would this person have to be to raise the average height by 1 inch?\nConsider the following lists:\n\\[(i) \\ 1, \\ 3,\\ 4, \\ 5,\\  7 \\qquad \\qquad (ii) \\ 6,\\ 8,\\ 9,\\ 10,\\ 12\\]\n\nFor each list, find the average and the standard deviation.\nHow is list (ii) related to list (i)? How does this relationship carry over to the average and to the standard deviation?\n\nWorkers at a particular mining site receive an average of 35 days paid vacation, which is lower than the national average. The manager of this plant is under pressure from a local union to increase the amount of paid time off. However, he does not want to give more days off to the workers because that would be costly. Instead he decides he should fire 10 employees in such a way as to raise the average number of days off that are reported by his employees. In order to achieve this goal, should he fire employees who have the most number of days off, least number of days off, or those who have about the average number of days off?\nLet’s return to the sunflowers! Recall that we used five different sampling schemes to obtain a sample of \\(n= 12\\) grid cells. You have now collected data on the number of healthy sunflowers in each of the 12 grid cells for each sampling scheme.\nFor each sampling scheme, calculate i) the average number of healthy sunflowers in a grid cell, as well as ii) the standard deviation of the number of healthy plants in a grid cell.\n\nTo show your work for this problem: it is sufficient to write-out the calculations by hand (using “…” notation is fine!) for just one of the sampling schemes. But be sure to report the statistics for all schemes.\nI highly encourage you to use R to help perform the actual calculations. If you do use R, write-out an example of the code you execute for just one of the sampling schemes."
  },
  {
    "objectID": "practice_probs/practice-01-study-design.html",
    "href": "practice_probs/practice-01-study-design.html",
    "title": "Study design",
    "section": "",
    "text": "Please work on the practice problems in your group. At least one of the following problems will be assigned to the weekly problem set.\n\nIn Spring 2024, Prof. Tang asked her STAT 311 students to fill out a mid-semester survey. She was particularly interested in the the amount of hours her STAT 311 students were spending per week on the course. The average time spent on the course was found to be 10.2 hours per week.\n\nBased on this information, identify each of the following: observation, variable, parameter, and statistic.\nWas this survey a census or (just) a sample? Why?\n\nTime to implement the sampling methods we learned! Keep track of your work on this problem; we will return to it in the next few classes.\nWe have a farmer who grows sunflowers for making sunflower oil. Her field is arranged in a grid pattern, with 12 rows and 12 columns as shown below. Water is important for crops, so irrigation ditches have been installed along the top and bottom of the field. It is expected that plants closer to a water source will perform better than those further away from water.\nThe farmer would like to estimate the number of healthy plants in the field, along with a few other characteristics about the sunflowers. It would be unfeasible to conduct a census, so we should choose to sample a subset of the grid cells. Suppose we’d like to sample \\(n = 12\\) grid cells total.\n\nUsing words (sentences or bulleted list are fine) and perhaps labeling the figure below, describe exactly how you would obtain a sample of 12 grid cells using simple random sampling. I should be able to read your work and know what to do without any questions! Think about how you will perform the random sampling.\nThen implement the method that you’ve written down, and either using the figure below or drawing your own 12x12 field, shade in the squares that correspond to your sample.\n\nUsing words (sentences or bulleted list are fine) and perhaps labeling the figure below, describe exactly how you would obtain a sample of 12 grid cells using stratified sampling where the strata are rows. I should be able to read your work and know what to do without any questions!\nThen implement the method that you’ve written down, and either using the figure below or drawing your own 12x12 field, shade in the squares that correspond to your sample.\n\nUsing words (sentences or bulleted list are fine) and perhaps labeling the figure below, describe exactly how you would obtain a sample of 12 grid cells using stratified sampling where the strata are columns. I should be able to read your work and know what to do without any questions!\nThen implement the method that you’ve written down, and either using the figure below or drawing your own 12x12 field, shade in the squares that correspond to your sample.\n\nUsing words (sentences or bulleted list are fine) and perhaps labeling the figure below, describe exactly how you would obtain a sample of 12 grid cells using cluster sampling where we have 24 clusters total. I should be able to read your work and know what to do without any questions!\nThen implement the method that you’ve written down, and either using the figure below or drawing your own 12x12 field, shade in the squares that correspond to your sample.\n\nUsing words (sentences or bulleted list are fine) and perhaps labeling the figure below, describe exactly how you would obtain a sample of 12 grid cells using multistage sampling. I should be able to read your work and know what to do without any questions!\nThen implement the method that you’ve written down, and either using the figure below or drawing your own 12x12 field, shade in the squares that correspond to your sample.\n\n\nSuppose we want to estimate household size, where a “household” is defined as people living together in the same dwelling, and sharing living accommodations. If we select students at random at an elementary school and ask them what their family size is, will this be a good measure of household size? Or will our average be biased? If so, will it overestimate or underestimate the true value?\nTo assess the effectiveness of taking large doses of vitamin C in reducing the duration of the common cold, researchers recruited 400 healthy volunteers from staff and students at a university. A quarter of the patients were assigned a placebo, and the rest were evenly divided between 1g Vitamin C, 3g Vitamin C, or 3g Vitamin C plus additives to be taken at onset of a cold for the following two days. All tablets had identical appearance and packaging. The nurses who handed the prescribed pills to the patients knew which patient received which treatment, but the researchers assessing the patients when they were sick did not. No statistically discernible differences were observed in any measure of cold duration or severity between the four groups, and the placebo group had the shortest duration of symptoms.\n\nWas this an experiment or an observational study? Why?\nWhat are the explanatory and response variables in this study?\nWere the patients blinded to their treatment?\nWas this study double-blind?\nParticipants are ultimately able to choose whether to use the pills prescribed to them. We might expect that not all of them will adhere and take their pills. Does this introduce a confounding variable to the study? Explain your reasoning.\n\n\n\n\n    a.  What are some advantages to this method of sampling?\n    b.  What are some disadvantages? Think about statistically and also ethically.\n-->"
  },
  {
    "objectID": "coding_practice/coding-practice-03-numerical-pt1.html",
    "href": "coding_practice/coding-practice-03-numerical-pt1.html",
    "title": "Numerical data coding practice",
    "section": "",
    "text": "Change your name in the YAML. Be sure to keep the quotation marks!\nIn the following code chunk, load in the openintro package. Then run the code in the code chunk. We will once again work with the cherry data set.\n\n\n\n\n\nIn the code chunk below, write code to find the mean and median of the diameter of trees in the cherry data frame.\n\n\n\n\n\nMake a box plot of the height of cherry trees. Can you make an informative axis label for your plot? Try changing the color of your boxplot by specifying col = \"color name\" in the function. Note that the name of the color must be in quotes! If you’re confused, look at the examples in the bottom of the Help file of the appropriate function,\n\n\n\n\n\nMake a scatter plot of the diameter and volume of cherry trees. Put volume on the y-axis.\n\n\n\n\nOnce you’re finished, be sure to knit and submit the output file to the corresponding Canvas assignment!"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html#recap",
    "href": "slides/slides-04-numerical-pt2.html#recap",
    "title": "Numerical data",
    "section": "Recap",
    "text": "Recap\nWe learned about the sample mean \\(\\bar{x}\\), the sample variance \\(s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_{i} - \\bar{x})^2\\), and the sample standard deviation \\(s = \\sqrt{s^2}\\)\n\nWhy care about standard deviation (SD)? Describes how far data are distributed from their mean\nUsually (but not always!!) about 70% of the data will be within one SD of the mean, and 95% will be within two SDs\n\nThese percentages are not precise, but are useful for intuition\nWe will come back to this later in semester"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html#boxplot",
    "href": "slides/slides-04-numerical-pt2.html#boxplot",
    "title": "Numerical data",
    "section": "Boxplot",
    "text": "Boxplot\nAnother commonly used visualization to display the distribution of a numerical variable is the boxplot. Boxplots are created using five statistics and identify unusual observations.\n\n\nDoes the orientation (vertical or horizontal) matter?"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html#median",
    "href": "slides/slides-04-numerical-pt2.html#median",
    "title": "Numerical data",
    "section": "Median",
    "text": "Median\n\nThe (sample) median \\(m\\) is another common measure of center of a distribution. It is the value of the data distribution where 50% of the data are less than \\(m\\) and 50% of the data are greater than \\(m\\).\n\nIf we order the data from smallest to largest, the median is the value in the middle.\nIf the number of observations \\(n\\) is even, then there will be two values in the middle, and the median is taken as their average\n\n\nConsider the following data: \\(\\boldsymbol{x} =116, 114, 112, 121, 108, 113, 129, 118, 119, 115\\). What is the median?\n\nThe median is also known the 50th percentile because 50% of the data fall below \\(m\\)"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html#mean-vs.-median",
    "href": "slides/slides-04-numerical-pt2.html#mean-vs.-median",
    "title": "Numerical data",
    "section": "Mean vs. Median",
    "text": "Mean vs. Median\n\nWhich is better measure of center? The mean or the median?"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html#interquartile-range",
    "href": "slides/slides-04-numerical-pt2.html#interquartile-range",
    "title": "Numerical data",
    "section": "Interquartile range",
    "text": "Interquartile range\nThe interquartile range (IQR) is another measure of variability/spread in the data.\n\\[\nIQR = Q_{3} - Q_{1}\n\\]\n\nIf the data are more spread out data, should the IQR increase or decrease?\nWhat is the IQR of the data \\(\\boldsymbol{x}\\)?\n\n\nIQR are not the values themselves, but rather, the spread of the middle 50% of the data. Interpretation once again depends on the scale of the data.\nQ3 = 119 and Q1 = 113 -> IQR = 6"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html#creating-the-boxplot",
    "href": "slides/slides-04-numerical-pt2.html#creating-the-boxplot",
    "title": "Numerical data",
    "section": "Creating the boxplot",
    "text": "Creating the boxplot\n\nThe “box” part of the boxplot is created using \\(Q_{1}\\), \\(m\\), and \\(Q_{3}\\)\nThen we draw out whiskers from the box that attempt to capture data outside the IQR\n\nHow long should the whiskers be? There isn’t a fixed rule, but \\(1.5 \\times IQR\\) below \\(Q_1\\) and above \\(Q_{3}\\) is common\nWe should “cut off” the whiskers to be true to the data\n\nLastly, we add dots for any cases that lie beyond the whiskers\n\nThese points are unusually high/low compared to the rest of the data and are worth identifying as potential outliers\nAn outlier is an observation that appears extreme relative to the rest of the data\n\nLet’s draw a boxplot for the data \\(\\boldsymbol{x}\\)!"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html#a-note-on-outliers",
    "href": "slides/slides-04-numerical-pt2.html#a-note-on-outliers",
    "title": "Numerical data",
    "section": "A note on outliers",
    "text": "A note on outliers\n\n\nWhy are we interested in identifying outliers?\n\n\nIdentifying strong skew\nIdentifying possible data collection/data entry errors\nProviding insight into interesting properties of the data\n\nAre outliers necessarily indicative of a problem in the data?"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html#histograms-vs-boxplots",
    "href": "slides/slides-04-numerical-pt2.html#histograms-vs-boxplots",
    "title": "Numerical data",
    "section": "Histograms vs boxplots",
    "text": "Histograms vs boxplots\n\nWhat characteristics of the distribution are apparent in the histogram and not in the box plot? What characteristics are apparent in the box plot but not in the histogram?"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html#robust-statistics",
    "href": "slides/slides-04-numerical-pt2.html#robust-statistics",
    "title": "Numerical data",
    "section": "Robust statistics",
    "text": "Robust statistics\n\n\n\n\nIn the data \\(\\boldsymbol{x} = 116, 114, 112, 121, 108, 113, 129, 118, 119, 115\\) that we have been working with, we have the following sample statistics:\n\n\n\\(\\qquad \\bar{x} =\\) 116.5, \\(s =\\) 5.8, \\(m =\\) 115.5 , \\(IQR =\\) 6\n\n\n\nSuppose we actually observed an additional data point with a value of \\(170\\). What are the sample statistics with this additional data point? How do they compare to the values above?\n\\(\\bar{x}' =\\) 121.4, \\(s' =\\) 17.03, \\(m' =\\) _____, \\(IQR' =\\) _____\n\nRobust statistics are statistics that are minimally affected by extreme values\n\nWhich of the statistics above would be considered robust?\n\n\nWhen should the mean be similar to the median (and the standard deviation similar to the IQR)?\n\n\n\nm’ = 116\nQ1’ = 113, Q3’ = 121, IQR’ = 8"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html#mean-vs.-median-1",
    "href": "slides/slides-04-numerical-pt2.html#mean-vs.-median-1",
    "title": "Numerical data",
    "section": "Mean vs. Median",
    "text": "Mean vs. Median\nLet’s return to the insurance data. In the plot below, the orange line denotes the sample mean charges, and the blue line denotes the sample median:\n\nWhich is better? The mean or the median?"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html#boxplots-vs-histograms",
    "href": "slides/slides-04-numerical-pt2.html#boxplots-vs-histograms",
    "title": "Numerical data",
    "section": "Boxplots vs histograms",
    "text": "Boxplots vs histograms\nWhich plot do you prefer?"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html#transforming-data",
    "href": "slides/slides-04-numerical-pt2.html#transforming-data",
    "title": "Numerical data",
    "section": "Transforming data",
    "text": "Transforming data\n\nWhen data are strongly skewed or take on an “inconvenient” range of values, we might transform them so they are easier to work with\nA transformation rescales the data using a function\n\ne.g. \\(f(x) = e^x\\), \\(f(x) = \\log_{10}(x)\\), \\(f(x) = \\ln(x)\\), \\(f(x) = \\sqrt{x}\\)\nThe exact transformation you choose depends heavily on the data!!"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#exploratory-data-analysis",
    "href": "slides/slides-05-data-vis.html#exploratory-data-analysis",
    "title": "Visualizations with ggplot",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nExploratory data analysis (EDA) is an approach to analyzing data sets to summarize the main characteristics.\n\nOften visual through plots\n\nBecause of its name “exploratory”, we typically perform EDA at the beginning of a project\nCan also calculate summary statistics and perform data wrangling/manipulation/transformation at (or before) this stage of the analysis"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#tidy-data",
    "href": "slides/slides-05-data-vis.html#tidy-data",
    "title": "Visualizations with ggplot",
    "section": "Tidy data",
    "text": "Tidy data\n\nThe first step of any data/statistical analysis is to understand the data you are working with. This often involves getting the data into R\nThen, it is a good idea to take a macro-level look at the data to ensure it is in tidy format, which means:\n\nEach row in the data set represent an observation/case\nEach columns represents a variable\n\nAnscombe data: four datasets with two variables each\n\n\n\n\nNon-tidy version:\n\n\n  x1 x2 x3 x4   y1   y2    y3   y4\n1 10 10 10  8 8.04 9.14  7.46 6.58\n2  8  8  8  8 6.95 8.14  6.77 5.76\n3 13 13 13  8 7.58 8.74 12.74 7.71\n4  9  9  9  8 8.81 8.77  7.11 8.84\n5 11 11 11  8 8.33 9.26  7.81 8.47\n6 14 14 14  8 9.96 8.10  8.84 7.04\n\n\n\nTidy version:\n\n\n   set  x     y\n1    I 10  8.04\n2    I  8  6.95\n3    I 13  7.58\n4    I  9  8.81\n5    I 11  8.33\n6    I 14  9.96\n7    I  6  7.24\n8    I  4  4.26\n9    I 12 10.84\n10   I  7  4.82\n11   I  5  5.68\n12  II 10  9.14\n13  II  8  8.14\n14  II 13  8.74\n15  II  9  8.77"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#country-footprint-datahttpswww.kaggle.comfootprintnetworkecological-footprint",
    "href": "slides/slides-05-data-vis.html#country-footprint-datahttpswww.kaggle.comfootprintnetworkecological-footprint",
    "title": "Visualizations with ggplot",
    "section": "Country footprint data^[https://www.kaggle.com/footprintnetwork/ecological-footprint]",
    "text": "Country footprint data^[https://www.kaggle.com/footprintnetwork/ecological-footprint]\n\nfootprint_data <- read_csv(\"data/countries_footprint.csv\")\nfootprint_data\n\n# A tibble: 188 × 14\n   Country   Region Population   HDI    GDP Cropland Grazing Forest Carbon  Fish\n   <chr>     <chr>       <dbl> <dbl>  <dbl>    <dbl>   <dbl>  <dbl>  <dbl> <dbl>\n 1 Afghanis… Middl…      29.8   0.46   615.     0.3     0.2    0.08   0.18  0   \n 2 Albania   North…       3.16  0.73  4534.     0.78    0.22   0.25   0.87  0.02\n 3 Algeria   Africa      38.5   0.73  5431.     0.6     0.16   0.17   1.14  0.01\n 4 Angola    Africa      20.8   0.52  4666.     0.33    0.15   0.12   0.2   0.09\n 5 Antigua … Latin…       0.09  0.78 13205.    NA      NA     NA     NA    NA   \n 6 Argentina Latin…      41.1   0.83 13540      0.78    0.79   0.29   1.08  0.1 \n 7 Armenia   Middl…       2.97  0.73  3426.     0.74    0.18   0.34   0.89  0.01\n 8 Aruba     Latin…       0.1  NA       NA     NA      NA     NA     NA    NA   \n 9 Australia Asia-…      23.0   0.93 66604.     2.68    0.63   0.89   4.85  0.11\n10 Austria   Europ…       8.46  0.88 51274.     0.82    0.27   0.63   4.14  0.06\n# ℹ 178 more rows\n# ℹ 4 more variables: Total <dbl>, EarthsRequired <dbl>,\n#   CountriesRequired <dbl>, DataQuality <chr>"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#country-footprint-data-httpswww.kaggle.comfootprintnetworkecological-footprint",
    "href": "slides/slides-05-data-vis.html#country-footprint-data-httpswww.kaggle.comfootprintnetworkecological-footprint",
    "title": "Visualizations with ggplot",
    "section": "Country footprint data ^[https://www.kaggle.com/footprintnetwork/ecological-footprint]",
    "text": "Country footprint data ^[https://www.kaggle.com/footprintnetwork/ecological-footprint]\n\nfootprint_data <- read_csv(\"data/countries_footprint.csv\")\nfootprint_data\n\n# A tibble: 188 × 14\n   Country   Region Population   HDI    GDP Cropland Grazing Forest Carbon  Fish\n   <chr>     <chr>       <dbl> <dbl>  <dbl>    <dbl>   <dbl>  <dbl>  <dbl> <dbl>\n 1 Afghanis… Middl…      29.8   0.46   615.     0.3     0.2    0.08   0.18  0   \n 2 Albania   North…       3.16  0.73  4534.     0.78    0.22   0.25   0.87  0.02\n 3 Algeria   Africa      38.5   0.73  5431.     0.6     0.16   0.17   1.14  0.01\n 4 Angola    Africa      20.8   0.52  4666.     0.33    0.15   0.12   0.2   0.09\n 5 Antigua … Latin…       0.09  0.78 13205.    NA      NA     NA     NA    NA   \n 6 Argentina Latin…      41.1   0.83 13540      0.78    0.79   0.29   1.08  0.1 \n 7 Armenia   Middl…       2.97  0.73  3426.     0.74    0.18   0.34   0.89  0.01\n 8 Aruba     Latin…       0.1  NA       NA     NA      NA     NA     NA    NA   \n 9 Australia Asia-…      23.0   0.93 66604.     2.68    0.63   0.89   4.85  0.11\n10 Austria   Europ…       8.46  0.88 51274.     0.82    0.27   0.63   4.14  0.06\n# ℹ 178 more rows\n# ℹ 4 more variables: Total <dbl>, EarthsRequired <dbl>,\n#   CountriesRequired <dbl>, DataQuality <chr>"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#country-footprint-data",
    "href": "slides/slides-05-data-vis.html#country-footprint-data",
    "title": "Visualizations with ggplot",
    "section": "Country footprint data",
    "text": "Country footprint data\nData on the ecological footprint by country in 2023\n\n\nfootprint_data <- read_csv(\"data/countries_footprint.csv\")\nfootprint_data\n\n# A tibble: 182 × 15\n   Country       Region  SDGi Life_Exectancy   HDI   GDP Income_Group Population\n   <chr>         <chr>  <dbl>          <dbl> <dbl> <dbl> <chr>             <dbl>\n 1 Afghanistan   Middl…  52.5             62  0.48    NA LI                 40.8\n 2 Albania       Other…  71.6             76  0.8  14889 UM                  2.9\n 3 Algeria       Africa  71.5             76  0.75 11137 UM                 45.4\n 4 Angola        Africa  50.9             62  0.59  6304 LM                 35  \n 5 Antigua and … Centr…  NA               78  0.79 18749 HI                  0.1\n 6 Argentina     South…  72.8             75  0.84 22117 UM                 46  \n 7 Armenia       Middl…  71.1             72  0.76 13548 LM                  3  \n 8 Australia     Asia-…  75.6             83  0.95 53053 HI                 26.1\n 9 Austria       EU-27   82.3             81  0.92 55460 HI                  9.1\n10 Azerbaijan    Middl…  73.5             69  0.75 14692 UM                 10.3\n# ℹ 172 more rows\n# ℹ 7 more variables: Cropland <dbl>, Grazing <dbl>, Forest_Product <dbl>,\n#   Carbon <dbl>, Fish <dbl>, Built_up_land <dbl>, Total <dbl>\n\n\n\nData obtained from https://www.kaggle.com/datasets/jainaru/global-ecological-footprint-2023"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#why-do-we-visualize",
    "href": "slides/slides-05-data-vis.html#why-do-we-visualize",
    "title": "Visualizations with ggplot",
    "section": "Why do we visualize?",
    "text": "Why do we visualize?\n\nSummary statistics from each of the four datasets in Anscombe:\n\n\n\n\n# A tibble: 4 × 5\n  set   mean_x mean_y  sd_x  sd_y\n  <fct>  <dbl>  <dbl> <dbl> <dbl>\n1 I          9   7.50  3.32  2.03\n2 II         9   7.50  3.32  2.03\n3 III        9   7.5   3.32  2.03\n4 IV         9   7.50  3.32  2.03\n\n\n\n\nLet’s visualize the four data sets. What would be an appropriate type of plot to examine the relationship between the two variables?"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#common-plots-numerical",
    "href": "slides/slides-05-data-vis.html#common-plots-numerical",
    "title": "Visualizations with ggplot",
    "section": "Common plots (numerical)",
    "text": "Common plots (numerical)"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#aesthetics",
    "href": "slides/slides-05-data-vis.html#aesthetics",
    "title": "Visualizations with ggplot",
    "section": "Aesthetics",
    "text": "Aesthetics\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges, col = smoker)) +\n  geom_point()\n\n\n\n\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges, col = age)) +\n  geom_point()\n\n\n\n\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges, col = age,\n                                       shape = smoker)) +\n  geom_point()\n\n\n\n\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges, alpha = age,\n                                       shape = smoker)) +\n  geom_point()"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#faceting",
    "href": "slides/slides-05-data-vis.html#faceting",
    "title": "Visualizations with ggplot",
    "section": "Faceting",
    "text": "Faceting"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#modifications",
    "href": "slides/slides-05-data-vis.html#modifications",
    "title": "Visualizations with ggplot",
    "section": "Modifications",
    "text": "Modifications\n\nAdding title\nChanging axis title"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#common-plots-numerical-in-ggplot",
    "href": "slides/slides-05-data-vis.html#common-plots-numerical-in-ggplot",
    "title": "Visualizations with ggplot",
    "section": "Common plots (numerical) in ggplot",
    "text": "Common plots (numerical) in ggplot\n\nWe have learned about histograms, density plots, boxplots, and scatterplots\nNow learn how to create these plots using the ggplot() function from the ggplot2 library\n\nPlots are constructed in layers\n\nAt a minimum, we need to specify 1) the dataset, 2) variable(s) from the dataset we’d like to plot, and 3) the type of plot\nThis is what the code will generally look like. Values in < > denote what you as the coder need to specify.\n\n\n\nggplot(data = <dataset>, \n       mapping = aes(x = <x-var>, y = <y-var>)) +\n  geom_xxx() +\n  <other options>\n\n\n\n\nNew lines and spacing don’t impact the execution of code, but are important for good coding style!"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#additional-variables",
    "href": "slides/slides-05-data-vis.html#additional-variables",
    "title": "Visualizations with ggplot",
    "section": "Additional variables",
    "text": "Additional variables\n\nDepending on the plot and data, we can map additional variables by using aesthetics (color, size, shape, alpha (transparency) or faceting"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#live-code",
    "href": "slides/slides-05-data-vis.html#live-code",
    "title": "Visualizations with ggplot",
    "section": "Live code",
    "text": "Live code\nNote: most of the code I will show will be presented at the end of these slides. However, we will most likely go off-script based on questions from the class!"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#geom_histogram",
    "href": "slides/slides-05-data-vis.html#geom_histogram",
    "title": "Visualizations with ggplot",
    "section": "geom_histogram()",
    "text": "geom_histogram()\n\nggplot(data = insurance, mapping = aes(x = charges)) +\n  geom_histogram()\n\n\n\n\n\n\n\nggplot(data = insurance, mapping = aes(x = charges)) +\n  geom_histogram(binwidth = 5000)\n\n\n\n\n\n\nggplot(data = insurance, mapping = aes(x = charges)) +\n  geom_histogram(bins = 20)"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#geom_density",
    "href": "slides/slides-05-data-vis.html#geom_density",
    "title": "Visualizations with ggplot",
    "section": "geom_density()",
    "text": "geom_density()\n\nggplot(data = insurance, mapping = aes(x = charges)) +\n  geom_density()"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#geom_boxplot",
    "href": "slides/slides-05-data-vis.html#geom_boxplot",
    "title": "Visualizations with ggplot",
    "section": "geom_boxplot()",
    "text": "geom_boxplot()\n\nggplot(data = insurance, mapping = aes(x = charges)) +\n  geom_boxplot()"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#side-by-side-boxplots",
    "href": "slides/slides-05-data-vis.html#side-by-side-boxplots",
    "title": "Visualizations with ggplot",
    "section": "Side-by-side boxplots",
    "text": "Side-by-side boxplots\n\nggplot(data = insurance, mapping = aes(x = sex, y = charges)) +\n  geom_boxplot()"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#geom_point",
    "href": "slides/slides-05-data-vis.html#geom_point",
    "title": "Visualizations with ggplot",
    "section": "geom_point()",
    "text": "geom_point()\n\nggplot(data = insurance, mapping = aes(x = age, y = charges)) +\n  geom_point()"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#when-to-map-to-variable",
    "href": "slides/slides-05-data-vis.html#when-to-map-to-variable",
    "title": "Visualizations with ggplot",
    "section": "When to map to variable",
    "text": "When to map to variable\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges)) +\n  geom_point(col = \"purple\")\n\n\n\n\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges, col = \"purple\")) +\n  geom_point()"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#facet_wrap",
    "href": "slides/slides-05-data-vis.html#facet_wrap",
    "title": "Visualizations with ggplot",
    "section": "facet_wrap()",
    "text": "facet_wrap()\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges)) +\n  geom_point() +\n  facet_wrap(~ smoker)\n\n\n\n\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges)) +\n  geom_point() +\n  facet_wrap(~ smoker, scales = \"free_y\")"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#facet_grid",
    "href": "slides/slides-05-data-vis.html#facet_grid",
    "title": "Visualizations with ggplot",
    "section": "facet_grid()",
    "text": "facet_grid()\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges)) +\n  geom_point() +\n  facet_grid(sex ~ smoker)"
  },
  {
    "objectID": "slides/slides-05-data-vis.html#adding-titles",
    "href": "slides/slides-05-data-vis.html#adding-titles",
    "title": "Visualizations with ggplot",
    "section": "Adding titles",
    "text": "Adding titles\n\nggplot(data = insurance, mapping = aes(x = charges)) +\n  geom_histogram() +\n  ggtitle(\"Histogram of charges\")\n\n\n\n\n\nggplot(data = insurance, mapping = aes(x = charges)) +\n  geom_histogram() +\n  ggtitle(\"Histogram of charges\") +\n  xlab(\"Charges ($)\")\n\n\n\n\n\nggplot(data = insurance, mapping = aes(x = charges)) +\n  geom_histogram() +\n  labs(title = \"Histogram of charges\",\n       x = \"Charges ($)\", y = \"Count\")"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#looking-at-your-data",
    "href": "slides/slides-06-categorical-data.html#looking-at-your-data",
    "title": "Categorical data",
    "section": "Looking at your data",
    "text": "Looking at your data\nIs your data tidy, or do you have a table of counts (i.e. a frequency table)?\n\n\n\n\n\n\n\n# A tibble: 5 × 1\n  fruit \n  <chr> \n1 apple \n2 apple \n3 orange\n4 apple \n5 orange\n\n\n\n\n\n# A tibble: 2 × 2\n  fruit  number\n  <chr>   <dbl>\n1 apple       3\n2 orange      2"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#univariate-visualizations",
    "href": "slides/slides-06-categorical-data.html#univariate-visualizations",
    "title": "Categorical data",
    "section": "Univariate visualizations",
    "text": "Univariate visualizations\nIf we are interested in visualizing the distribution of a single categorical variable, it is common to use a barplot, where the different levels are displayed on ones axis and the counts of each level are portrayed on the the other axis.\n\n\n# A tibble: 2 × 2\n  smoker     n\n  <chr>  <int>\n1 no       155\n2 yes       45"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#contingency-tables",
    "href": "slides/slides-06-categorical-data.html#contingency-tables",
    "title": "Categorical data",
    "section": "Contingency tables",
    "text": "Contingency tables\n\nPerhaps we are interested in examining the distribution of two categorical variables at the same time\nWe can summarize the distribution using a two-way table of counts known as a contingency table, where each value in the table count the number of times a particular combination of variable 1 and variable 2 outcomes/levels occurred\n\n\n\n\nContingency table\n\n\nsmoker\nfemale\nmale\n\n\n\n\nno\n87\n68\n\n\nyes\n17\n28\n\n\n\n\n\n\n\nNote: can easily obtain the distribution of just one of the variables by looking row-wise or column-wise\n\nWe essentially convert the contingency table to a visualization to visualize the distribution of two categorical variables"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#dodged-bar-plot",
    "href": "slides/slides-06-categorical-data.html#dodged-bar-plot",
    "title": "Categorical data",
    "section": "Dodged bar plot",
    "text": "Dodged bar plot\nThe dodged bar plot directly converts the contingency table to a visualization.\n\n\n\n\n\n\n\n\n\nContingency table\n \n  \n    smoker \n    female \n    male \n  \n \n\n  \n    no \n    87 \n    68 \n  \n  \n    yes \n    17 \n    28"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#stacked-bar-plot",
    "href": "slides/slides-06-categorical-data.html#stacked-bar-plot",
    "title": "Categorical data",
    "section": "Stacked bar plot",
    "text": "Stacked bar plot\n\n\nThe stacked bar plot looks at the counts either row-wise or column-wise.\n\n\n\n\n\nContingency table\n \n  \n    smoker \n    female \n    male \n  \n \n\n  \n    no \n    87 \n    68 \n  \n  \n    yes \n    17 \n    28"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#proportions",
    "href": "slides/slides-06-categorical-data.html#proportions",
    "title": "Categorical data",
    "section": "Proportions",
    "text": "Proportions\nCan convert the contingency table to proportions row-wise or column-wise to obtain the fractional breakdown of one variable in another.\n\n\n\n\n\n\nContingency table\n \n  \n    smoker \n    female \n    male \n  \n \n\n  \n    no \n    87 \n    68 \n  \n  \n    yes \n    17 \n    28 \n  \n\n\n\n\n\n\n\n\n\n\n\nRow-wise proportions\n \n  \n    smoker \n    female \n    male \n  \n \n\n  \n    no \n    0.561 \n    0.439 \n  \n  \n    yes \n    0.378 \n    0.622 \n  \n\n\n\n\n\n\n\n\n\n\n\nWhat does the quantity 0.378 represent?\nIf we take the proportions row-wise, does each row need to sum to 1?\nIf we take the proportions row-wise, does each column need to sum to 1?"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#proportions-cont.",
    "href": "slides/slides-06-categorical-data.html#proportions-cont.",
    "title": "Categorical data",
    "section": "Proportions (cont.)",
    "text": "Proportions (cont.)\n\nSet up how to find the column-wise proportions using our contingency table\n\n\n\n\n\nContingency table\n \n  \n    smoker \n    female \n    male \n  \n \n\n  \n    no \n    87 \n    68 \n  \n  \n    yes \n    17 \n    28"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#standardized-bar-plot",
    "href": "slides/slides-06-categorical-data.html#standardized-bar-plot",
    "title": "Categorical data",
    "section": "Standardized bar plot",
    "text": "Standardized bar plot\nThe standardized bar plot visualizes these row-wise or column-wise proportions."
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#choosing-a-bar-plot",
    "href": "slides/slides-06-categorical-data.html#choosing-a-bar-plot",
    "title": "Categorical data",
    "section": "Choosing a bar plot",
    "text": "Choosing a bar plot\n\n\n\nUsing any of the plots, do you believe the smoker status and sex are associated?\nWhen might you prefer to use the stacked, dodged, or standardized bar plot?"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#live-code",
    "href": "slides/slides-06-categorical-data.html#live-code",
    "title": "Categorical data",
    "section": "Live code",
    "text": "Live code\n\nBar plots\nAesthetics: fill, shape\nFaceting"
  },
  {
    "objectID": "live_code/dplyr.html",
    "href": "live_code/dplyr.html",
    "title": "Data wrangling with dplyr",
    "section": "",
    "text": "library(tidyverse)\n\n## modify this line accordingly!\ndatascience <- read_csv(\"data/datascience_survey_subset.csv\")\nBy default, all dplyr functions expect the first argument to be a data frame."
  },
  {
    "objectID": "live_code/dplyr.html#selecting-a-single-column",
    "href": "live_code/dplyr.html#selecting-a-single-column",
    "title": "Untitled",
    "section": "Selecting a single column",
    "text": "Selecting a single column\nSometimes, there are a lot of columns in a data frame and we might not want all of them. The select() function gives us an easy way to choose which columns/variables we’d like to work with.\nThe select() function requires by default two arguments: the data frame and the variable names to choose from that data frame.\nThe following code works…\n\nselect(datascience, Age)\n\n# A tibble: 2,618 × 1\n     Age\n   <dbl>\n 1    56\n 2    33\n 3    26\n 4    25\n 5    33\n 6    21\n 7    22\n 8    29\n 9    35\n10    37\n# ℹ 2,608 more rows\n\n\n…but it’s preferable to take advantage of piping in order to make code more readable:\n\ndatascience |>\n  select(Age)\n\n# A tibble: 2,618 × 1\n     Age\n   <dbl>\n 1    56\n 2    33\n 3    26\n 4    25\n 5    33\n 6    21\n 7    22\n 8    29\n 9    35\n10    37\n# ℹ 2,608 more rows\n\n\n\n\nWhat’s going on here?\n\nStart with the data frame datascience\nPipe the data frame using |> the select() function and specify that we want the variable Age\nThe result is a data frame with 2618 rows and 1 column\n\nBy default, all dplyr functions expect the first argument to be a data frame."
  },
  {
    "objectID": "practice_probs/practice-04-numerical-data-pt2.html",
    "href": "practice_probs/practice-04-numerical-data-pt2.html",
    "title": "Numerical data (part 2)",
    "section": "",
    "text": "Please work on the practice problems in your group. Problems with an asterisk \\(^*\\) will be assigned to the weekly problem set.\n\nThe infant mortality rate is defined as the number of infant deaths per 1,000 live births. This rate is often used as an indicator of the level of health in a country. The histogram below shows the distribution of estimated infant death rates for 224 countries for which such data were available in 2014. In particular, this is a relative frequency histogram, which shows proportions instead of raw counts on the y-axis:\n\n\nEstimate \\(Q_{1}\\), the median \\(m\\), and \\(Q_{3}\\) from the histogram.\nWould you expect the mean of this dataset to be smaller or larger than the median? Explain your reasoning.\n\nSuppose that an exam has a total of 100 possible points, and the average score was an 85 with standard deviation of 15. Is the distribution of the scores on this exam symmetric? If not, what shape would you expect this distribution to have? Explain your reasoning.\nFor registered students at universities in the United States, do you expect the average age or the median age to be larger? Why?\n(\\(^*\\)) The statistic \\(\\frac{\\bar{x}}{m}\\) can be used as a measure of skewness. Suppose we have a distribution where all observations are greater than 0 (i.e. \\(x_{i} > 0\\) for all observations \\(i = 1,\\ldots, n\\)). What is the expected shape of the distribution under the following conditions? Explain your reasoning.\n\n\\(\\frac{\\bar{x}}{m} = 1\\)\n\\(\\frac{\\bar{x}}{m} > 1\\)\n\\(\\frac{\\bar{x}}{m} > 1\\)"
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#data",
    "href": "slides/slides-03-numerical-pt1.html#data",
    "title": "Numerical data",
    "section": "Data",
    "text": "Data\nWhich of the following variables are numerical?"
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#scatterplots",
    "href": "slides/slides-03-numerical-pt1.html#scatterplots",
    "title": "Numerical data",
    "section": "Scatterplots",
    "text": "Scatterplots\nScatterplots are bivariate (two-variable) visualizations that provide a case-by-case view of the data for two numerical variables\n\nEach point represents the observed pair of values of variables 1 and 2 for a case in the dataset"
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#live-code",
    "href": "slides/slides-03-numerical-pt1.html#live-code",
    "title": "Numerical data",
    "section": "Live code",
    "text": "Live code\nIf you’d like to follow along, please download the .Rmd template associated with today’s class! Otherwise, feel free to just watch and try coding on your own later on.\nWe will cover:\n\nScatterplots and histograms in base R"
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#scatterplots-cont.",
    "href": "slides/slides-03-numerical-pt1.html#scatterplots-cont.",
    "title": "Numerical data",
    "section": "Scatterplots (cont.)",
    "text": "Scatterplots (cont.)\n\nHow do we determine which variable to put on each axis?\nWhat do scatterplots reveal about the data, and how are they useful?\n\n\nAssociations/patterns (linear, exponential, etc) between two variables. They might not tell the complete story, however!\nPositive vs negative associations"
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#dot-plots",
    "href": "slides/slides-03-numerical-pt1.html#dot-plots",
    "title": "Numerical data",
    "section": "Dot plots",
    "text": "Dot plots\n\nDot plots are a basic visualization that show the distribution of a single variable (univariate)\nIn the following, we have a dot plot of BMI rounded to the nearest integer.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTypically, one dot for each case in our data. What is a disadvantage of dot plots?"
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#binning",
    "href": "slides/slides-03-numerical-pt1.html#binning",
    "title": "Numerical data",
    "section": "Binning",
    "text": "Binning\n\nWe will sacrifice a bit more of precision for convenience by binning:\n\nSegment the variable into equal-sized bins\nVisualize the value of each observation using its corresponding bin\n\nFor example, the bmi variable has observed values of \\(15.96\\) through \\(49.6\\). Consider the following bins of size 5: [15, 19), [19, 23), [23, 27), …, [49, 53)\n\nConvention of left or right inclusive?\n\nWe tabulate/count up the number of observations that fall into each bin."
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#histograms",
    "href": "slides/slides-03-numerical-pt1.html#histograms",
    "title": "Numerical data",
    "section": "Histograms",
    "text": "Histograms\nHistograms are visualizations that display the binned counts as bars for each bin.\n\nHistograms provide a view of the density of the data (the values the data take on as well as how often)\n\n\n\n\n\n\n\n\n\nbmi_bin\ncount\n\n\n\n\n[15, 19)\n5\n\n\n[19, 23)\n12\n\n\n[23, 27)\n35\n\n\n[27, 31)\n58\n\n\n[31, 35)\n41\n\n\n[35, 39)\n35\n\n\n[39, 43)\n13\n\n\n[49, 52)\n1"
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#histograms-cont.",
    "href": "slides/slides-03-numerical-pt1.html#histograms-cont.",
    "title": "Numerical data",
    "section": "Histograms (cont.)",
    "text": "Histograms (cont.)\n\nHow would you describe the shape of the distributions in the following two histograms?"
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#histograms-cont.-1",
    "href": "slides/slides-03-numerical-pt1.html#histograms-cont.-1",
    "title": "Numerical data",
    "section": "Histograms (cont.)",
    "text": "Histograms (cont.)\n\nHow would you describe the shape and modality in the following two histograms?"
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#mean",
    "href": "slides/slides-03-numerical-pt1.html#mean",
    "title": "Numerical data",
    "section": "Mean",
    "text": "Mean\n\nBy far the most common way to measure the center of the distribution of a numerical variable is using the mean (also called the average)\nWe use the term sample mean when calculating a mean using sampled data. The sample mean is typically denoted as \\(\\bar{x}\\)\n\n\\(x\\) is a placeholder for the variable of interest (e.g. BMI, charges)\nThe bar communicates that we are looking at the average\n\nThe sample mean is the sum over all the observed values of the variable, divided by total number of observations \\(n\\):\n\n\n\\[\\bar{x} = \\frac{x_{1} + x_{2} + \\ldots x_{n}}{n} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i}\\]"
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#mean-cont.",
    "href": "slides/slides-03-numerical-pt1.html#mean-cont.",
    "title": "Numerical data",
    "section": "Mean (cont.)",
    "text": "Mean (cont.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe sample mean \\(\\bar{x}\\) is an example of a sample statistic\nThe mean over the entire population is an example of a population parameter. The population mean is often denoted \\(\\mu\\) (Greek letter mu)\nThe sample mean \\(\\bar{x}\\) is often used as an estimate for \\(\\mu\\) (more on this in STAT 311!)"
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#example",
    "href": "slides/slides-03-numerical-pt1.html#example",
    "title": "Numerical data",
    "section": "Example",
    "text": "Example\nWe will be looking at some medical insurance data throughout these slides.\n\nWhich of the following variables are numerical? Which are discrete vs. continuous?"
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#means-depend-on-proportions",
    "href": "slides/slides-03-numerical-pt1.html#means-depend-on-proportions",
    "title": "Numerical data",
    "section": "Means depend on proportions",
    "text": "Means depend on proportions\n\n\nWhat is the average of the following values? \\(\\qquad 1, 4, 4\\)\nIf instead there were ten 1’s and twenty 4’s, would the average be the same?\n\n\n\n\\(\\bar{x} = \\frac{1+4+4}{3} = 1\\left(\\frac{1}{3} \\right) + 4\\left( \\frac{2}{3}\\right) = \\frac{9}{3} = 3\\)\n\\(\\bar{x} = 1\\left(\\frac{10}{30}\\right) + 4 \\left(\\frac{20}{30} \\right) = \\frac{90}{30} = 3\\)"
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#variability",
    "href": "slides/slides-03-numerical-pt1.html#variability",
    "title": "Numerical data",
    "section": "Variability",
    "text": "Variability\n\nHowever, at the heart of statistics is also the variability or spread of the distribution of the variable\nWe will work with variance and standard deviation, which are ways to describe how spread out data are from their mean"
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#deviation",
    "href": "slides/slides-03-numerical-pt1.html#deviation",
    "title": "Numerical data",
    "section": "Deviation",
    "text": "Deviation\nWe begin with deviation, which is the distance or difference between an observation from the (sample) mean\n\nHow might we write this using statistical notation?\nLet’s write out the deviations of our estimated weights"
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#variance-and-standard-deviation",
    "href": "slides/slides-03-numerical-pt1.html#variance-and-standard-deviation",
    "title": "Numerical data",
    "section": "Variance and standard deviation",
    "text": "Variance and standard deviation\n\nThe sample variance \\(s^2\\) squares the deviations and takes an average:\n\\[\ns^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (x_{i} - \\bar{x})^2\n\\]\n\nLet’s talk about this notation and intuition behind this formula. In particular, there are at least two things to note\n\n\nSet-up the calculation of the sample variance for our data\n\n\nI will calculate this in R\n\nThe sample standard deviation \\(s\\) is the simply the square root of the sample variance (\\(s = \\sqrt{s^2}\\))"
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#variance-and-standard-deviation-cont.",
    "href": "slides/slides-03-numerical-pt1.html#variance-and-standard-deviation-cont.",
    "title": "Numerical data",
    "section": "Variance and standard deviation (cont.)",
    "text": "Variance and standard deviation (cont.)\n\nLike the mean, the population values for variance and standard deviation are denoted with Greek letters:\n\n\\(\\sigma\\) for population standard deviation (sigma)\n\\(\\sigma^2\\) for population variance\n\n\nIf the calculation of standard deviation is a more complicated quantity than the variance, why do we bother with standard deviation?"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html#comparing-numerical-data-across-groups",
    "href": "slides/slides-04-numerical-pt2.html#comparing-numerical-data-across-groups",
    "title": "Numerical data",
    "section": "Comparing numerical data across groups",
    "text": "Comparing numerical data across groups\n\nWhile we haven’t yet discussed categorical data, it is common to want to visualize the distribution of a numerical variable across different groups/levels of a categorical variable"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html#live-code",
    "href": "slides/slides-04-numerical-pt2.html#live-code",
    "title": "Numerical data",
    "section": "Live code",
    "text": "Live code\n\nmedian()\nBoxplots in base R"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html",
    "href": "slides/slides-06-categorical-data.html",
    "title": "Categorical data",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nRows: 200 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): sex, smoker, region\ndbl (4): age, bmi, children, charges\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#factoring",
    "href": "slides/slides-06-categorical-data.html#factoring",
    "title": "Categorical data",
    "section": "Factoring",
    "text": "Factoring"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html",
    "href": "slides/slides-05-numerical-data-viz.html",
    "title": "Visualizations with ggplot",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nRows: 1338 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): sex, smoker, region\ndbl (4): age, bmi, children, charges\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#exploratory-data-analysis",
    "href": "slides/slides-05-numerical-data-viz.html#exploratory-data-analysis",
    "title": "Visualizations with ggplot",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nExploratory data analysis (EDA) is an approach to analyzing data sets to summarize the main characteristics.\n\nOften visual through plots\n\nBecause of its name “exploratory”, we typically perform EDA at the beginning of a project\nCan also calculate summary statistics and perform data wrangling/manipulation/transformation at (or before) this stage of the analysis"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#tidy-data",
    "href": "slides/slides-05-numerical-data-viz.html#tidy-data",
    "title": "Visualizations with ggplot",
    "section": "Tidy data",
    "text": "Tidy data\n\nWhen working with data in R, always look at the data to ensure it is in tidy format:\n\nEach row represents an observation, each column represents a variable describing the observations\n\nanscombe data frame: four datasets each with 11 observations each and the same two variables\n\n\n\n\nNon-tidy version:\n\n\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89\n\n\n\nTidy version (first 15 rows):\n\n\n   set  x     y\n1    I 10  8.04\n2    I  8  6.95\n3    I 13  7.58\n4    I  9  8.81\n5    I 11  8.33\n6    I 14  9.96\n7    I  6  7.24\n8    I  4  4.26\n9    I 12 10.84\n10   I  7  4.82\n11   I  5  5.68\n12  II 10  9.14\n13  II  8  8.14\n14  II 13  8.74\n15  II  9  8.77"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#why-do-we-visualize",
    "href": "slides/slides-05-numerical-data-viz.html#why-do-we-visualize",
    "title": "Visualizations with ggplot",
    "section": "Why do we visualize?",
    "text": "Why do we visualize?\n\nSummary statistics from each of the four datasets in anscombe:\n\n\n\n\n# A tibble: 4 × 5\n  set   mean_x mean_y  sd_x  sd_y\n  <fct>  <dbl>  <dbl> <dbl> <dbl>\n1 I          9   7.50  3.32  2.03\n2 II         9   7.50  3.32  2.03\n3 III        9   7.5   3.32  2.03\n4 IV         9   7.50  3.32  2.03\n\n\n\n\nLet’s visualize the four data sets. What would be an appropriate type of plot to examine the relationship between the two variables x and y?"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#common-plots-numerical-in-ggplot",
    "href": "slides/slides-05-numerical-data-viz.html#common-plots-numerical-in-ggplot",
    "title": "Visualizations with ggplot",
    "section": "Common plots (numerical) in ggplot",
    "text": "Common plots (numerical) in ggplot\n\nWe have learned about histograms, density plots, boxplots, and scatterplots, and how to code them in base R\nNow learn how to create these plots using the ggplot() function from the ggplot2 library\n\nPlots are constructed in layers\n\nAt a minimum, we need to specify 1) the dataset, 2) variable(s) from the dataset we’d like to plot, and 3) the type of plot\n\nHow does this differ from what we’ve seen in the past?\n\nThis is what the code will generally look like. Values in < > denote what you as the coder need to specify.\n\n\n\nggplot(data = <dataset>, \n       mapping = aes(x = <x-var>, y = <y-var>)) +\n  geom_xxx() +\n  <other options>\n\n\n\n\nNew lines and spacing don’t impact the execution of code, but are important for good coding style!"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#country-footprint-data",
    "href": "slides/slides-05-numerical-data-viz.html#country-footprint-data",
    "title": "Visualizations with ggplot",
    "section": "Country footprint data",
    "text": "Country footprint data\nData on the ecological footprint by country in 2023\n\n\nfootprint_data <- read_csv(\"data/countries_footprint.csv\")\nfootprint_data\n\n# A tibble: 182 × 15\n   Country       Region  SDGi Life_Exectancy   HDI   GDP Income_Group Population\n   <chr>         <chr>  <dbl>          <dbl> <dbl> <dbl> <chr>             <dbl>\n 1 Afghanistan   Middl…  52.5             62  0.48    NA LI                 40.8\n 2 Albania       Other…  71.6             76  0.8  14889 UM                  2.9\n 3 Algeria       Africa  71.5             76  0.75 11137 UM                 45.4\n 4 Angola        Africa  50.9             62  0.59  6304 LM                 35  \n 5 Antigua and … Centr…  NA               78  0.79 18749 HI                  0.1\n 6 Argentina     South…  72.8             75  0.84 22117 UM                 46  \n 7 Armenia       Middl…  71.1             72  0.76 13548 LM                  3  \n 8 Australia     Asia-…  75.6             83  0.95 53053 HI                 26.1\n 9 Austria       EU-27   82.3             81  0.92 55460 HI                  9.1\n10 Azerbaijan    Middl…  73.5             69  0.75 14692 UM                 10.3\n# ℹ 172 more rows\n# ℹ 7 more variables: Cropland <dbl>, Grazing <dbl>, Forest_Product <dbl>,\n#   Carbon <dbl>, Fish <dbl>, Built_up_land <dbl>, Total <dbl>\n\n\n\nData obtained from https://www.kaggle.com/datasets/jainaru/global-ecological-footprint-2023"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#additional-variables",
    "href": "slides/slides-05-numerical-data-viz.html#additional-variables",
    "title": "Visualizations with ggplot",
    "section": "Additional variables",
    "text": "Additional variables\n\nDepending on the plot and data, we can map additional variables by using aesthetics (color, size, shape, alpha (transparency) or faceting"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#modifications",
    "href": "slides/slides-05-numerical-data-viz.html#modifications",
    "title": "Visualizations with ggplot",
    "section": "Modifications",
    "text": "Modifications\n\nAdding title\nChanging axis title"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#live-code",
    "href": "slides/slides-05-numerical-data-viz.html#live-code",
    "title": "Visualizations with ggplot",
    "section": "Live code",
    "text": "Live code\nNote: most of the code I will show is included in the remaining slides. However, we will most likely go off-script based on questions from the class!"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#geom_histogram",
    "href": "slides/slides-05-numerical-data-viz.html#geom_histogram",
    "title": "Visualizations with ggplot",
    "section": "geom_histogram()",
    "text": "geom_histogram()\n\nggplot(data = insurance,  mapping = aes(x = charges)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\nNote the message provided when you execute this code!"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#geom_density",
    "href": "slides/slides-05-numerical-data-viz.html#geom_density",
    "title": "Visualizations with ggplot",
    "section": "geom_density()",
    "text": "geom_density()\n\nggplot(data = insurance, mapping = aes(x = charges)) +\n  geom_density()"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#geom_boxplot",
    "href": "slides/slides-05-numerical-data-viz.html#geom_boxplot",
    "title": "Visualizations with ggplot",
    "section": "geom_boxplot()",
    "text": "geom_boxplot()\n\n\n\nggplot(data = insurance, mapping = aes(x = charges)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nggplot(data = insurance, \n       mapping = aes(y = charges)) +\n  geom_boxplot()"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#side-by-side-boxplots",
    "href": "slides/slides-05-numerical-data-viz.html#side-by-side-boxplots",
    "title": "Visualizations with ggplot",
    "section": "Side-by-side boxplots",
    "text": "Side-by-side boxplots\n\nggplot(data = insurance, mapping = aes(x = sex, y = charges)) +\n  geom_boxplot()\n\n\n\n\n\n\nBivariate plot for a numerical and a categorical variable."
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#geom_point",
    "href": "slides/slides-05-numerical-data-viz.html#geom_point",
    "title": "Visualizations with ggplot",
    "section": "geom_point()",
    "text": "geom_point()\n\nggplot(data = insurance, mapping = aes(x = age, y = charges)) +\n  geom_point()"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#aesthetics",
    "href": "slides/slides-05-numerical-data-viz.html#aesthetics",
    "title": "Visualizations with ggplot",
    "section": "Aesthetics",
    "text": "Aesthetics\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges, \n                                       col = smoker)) +\n  geom_point()\n\n\n\n\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges, col = age)) +\n  geom_point()\n\n\n\n\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges, col = age,\n                                       shape = smoker)) +\n  geom_point()\n\n\n\n\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges, alpha = age,\n                                       shape = smoker)) +\n  geom_point()"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#when-to-map-to-variable",
    "href": "slides/slides-05-numerical-data-viz.html#when-to-map-to-variable",
    "title": "Visualizations with ggplot",
    "section": "When to map to variable",
    "text": "When to map to variable\nWhat’s going on here?\n\n\n\nggplot(data = insurance, \n       mapping = aes(x = bmi, y = charges)) +\n  geom_point(col = \"purple\")\n\n\n\n\n\n\n\n\nggplot(data = insurance, \n       mapping = aes(x = bmi, y = charges)) +\n  geom_point(aes(col = \"purple\"))\n\n\n\n\n\n\n\n\n\n\nKey takeaway: aesthetics should correspond/map to a variable in the data frame\n\n\n“Fixed” visual cues are set outside of aes()"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#facet_wrap",
    "href": "slides/slides-05-numerical-data-viz.html#facet_wrap",
    "title": "Visualizations with ggplot",
    "section": "facet_wrap()",
    "text": "facet_wrap()\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges)) +\n  geom_point() +\n  facet_wrap(~ smoker)\n\n\n\n\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges)) +\n  geom_point() +\n  facet_wrap(~ smoker, scales = \"free_y\")"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#facet_grid",
    "href": "slides/slides-05-numerical-data-viz.html#facet_grid",
    "title": "Visualizations with ggplot",
    "section": "facet_grid()",
    "text": "facet_grid()\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges)) +\n  geom_point() +\n  facet_grid(sex ~ smoker)"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#adding-titles",
    "href": "slides/slides-05-numerical-data-viz.html#adding-titles",
    "title": "Visualizations with ggplot",
    "section": "Adding titles",
    "text": "Adding titles\n\nggplot(data = insurance, mapping = aes(x = charges)) +\n  geom_histogram() +\n  ggtitle(\"Histogram of charges\")\n\n\n\n\n\nggplot(data = insurance, mapping = aes(x = charges)) +\n  geom_histogram() +\n  ggtitle(\"Histogram of charges\") +\n  xlab(\"Charges ($)\")\n\n\n\n\n\nggplot(data = insurance, mapping = aes(x = charges)) +\n  geom_histogram() +\n  labs(title = \"Histogram of charges\",\n       x = \"Charges ($)\", y = \"Count\")"
  },
  {
    "objectID": "live_code/dplyr.html#selecting-columns",
    "href": "live_code/dplyr.html#selecting-columns",
    "title": "Data wrangling with dplyr",
    "section": "Selecting columns",
    "text": "Selecting columns\nSometimes, there are a lot of columns in a data frame and we might not want all of them. The select() function gives us an easy way to choose which columns/variables we’d like to work with.\nThe select() function requires by default two arguments: the data frame and the variable names to choose from that data frame.\nThe following code works…\n\nselect(datascience, Age)\n\n# A tibble: 2,288 × 1\n     Age\n   <dbl>\n 1    56\n 2    33\n 3    26\n 4    25\n 5    33\n 6    22\n 7    29\n 8    35\n 9    37\n10    31\n# ℹ 2,278 more rows\n\n\n…but it’s preferable to take advantage of piping in order to make code more readable:\n\ndatascience |>\n  select(Age)\n\n# A tibble: 2,288 × 1\n     Age\n   <dbl>\n 1    56\n 2    33\n 3    26\n 4    25\n 5    33\n 6    22\n 7    29\n 8    35\n 9    37\n10    31\n# ℹ 2,278 more rows\n\n\n\n\nWhat’s going on here?\n\nStart with the data frame datascience\nPipe (|>) the data frame to the select() function and specify that we want the variable Age\nThe result is a data frame with 2288 rows and 1 column with the Age variable\n\n\n\n\n\n\n\nCheck\n\n\n\nWhy do we type Age and not age?\n\n\n\nMultiple variables and excluding\n\n\n\n\n\n\nExpand\n\n\n\n\n\n\ndatascience |>\n  select(Age, Major)\n\n# A tibble: 2,288 × 2\n     Age Major                                                       \n   <dbl> <chr>                                                       \n 1    56 Mathematics or statistics                                   \n 2    33 Other                                                       \n 3    26 Computer Science                                            \n 4    25 Physics                                                     \n 5    33 Electrical Engineering                                      \n 6    22 Information technology, networking, or system administration\n 7    29 Computer Science                                            \n 8    35 Physics                                                     \n 9    37 Electrical Engineering                                      \n10    31 Computer Science                                            \n# ℹ 2,278 more rows\n\n\n\n\n\n\n\n\nCheck\n\n\n\nWhat happens if we swap the order of the variable names?\n\n\n\nA range of variables\n\ndatascience |>\n  select(Gender:EmploymentStatus)\n\n# A tibble: 2,288 × 3\n   Gender   Age EmploymentStatus                                    \n   <chr>  <dbl> <chr>                                               \n 1 Male      56 Independent contractor, freelancer, or self-employed\n 2 Male      33 Employed full-time                                  \n 3 Male      26 Employed full-time                                  \n 4 Male      25 Employed part-time                                  \n 5 Male      33 Employed full-time                                  \n 6 Male      22 Employed full-time                                  \n 7 Male      29 Employed full-time                                  \n 8 Male      35 Employed full-time                                  \n 9 Male      37 Employed full-time                                  \n10 Male      31 Employed part-time                                  \n# ℹ 2,278 more rows\n\n\n\n\nExcluding variables\n\ndatascience |>\n  select(-Country)\n\n# A tibble: 2,288 × 16\n   Gender   Age EmploymentStatus          EmployerIndustry FormalEducation Major\n   <chr>  <dbl> <chr>                     <chr>            <chr>           <chr>\n 1 Male      56 Independent contractor, … Mix of fields    Master's degree Math…\n 2 Male      33 Employed full-time        Internet-based   Bachelor's deg… Other\n 3 Male      26 Employed full-time        Financial        Master's degree Comp…\n 4 Male      25 Employed part-time        Academic         Bachelor's deg… Phys…\n 5 Male      33 Employed full-time        Telecommunicati… Doctoral degree Elec…\n 6 Male      22 Employed full-time        Mix of fields    Bachelor's deg… Info…\n 7 Male      29 Employed full-time        Pharmaceutical   Master's degree Comp…\n 8 Male      35 Employed full-time        Technology       Doctoral degree Phys…\n 9 Male      37 Employed full-time        Technology       Master's degree Elec…\n10 Male      31 Employed part-time        Technology       Doctoral degree Comp…\n# ℹ 2,278 more rows\n# ℹ 10 more variables: CompensationAmount <dbl>, CompensationCurrency <chr>,\n#   CurrentJobTitle <chr>, TitleFit <chr>, LanguageRecommendation <chr>,\n#   DataScienceIdentity <chr>, WorkDataVisualizations <chr>,\n#   JobSatisfaction <chr>, JobSatisfaction2 <dbl>, ConversionUSD <dbl>"
  },
  {
    "objectID": "live_code/dplyr.html#arranging-rows",
    "href": "live_code/dplyr.html#arranging-rows",
    "title": "Data wrangling with dplyr",
    "section": "Arranging rows",
    "text": "Arranging rows\nWe might want to re-arrange rows in ascending or descending order according to a certain variable:\n\ndatascience |>\n  select(Age, Major) |>\n  arrange(Age)\n\n# A tibble: 2,288 × 2\n     Age Major                                                       \n   <dbl> <chr>                                                       \n 1     0 Mathematics or statistics                                   \n 2     1 Other                                                       \n 3    19 Computer Science                                            \n 4    19 Biology                                                     \n 5    20 Information technology, networking, or system administration\n 6    20 Mathematics or statistics                                   \n 7    20 Computer Science                                            \n 8    20 Mathematics or statistics                                   \n 9    21 Other                                                       \n10    21 Computer Science                                            \n# ℹ 2,278 more rows\n\n\n\n\nBy default, arrange() will reorder in ascending order. If we’d like to go in descending order, we can code arrange(desc(Age))."
  },
  {
    "objectID": "live_code/dplyr.html#slicing-for-certain-row-numbers",
    "href": "live_code/dplyr.html#slicing-for-certain-row-numbers",
    "title": "Data wrangling with dplyr",
    "section": "Slicing for certain row numbers",
    "text": "Slicing for certain row numbers\nRemember, data frames are in tabular format. So each row has a certain index, as does each column. The first row in index 1, the second row index 2, etc.\nThe slice() function expects a vector of row indices to retain:\n\ndatascience |>\n  slice(1:5)\n\n# A tibble: 5 × 17\n  Country   Gender   Age EmploymentStatus EmployerIndustry FormalEducation Major\n  <chr>     <chr>  <dbl> <chr>            <chr>            <chr>           <chr>\n1 United S… Male      56 Independent con… Mix of fields    Master's degree Math…\n2 Russia    Male      33 Employed full-t… Internet-based   Bachelor's deg… Other\n3 Taiwan    Male      26 Employed full-t… Financial        Master's degree Comp…\n4 United S… Male      25 Employed part-t… Academic         Bachelor's deg… Phys…\n5 United S… Male      33 Employed full-t… Telecommunicati… Doctoral degree Elec…\n# ℹ 10 more variables: CompensationAmount <dbl>, CompensationCurrency <chr>,\n#   CurrentJobTitle <chr>, TitleFit <chr>, LanguageRecommendation <chr>,\n#   DataScienceIdentity <chr>, WorkDataVisualizations <chr>,\n#   JobSatisfaction <chr>, JobSatisfaction2 <dbl>, ConversionUSD <dbl>\n\n\n\n\n\n\n\n\nCheck\n\n\n\nWhat is the difference between select() and slice()?"
  },
  {
    "objectID": "live_code/dplyr.html#filtering-to-select-a-subset-of-rows",
    "href": "live_code/dplyr.html#filtering-to-select-a-subset-of-rows",
    "title": "Data wrangling with dplyr",
    "section": "Filtering to select a subset of rows",
    "text": "Filtering to select a subset of rows\nThe slice() function is nice, but unless the rows of your data frame are ordered meaningfully, its actual utility is limited. We might want to look at a set of the cases in which a certain condition is met.\nIn the following code, we only retain the observations where the person’s Major was Computer Science:\n\ndatascience |>\n  filter(Major == \"Computer Science\")\n\n# A tibble: 681 × 17\n   Country  Gender   Age EmploymentStatus EmployerIndustry FormalEducation Major\n   <chr>    <chr>  <dbl> <chr>            <chr>            <chr>           <chr>\n 1 Taiwan   Male      26 Employed full-t… Financial        Master's degree Comp…\n 2 Poland   Male      29 Employed full-t… Pharmaceutical   Master's degree Comp…\n 3 Iran     Male      31 Employed part-t… Technology       Doctoral degree Comp…\n 4 Brazil   Male      25 Employed full-t… Academic         Master's degree Comp…\n 5 Brazil   Male      32 Employed full-t… Academic         Master's degree Comp…\n 6 Russia   Male      31 Independent con… CRM/Marketing    Some college/u… Comp…\n 7 India    Male      23 Employed full-t… Technology       Master's degree Comp…\n 8 Canada   Male      52 Employed full-t… Academic         Bachelor's deg… Comp…\n 9 Russia   Male      26 Independent con… Military/Securi… Bachelor's deg… Comp…\n10 Czech R… Male      25 Independent con… Internet-based   Master's degree Comp…\n# ℹ 671 more rows\n# ℹ 10 more variables: CompensationAmount <dbl>, CompensationCurrency <chr>,\n#   CurrentJobTitle <chr>, TitleFit <chr>, LanguageRecommendation <chr>,\n#   DataScienceIdentity <chr>, WorkDataVisualizations <chr>,\n#   JobSatisfaction <chr>, JobSatisfaction2 <dbl>, ConversionUSD <dbl>\n\n\n\nMultiple conditions\n\n\n\n\n\n\nExpand\n\n\n\n\n\nWe can also filter for more than one condition at once. Within filter(), the comma , specifies that all conditions must be true. It can be read as “and”:\n\ndatascience |>\n  filter(Major == \"Computer Science\", \n         Age == 30) |>\n  select(Major, Age)\n\n# A tibble: 36 × 2\n   Major              Age\n   <chr>            <dbl>\n 1 Computer Science    30\n 2 Computer Science    30\n 3 Computer Science    30\n 4 Computer Science    30\n 5 Computer Science    30\n 6 Computer Science    30\n 7 Computer Science    30\n 8 Computer Science    30\n 9 Computer Science    30\n10 Computer Science    30\n# ℹ 26 more rows\n\n\nIf we just need at least one of multiple conditions to be true, we can use the | operator which stands for “or”:\n\ndatascience |>\n  filter(Major == \"Computer Science\" | \n         Age == 30) |>\n  select(Major, Age)\n\n# A tibble: 765 × 2\n   Major              Age\n   <chr>            <dbl>\n 1 Computer Science    26\n 2 Computer Science    29\n 3 Computer Science    31\n 4 Computer Science    25\n 5 Computer Science    32\n 6 Computer Science    31\n 7 A social science    30\n 8 Computer Science    23\n 9 Biology             30\n10 Computer Science    52\n# ℹ 755 more rows\n\n\n\ndatascience |>\n  filter(Major == \"Computer Science\" | Major == \"Other\",\n         Age == 30) |>\n  select(Major, Age)\n\n# A tibble: 44 × 2\n   Major              Age\n   <chr>            <dbl>\n 1 Computer Science    30\n 2 Computer Science    30\n 3 Computer Science    30\n 4 Computer Science    30\n 5 Computer Science    30\n 6 Computer Science    30\n 7 Computer Science    30\n 8 Computer Science    30\n 9 Other               30\n10 Computer Science    30\n# ℹ 34 more rows"
  },
  {
    "objectID": "live_code/dplyr.html#distinct-to-filter-for-unique-rows",
    "href": "live_code/dplyr.html#distinct-to-filter-for-unique-rows",
    "title": "Data wrangling with dplyr",
    "section": "Distinct to filter for unique rows",
    "text": "Distinct to filter for unique rows\n\ndatascience |>\n  distinct(FormalEducation)\n\n# A tibble: 5 × 1\n  FormalEducation                                                  \n  <chr>                                                            \n1 Master's degree                                                  \n2 Bachelor's degree                                                \n3 Doctoral degree                                                  \n4 Some college/university study without earning a bachelor's degree\n5 I prefer not to answer                                           \n\ndatascience |>\n  distinct(FormalEducation, Major) |>\n  arrange(FormalEducation)\n\n# A tibble: 58 × 2\n   FormalEducation   Major                                                      \n   <chr>             <chr>                                                      \n 1 Bachelor's degree Other                                                      \n 2 Bachelor's degree Physics                                                    \n 3 Bachelor's degree Information technology, networking, or system administrati…\n 4 Bachelor's degree A social science                                           \n 5 Bachelor's degree Electrical Engineering                                     \n 6 Bachelor's degree Mathematics or statistics                                  \n 7 Bachelor's degree Computer Science                                           \n 8 Bachelor's degree Engineering (non-computer focused)                         \n 9 Bachelor's degree A humanities discipline                                    \n10 Bachelor's degree Management information systems                             \n# ℹ 48 more rows\n\n\n\n\n\n\n\n\nCheck\n\n\n\nWhat variables are by default included in the output from distinct()?"
  },
  {
    "objectID": "live_code/dplyr.html#mutate-to-add-a-new-variable",
    "href": "live_code/dplyr.html#mutate-to-add-a-new-variable",
    "title": "Data wrangling with dplyr",
    "section": "Mutate to add a new variable",
    "text": "Mutate to add a new variable\nIt is typical for us to want to add variables to a given data frame. We do this with the mutate() function. We must specify the name of the new variable and how to calculate the value of that variable for each observation:\n\ndatascience %>%\n  mutate(compensation_1k = CompensationAmount/1000) |>\n  select(CompensationAmount, compensation_1k)\n\n# A tibble: 2,288 × 2\n   CompensationAmount compensation_1k\n                <dbl>           <dbl>\n 1             250000             250\n 2            1200000            1200\n 3            1100000            1100\n 4              20000              20\n 5             100000             100\n 6             624000             624\n 7             126000             126\n 8             133000             133\n 9              80000              80\n10              15000              15\n# ℹ 2,278 more rows\n\n\n\n\n\n\n\n\nCheck\n\n\n\nWhat exactly is going on in the second line of code?"
  },
  {
    "objectID": "live_code/dplyr.html#counting-to-create-frequency-tables",
    "href": "live_code/dplyr.html#counting-to-create-frequency-tables",
    "title": "Data wrangling with dplyr",
    "section": "Counting to create frequency tables",
    "text": "Counting to create frequency tables\nWe can count the number of instances we observed each level of a given categorical variable:\n\ndatascience |>\n  count(EmployerIndustry)\n\n# A tibble: 16 × 2\n   EmployerIndustry                     n\n   <chr>                            <int>\n 1 Academic                           478\n 2 CRM/Marketing                       70\n 3 Financial                          211\n 4 Government                         137\n 5 Hospitality/Entertainment/Sports    27\n 6 Insurance                           68\n 7 Internet-based                     134\n 8 Manufacturing                       75\n 9 Military/Security                   35\n10 Mix of fields                      195\n11 Non-profit                          35\n12 Other                              198\n13 Pharmaceutical                      54\n14 Retail                              61\n15 Technology                         445\n16 Telecommunications                  65\n\n\n\n\n\n\n\n\nCheck\n\n\n\nHow does the resulting data frame from count() compare to the original data frame we passed in?\n\n\n\nMaking frequency tables useful\nWe typically want to present the counts in ascending or descending order.\n\n\n\n\n\n\nExpand\n\n\n\n\n\nNote that the following chunks of code do the same thing. One of them takes advantage of an additional argument in count(), whereas the other block of the uses an additional function:\n\ndatascience |>\n  count(EmployerIndustry, sort = T)\n\n# A tibble: 16 × 2\n   EmployerIndustry                     n\n   <chr>                            <int>\n 1 Academic                           478\n 2 Technology                         445\n 3 Financial                          211\n 4 Other                              198\n 5 Mix of fields                      195\n 6 Government                         137\n 7 Internet-based                     134\n 8 Manufacturing                       75\n 9 CRM/Marketing                       70\n10 Insurance                           68\n11 Telecommunications                  65\n12 Retail                              61\n13 Pharmaceutical                      54\n14 Military/Security                   35\n15 Non-profit                          35\n16 Hospitality/Entertainment/Sports    27\n\n\n\ndatascience |>\n  count(EmployerIndustry) |>\n  arrange(desc(n))\n\n# A tibble: 16 × 2\n   EmployerIndustry                     n\n   <chr>                            <int>\n 1 Academic                           478\n 2 Technology                         445\n 3 Financial                          211\n 4 Other                              198\n 5 Mix of fields                      195\n 6 Government                         137\n 7 Internet-based                     134\n 8 Manufacturing                       75\n 9 CRM/Marketing                       70\n10 Insurance                           68\n11 Telecommunications                  65\n12 Retail                              61\n13 Pharmaceutical                      54\n14 Military/Security                   35\n15 Non-profit                          35\n16 Hospitality/Entertainment/Sports    27\n\n\n\n\n\n\n\n\nCheck\n\n\n\nWhat happens if you pass in more than one variable into count()?"
  },
  {
    "objectID": "live_code/dplyr.html#practice",
    "href": "live_code/dplyr.html#practice",
    "title": "Data wrangling with dplyr",
    "section": "Practice",
    "text": "Practice\nSuppose I want to report a data frame that reports each unique level of Major and the proportion of times each level was observed in the data set in order of most popular to least popular. How might we do that?\n\n\nCode\ndatascience |>\n  count(Major) |>\n  mutate(prop = n/sum(n)) |>\n  select(Major, prop) |>\n  arrange(desc(prop))"
  },
  {
    "objectID": "live_code/dplyr.html#summarising-for-summary-statistics",
    "href": "live_code/dplyr.html#summarising-for-summary-statistics",
    "title": "Data wrangling with dplyr",
    "section": "Summarising for summary statistics",
    "text": "Summarising for summary statistics\nThe summarise() function gives us an easy way to calculate summary statistics of variables in the data frame! We just need to know the name of the function that will calculate the summary statistic for us.\n\ndatascience |>\n  summarise(mean_age = mean(Age))\n\n# A tibble: 1 × 1\n  mean_age\n     <dbl>\n1     34.4\n\n\n\n\nYou can obtain multiple summary statistics at once by separating the desired summary statistics with commas.\nThe summarise() function changes the data frame entirely. It collapses rows down to a single/multiple summary statistic, and removes all columns that are irrelevant to the calculation.\n\n\n\n\n\n\nCheck\n\n\n\nWhat happens if you type summarise(mean(Age)) instead?"
  },
  {
    "objectID": "live_code/dplyr.html#grouping-by-grouped-operations",
    "href": "live_code/dplyr.html#grouping-by-grouped-operations",
    "title": "Data wrangling with dplyr",
    "section": "Grouping by grouped operations",
    "text": "Grouping by grouped operations\nSometimes, we want to look at a given statistic or create a new variable focusing on each level of a specific categorical variable. The group_by() function tells R to treat each unique level as a separate data set.\n\ndatascience |>\n  group_by(EmploymentStatus) |>\n  summarise(mean_age = mean(Age))\n\n# A tibble: 3 × 2\n  EmploymentStatus                                     mean_age\n  <chr>                                                   <dbl>\n1 Employed full-time                                       34.3\n2 Employed part-time                                       29.5\n3 Independent contractor, freelancer, or self-employed     39.0"
  },
  {
    "objectID": "slides/slides-07-wrangling.html#working-dataset",
    "href": "slides/slides-07-wrangling.html#working-dataset",
    "title": "Data wrangling with dplyr",
    "section": "Working dataset",
    "text": "Working dataset\nData from Kaggle: In 2017, Kaggle conducted an industry-wide survey to establish a comprehensive view of the state of data science and machine learning. We will be looking at just a subset of the data.\n\nLet’s go ahead and pull to get the data locally\nThen open a new .Rmd to work in\nLet’s load in the data together and take a quick look at it before diving into data wrangling"
  },
  {
    "objectID": "slides/slides-07-wrangling.html#grammar-of-data-wrangling",
    "href": "slides/slides-07-wrangling.html#grammar-of-data-wrangling",
    "title": "Data wrangling with dplyr",
    "section": "Grammar of data wrangling",
    "text": "Grammar of data wrangling\n\n\n\n\nRecall: data frames are objects in R that store tabular data in tidy form\nThe dplyr package uses the concept of functions as verbs that manipulate data frames\n\nselect(): pick columns by name\nslice(): pick rows using indices\nfilters(): pick rows matching criteria\ndistinct(): filter for unique rows\nmutate(): add new variables as columns\nsummarise(): reduce variables to quantitative values\ngroup_by(): for grouped operations based on a variable\nand many more!!!"
  },
  {
    "objectID": "slides/slides-07-wrangling.html#rules-of-dplyr-functions",
    "href": "slides/slides-07-wrangling.html#rules-of-dplyr-functions",
    "title": "Data wrangling with dplyr",
    "section": "Rules of dplyr functions",
    "text": "Rules of dplyr functions\n\nThe first argument is always a data frame\nSubsequent argument(s) say what to do with that data frame\n\nWe connect lines to code using a pipe operator (see next slide)\n\nAlways return a data frame, unless specifically told otherwise"
  },
  {
    "objectID": "slides/slides-07-wrangling.html#pipes",
    "href": "slides/slides-07-wrangling.html#pipes",
    "title": "Data wrangling with dplyr",
    "section": "Pipes",
    "text": "Pipes\n\nIn programming, a pipe is a technique for passing information from one process to another\nIn dplyr, the pipes are coded as |> (i.e. vertical bar and greater than sign)\n\nNot to be confused with +\n\nWe can think about pipes as following a sequence of actions which provide a more natural and easier to read structure\nFor example: suppose that in order to get to work, I need to find my car keys, start my car, drive to work, and then park my car\n\n\n\n\nExpressed as a set of nested R pseudocode, this may look like:\n\n\n\npark(drive(start_car(find(\"car_keys\")), \n           to = \"work\"))\n\n\n\n\nExpressed using pipes, this may look like:\n\n\n\nfind(\"car_keys\") |>\n  start_car() |>\n  drive(to = \"work\") |>\n  park()"
  },
  {
    "objectID": "slides/slides-07-wrangling.html#logical-operators-in-r",
    "href": "slides/slides-07-wrangling.html#logical-operators-in-r",
    "title": "Data wrangling with dplyr",
    "section": "Logical operators in R",
    "text": "Logical operators in R\nIt is common to compare two quantities using logical operators. All of these operators will return a logical TRUE or FALSE. List of some common operators:\n\n<: less than\n<=: less than or equal to\n>: greater than\n>=: greater than or equal to\n==: (exactly) equal to\n!=: not equal to\n\n\n\n1 < 4\n\n[1] TRUE\n\n\n\n\n\n2==3\n\n[1] FALSE\n\n\n\n\n\n2!=3\n\n[1] TRUE"
  },
  {
    "objectID": "slides/slides-07-wrangling.html#logical-operators-cont.",
    "href": "slides/slides-07-wrangling.html#logical-operators-cont.",
    "title": "Data wrangling with dplyr",
    "section": "Logical operators (cont.)",
    "text": "Logical operators (cont.)\nWe might also want to know if a certain quantity “behaves” a certain way. The following also return logical outputs:\n\nis.na(x): test if x is NA\nx %in% y: test if x is in y\n!x: not x\n\n\n\nis.na(NA)\n\n[1] TRUE\n\n\n\n\n\nis.na(\"apple\")\n\n[1] FALSE\n\n\n\n\n\n3 %in% 1:10\n\n[1] TRUE\n\n\n\n\n\n!TRUE\n\n[1] FALSE"
  },
  {
    "objectID": "slides/slides-07-wrangling.html#commenting-code",
    "href": "slides/slides-07-wrangling.html#commenting-code",
    "title": "Data wrangling with dplyr",
    "section": "Commenting code",
    "text": "Commenting code\nRecall that in R, we can comment out lines of code using the # symbol. The line of code will still be displayed, but it will not execute:\n\n1 + 1\n\n[1] 2\n\n# 2 * 1\n3 %in% 1:10\n\n[1] TRUE"
  },
  {
    "objectID": "slides/slides-07-wrangling.html#live-code",
    "href": "slides/slides-07-wrangling.html#live-code",
    "title": "Data wrangling with dplyr",
    "section": "Live code",
    "text": "Live code\nData from Kaggle: In 2017, Kaggle conducted an industry-wide survey to establish a comprehensive view of the state of data science and machine learning. We will be looking at just a subset of the data.\nCopy and paste the following code into a code chunk in your live code! We will load in the data together and take a quick look at it before diving into data wrangling\n\nlibrary(readr)\nurl_file <- \"https://raw.githubusercontent.com/midd-stat201-fall2024/midd-stat201-fall2024.github.io/refs/heads/main/data/datascience_survey_subset.csv\""
  },
  {
    "objectID": "slides/slides-07-wrangling.html#piping-to-ggplot",
    "href": "slides/slides-07-wrangling.html#piping-to-ggplot",
    "title": "Data wrangling with dplyr",
    "section": "Piping to ggplot",
    "text": "Piping to ggplot\n\nRemember that when creating plots, the ggplot() function expect a data frame as its first argument\nWe may sometimes need to wrangle data prior to visualizing it. We have two options (both have pros and cons)\n\nWrangle the data, store the resulting data frame with a new variable name, and then refer to that data frame with ggplot(), or\n\n\ndf_new <- df |>\n  mutate(age_months = age*12)\nggplot(df_new, aes(x = age_months)) +\n  geom_histogram()\n\n\nWrangle the data, and then directly pipe the result into ggplot()\n\n\ndf |>\n  mutate(age_months = age*12) |>\n  ggplot(aes(x = age_months)) +\n  geom_histogram()\n\n\n\n\nWhen do we use |> and when do we use + to connect lines of code?"
  },
  {
    "objectID": "slides/slides-08-probability.html#key-terms",
    "href": "slides/slides-08-probability.html#key-terms",
    "title": "Probability basics",
    "section": "Key terms",
    "text": "Key terms\n\nRandom process: a situation in which a particular result, called an outcome, is random/not known ahead of time\n\nExamples: flipping a coin, rolling six-sided die, sports game, if a treatment is effective\n\nA sample space \\(S\\) is the set of all possible outcomes of the random process\n\n\nWhat are possible sample spaces for the above examples?\n\n\nAn event is a set of outcomes from a random process"
  },
  {
    "objectID": "slides/slides-08-probability.html#probability",
    "href": "slides/slides-08-probability.html#probability",
    "title": "Probability basics",
    "section": "Probability",
    "text": "Probability\n\nFor us, the probability of an outcome is the proportion of times the outcome would occur if we observed the random process an infinite number of times\n\nProbability is used to express the likelihood that some outcome or event will or will not occur\nThink of as a proportion\n\nLet \\(A\\) denote some outcome or event. We denote the probability of \\(A\\) occurring as \\(\\text{P}(A)\\) or \\(\\text{Pr}(A)\\).\nWhen the sample space \\(S\\) is discrete with a finite size, then \\(\\text{Pr}(A) = \\frac{\\text{ number of outcomes favorable to } A}{\\text{ number of total outcomes possible} }\\)"
  },
  {
    "objectID": "slides/slides-08-probability.html#example",
    "href": "slides/slides-08-probability.html#example",
    "title": "Probability basics",
    "section": "Example",
    "text": "Example\nLet the random process rolling a fair, six-sided die. Let \\(X\\) a random variable representing the value of the die.\n\nFor each of the following, determine the outcome(s) and event under consideration, along with the value of the probability itself:\n\n\\(\\text{Pr}(X = 1)\\)\n\\(\\text{Pr}(X = 1 \\text{ and } 2)\\)\n\\(\\text{Pr}(X \\text{ is even})\\)\n\n\n\nConvince ourselves that \\(X\\) is a RV. Recall: sample space is 1,..,6.\nPossible outcome is 1, and an event would be \\(X=1\\); the RV being 1.\nPossible event is 1 or 2."
  },
  {
    "objectID": "slides/slides-08-probability.html#operations-with-events",
    "href": "slides/slides-08-probability.html#operations-with-events",
    "title": "Probability basics",
    "section": "Operations with events",
    "text": "Operations with events\nLet \\(A\\) and \\(B\\) be two possible events.\n\nThe intersection of \\(A\\) and \\(B\\) is denoted as \\(A \\cap B\\), and is the set of outcomes that belong to both events \\(A\\) and \\(B\\)\nThe union of \\(A\\) and \\(B\\) is denoted as \\(A \\cup B\\), and is the set of outcomes that belong to \\(A\\) and/or \\(B\\)\n\n\nWhen we have only two or three events, Venn diagrams can be very useful for visualizing probabilities!"
  },
  {
    "objectID": "slides/slides-08-probability.html#addition-rule",
    "href": "slides/slides-08-probability.html#addition-rule",
    "title": "Probability basics",
    "section": "Addition rule",
    "text": "Addition rule\nLet \\(A\\) and \\(B\\) be two possible events. Then the addition rule states that the probability that at least one will occur is:\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\\]\n\nVenn diagram\nExample: in a standard deck of 52 cards, we have four suits (diamond, heart, club, spade) with 13 cards within each suit (1-10, Jack, Queen, King).\n\nSuppose we randomly draw one card from the shuffled deck.\nLet \\(A\\) be the event that the card is a spade.\nLet \\(B\\) be the event that the card is a face card (Jack, Queen or King).\nFind \\(P(A \\cup B)\\)."
  },
  {
    "objectID": "slides/slides-08-probability.html#disjoint-events",
    "href": "slides/slides-08-probability.html#disjoint-events",
    "title": "Probability basics",
    "section": "Disjoint events",
    "text": "Disjoint events\nTwo events are disjoint or mutually exclusive if they cannot simultaneously happen.\n\nThat is, if \\(A\\) and \\(B\\) are disjoint, then \\(\\text{Pr}(A \\cap B) = ?\\)\n\nIf our random process is rolling a six-sided die one time, what are some examples of disjoint events?"
  },
  {
    "objectID": "slides/slides-08-probability.html#rules-of-probability",
    "href": "slides/slides-08-probability.html#rules-of-probability",
    "title": "Probability basics",
    "section": "Rules of probability",
    "text": "Rules of probability\nKolmogorov axioms\n\nThe probability of any event is non-negative real number\nThe probability of the entire sample space 1\nIf \\(A\\) and \\(B\\) are disjoint, then \\(\\text{Pr}(A \\cup B) = \\text{Pr}(A) + \\text{Pr}(B)\\)\n\n\nThese axioms imply that all probabilities are between 0 and 1 inclusive, and lead to some important rules!"
  },
  {
    "objectID": "slides/slides-08-probability.html#probability-distributions",
    "href": "slides/slides-08-probability.html#probability-distributions",
    "title": "Probability basics",
    "section": "Probability distributions",
    "text": "Probability distributions\nWhen a random variable is discrete, it can be useful to discuss its probability distribution, which is a table of all (disjoint) outcomes and their associated probabilities.\n\n\nLet \\(X\\) be the sum of two fair, six-sided dice. What is the sample space \\(S\\)?\nFill out the table below to display the probability distribution of \\(X\\):\n\n\n\n\n\n\n\\(X\\)\n2\n3\n4\n5\n6\n7\n\n\nProbability\n\n\n\n\n\n\n\n\n\\(X\\)\n8\n9\n10\n11\n12\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n\n\nWhy not include 1 or 13?"
  },
  {
    "objectID": "slides/slides-08-probability.html#probability-distributions-cont.",
    "href": "slides/slides-08-probability.html#probability-distributions-cont.",
    "title": "Probability basics",
    "section": "Probability distributions (cont.)",
    "text": "Probability distributions (cont.)\nThe probability distribution of a discrete random variable must satisfy the following three rules:\n\nThe outcomes listed must be disjoint\nEach probability must be between 0 and 1 (inclusive)\nThe probabilities must sum to 1\n\n\nLet’s confirm that the distribution we found on the previous slide satisfies these rules!"
  },
  {
    "objectID": "slides/slides-08-probability.html#complement",
    "href": "slides/slides-08-probability.html#complement",
    "title": "Probability basics",
    "section": "Complement",
    "text": "Complement\n\nThe complement of an event \\(A\\) is the set of all outcomes in \\(S\\) that are not in \\(A\\)\n\nDenoted as \\(A^c\\)\n\nContinuing the dice example, if \\(A\\) is the event that a 1 or 2 is rolled, what is \\(A^c\\)?\nComplement rule: \\(\\text{Pr}(A^c) = 1 - \\text{Pr}(A)\\)\n\nLet our random process be the sum of two dice. What is the probability that…\n\nthe sum of the dice is \\(not\\) 6?\nthe sum is at least 4?"
  },
  {
    "objectID": "slides/slides-08-probability.html#demorgans-laws",
    "href": "slides/slides-08-probability.html#demorgans-laws",
    "title": "Probability basics",
    "section": "DeMorgan’s Laws",
    "text": "DeMorgan’s Laws\nLet’s use Venn diagrams to try and determine formulas for the following:\n\nComplement of union: \\((A \\cup B)^c = \\ ?\\)\nComplement of intersection: \\((A \\cap B)^c = \\ ?\\)"
  },
  {
    "objectID": "slides/slides-08-probability.html#independence",
    "href": "slides/slides-08-probability.html#independence",
    "title": "Probability basics",
    "section": "Independence",
    "text": "Independence\n\nQualitatively, two processes are independent if knowing the outcome of one does not provide any information about the outcome of the other process\n\nExamples and non-examples? How to formalize this?\n\nIf \\(A\\) and \\(B\\) are independent events from two different and independent processes, then \\(\\text{Pr}(A \\cap B) = \\text{Pr}(A) \\times \\text{Pr}(B)\\)\nMore generally, if \\(\\text{Pr}(A \\cap B) = \\text{Pr}(A) \\times \\text{Pr}(B)\\) and \\(A\\) and \\(B\\) are events from the same process, then \\(A\\) and \\(B\\) are independent.\n\nThis is known as the multiplication rule for independent events"
  },
  {
    "objectID": "slides/slides-08-probability.html#practice",
    "href": "slides/slides-08-probability.html#practice",
    "title": "Probability basics",
    "section": "Practice",
    "text": "Practice\n\nA Pew Research survey asked 2,373 randomly sampled registered voters their political affiliation (Republican, Democrat, or Independent) and whether or not they identify as swing voters. 35% of respondents identified as Independent, 23% identified as swing voters, and 11% identified as both.\n\nWhat percent of voters are Independent but not swing voters?\nWhat percent of voters are Independent or swing voters?\nWhat percent of voters are neither Independent nor swing voters?\nIs the event that someone is a swing voter independent of the event that someone is a political Independent?"
  },
  {
    "objectID": "slides/slides-08-probability.html#more-practice",
    "href": "slides/slides-08-probability.html#more-practice",
    "title": "Probability basics",
    "section": "More practice",
    "text": "More practice\n\nA Pew Research survey asked 2,373 randomly sampled registered voters their political afilliation (Republican, Democrat, or Independent) and whether or not they identify as swing voters. 35% of respondents identified as Independent, 23% identified as swing voters, and 11% identified as both.\n\nWhat percent of voters are Independent but not swing voters?\nWhat percent of voters are Independent or swing voters?\nWhat percent of voters are neither Independent nor swing voters?\nIs the event that someone is a swing voter independent of the event that someone is a political Independent?"
  },
  {
    "objectID": "slides/slides-09-conditional-probability.html#probabilities-with-contingency-tables",
    "href": "slides/slides-09-conditional-probability.html#probabilities-with-contingency-tables",
    "title": "Conditional probability",
    "section": "Probabilities with contingency tables",
    "text": "Probabilities with contingency tables\n\nAs we saw in the previous class, sometimes the probabilities of events are quite clear to calculate (e.g. dice rolls or drawing cards)\nBut oftentimes we have to use data to try and estimate probabilities\n\nWhy? Some probabilities are not known, and we use proportions from data as a proxy\n\nWhen we have two (or more) variables, we often want to understand the relationships between them (e.g. \\(A \\cap B\\))"
  },
  {
    "objectID": "slides/slides-09-conditional-probability.html#practice",
    "href": "slides/slides-09-conditional-probability.html#practice",
    "title": "Conditional probability",
    "section": "Practice",
    "text": "Practice\n\nSource: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5788283/\n\n\n\n\n\nDid not die\nDied\nTotal\n\n\n\n\nDoes not drink coffee\n5438\n1039\n6477\n\n\nDrinks coffee occasionally\n29712\n4440\n34152\n\n\nDrinks coffee regularly\n24934\n3601\n28535\n\n\nTotal\n60084\n9080\n69164\n\n\n\n\n\n\nDefine events \\(A\\) = died and \\(B\\) = non-coffee drinker. Calculate/set-up the calculations for the following for a randomly selected person in the cohort:\n\n\\(\\text{P}(A)\\)\n\\(\\text{P}(A \\cap B)\\)\n\\(\\text{P}(A \\cup B^c)\\)"
  },
  {
    "objectID": "slides/slides-09-conditional-probability.html#marginal-and-joint-probabilities",
    "href": "slides/slides-09-conditional-probability.html#marginal-and-joint-probabilities",
    "title": "Conditional probability",
    "section": "Marginal and joint probabilities",
    "text": "Marginal and joint probabilities\n\n\\(\\text{P}(A)\\) is an example of a marginal probability, which is a probability involving a single event\n\nFrom the contingency table, we use row totals or column totals and the overall total to obtain marginal probabilities\n\n\\(\\text{P}(A \\cap B)\\) and \\(\\text{P}(A \\cup B^c)\\) are examples of a joint probability, which is a probability involving two or more events that have yet to occur\n\nFrom the contingency table, we use specific cells and the overall total to obtain joint probabilities"
  },
  {
    "objectID": "slides/slides-09-conditional-probability.html#marginal-from-joint",
    "href": "slides/slides-09-conditional-probability.html#marginal-from-joint",
    "title": "Conditional probability",
    "section": "Marginal from joint",
    "text": "Marginal from joint\nUsing LoTP, we can obtain the marginal probabilities from joint probabilities (which some of you intuitively did)!\n\n\n\n\nDid not die\nDied\nTotal\n\n\n\n\nDoes not drink coffee\n5438\n1039\n6477\n\n\nDrinks coffee occasionally\n29712\n4440\n34152\n\n\nDrinks coffee regularly\n24934\n3601\n28535\n\n\nTotal\n60084\n9080\n69164\n\n\n\n\n\\[\\begin{align*}\n\\text{P}(B) &=\\text{P}(\\text{no coffee}) \\\\\n&\\overset{\\text{LoTP}}{=} \\text{P}(\\text{no coffee} \\ \\cap \\text{ did not die}) + \\text{P}(\\text{no coffee} \\ \\cap \\text{ died})  \\\\\n&= \\text{P}(B \\cap A) + \\text{P}(B \\cap A^c) \\\\\n&= \\frac{5438}{69164 } + \\frac{1039}{69164} \\\\\n&= 0.0936\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/slides-09-conditional-probability.html#conditional-probability",
    "href": "slides/slides-09-conditional-probability.html#conditional-probability",
    "title": "Conditional probability",
    "section": "Conditional probability",
    "text": "Conditional probability\n\nConditional probability: a probability that an event will occur given that another event has already occurred\n\nE.g. Given that it rained yesterday, what is the probability that it will rain today?\nIt is called “conditional” because we calculate a probability under a specific condition\n\n\n\n\\(\\text{Pr}(A | B)\\) : probability of \\(A\\) given \\(B\\)\n\nNot to be confused with the coding | which is “or”\nAppears to involve two events, but we assume that the event that is conditioned on (in this case \\(B\\)) has already happened\n\nWe can easily obtain conditional probabilities from contingency tables!"
  },
  {
    "objectID": "slides/slides-09-conditional-probability.html#conditional-probability-with-contingency-tables",
    "href": "slides/slides-09-conditional-probability.html#conditional-probability-with-contingency-tables",
    "title": "Conditional probability",
    "section": "Conditional probability with contingency tables",
    "text": "Conditional probability with contingency tables\n\n\n\n\nDid not die\nDied\nTotal\n\n\n\n\nDoes not drink coffee\n5438\n1039\n6477\n\n\nDrinks coffee occasionally\n29712\n4440\n34152\n\n\nDrinks coffee regularly\n24934\n3601\n28535\n\n\nTotal\n60084\n9080\n69164\n\n\n\n\nFrom contingency table, we use specific cells and row or column totals to obtain conditional probabilities\n\n\n\nRecall events \\(A\\) = died and \\(B\\) = non-coffee drinker. Write \\(\\text{P}()\\) notation for the conditional probability of dying given that someone does not drink coffee, and then obtain this probability."
  },
  {
    "objectID": "slides/slides-09-conditional-probability.html#conditional-probability-formula",
    "href": "slides/slides-09-conditional-probability.html#conditional-probability-formula",
    "title": "Conditional probability",
    "section": "Conditional probability formula",
    "text": "Conditional probability formula\nWe can re-arrange the general multiplication formula to obtain the following general formula for conditional probability. For any events \\(A\\) and \\(B\\):\n\n\\[\n\\text{P}(A| B) = \\frac{\\text{P}(A \\cap B)}{\\text{P}(B)}\n\\]\n\n\n\nCome up with a similar formula for \\(\\text{P}(B|A)\\)\n\n\nNote: complement rule holds for conditional probabilities if we condition on the same information: \\(\\text{P}(A|B) = 1 - \\text{P}(A^c | B)\\)"
  },
  {
    "objectID": "slides/slides-09-conditional-probability.html#independence-and-conditional-probabilities",
    "href": "slides/slides-09-conditional-probability.html#independence-and-conditional-probabilities",
    "title": "Conditional probability",
    "section": "Independence and conditional probabilities",
    "text": "Independence and conditional probabilities\n\nRecall, events \\(A\\) and \\(B\\) are independent when what is true about their joint probability?\nUsing the general multiplication rule, what is another way to determine if events \\(A\\) and \\(B\\) are independent?\n\nWhy does this make sense “intuitively”?\n\n\nUsing this new test of independence, are dying and abstaining from coffee independent events?"
  },
  {
    "objectID": "slides/slides-09-conditional-probability.html#bayes-rule-1",
    "href": "slides/slides-09-conditional-probability.html#bayes-rule-1",
    "title": "Conditional probability",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\n\nAs we saw before, the two conditional probabilities \\(P(A|B)\\) and \\(P(B|A)\\) are not the same. But are they related in some way?\nBayes’ rule:\n\n\n\\[\n\\text{P}(A|B) =\n\\]\n\n\nWhy is this seemingly more complicated formula useful?"
  },
  {
    "objectID": "slides/slides-09-conditional-probability.html#bayes-theorem-more-general",
    "href": "slides/slides-09-conditional-probability.html#bayes-theorem-more-general",
    "title": "Conditional probability",
    "section": "Bayes’ Theorem (more general)",
    "text": "Bayes’ Theorem (more general)\n\nSuppose we have a random process and have a defined event \\(A\\)\nFurther suppose we can break up the sample space into \\(k\\) disjoint/mutually exclusive outcomes or events \\(B_{1}, B_{2}, \\ldots, B_{k}\\)\nWithout loss of generality, suppose we want \\(\\text{P}(B_{1} | A)\\)\nBayes’ Theorem states:\n\\[\\begin{align*}\n\\text{P}(B_{1} |  A ) &= \\frac{\\text{P}(A|B_{1}) \\text{P}(B_{1})}{\\text{P}(A)}\\qquad \\qquad\\qquad \\qquad \\text{(Bayes' Rule)} \\\\\n&= \\frac{\\text{P}(A|B_{1})\\text{P}(B_{1})}{\\text{P}(A\\cap B_{1}) + \\text{P}(A \\cap B_{2}) + \\ldots + \\text{P}(A \\cap B_{k})} \\qquad \\qquad \\text{(LoTP)} \\\\\n&=\\frac{\\text{P}(A|B_{1}) \\text{P}(B_{1})}{\\text{P}(A|B_{1}) \\text{P}(B_{1}) + \\text{P}(A | B_{2}) \\text{P}(B_{2}) + \\ldots + \\text{P}(A | B_{k} ) \\text{P}(B_{k})}\n\\end{align*}\\]\n\n\nHow would this change if we wanted \\(P(A_{2} | B)\\) instead?\nWhy is this important? We want P(B_i | A), but sometimes we only have probabilities in the other order of conditioning!"
  },
  {
    "objectID": "slides/slides-09-conditional-probability.html#diagnostic-testing-example",
    "href": "slides/slides-09-conditional-probability.html#diagnostic-testing-example",
    "title": "Conditional probability",
    "section": "Diagnostic testing example",
    "text": "Diagnostic testing example\nSuppose we are interested in the performance of a medical diagnostic test. Let \\(D\\) be the event that a patient has the disease, and let \\(T\\) be the event that the test is positive for the disease.\n\nSome definitions (don’t worry about memorizing):\n\nPrevalence: \\(P(D)\\)\nSensitivity of test: \\(P(T|D)\\)\nSpecificity of test: \\(P(T^c | D^c)\\)\nPositive predictive value: \\(P(D | T)\\)\nNegative predictive value: \\(P(D^c | T^c)\\)\n\n\nWhat do these probabilities mean in plain English? Which ones do we hope are low? Which ones do we hope are high?\nWhich probability would you be most interested in knowing?"
  },
  {
    "objectID": "slides/slides-09-conditional-probability.html#example",
    "href": "slides/slides-09-conditional-probability.html#example",
    "title": "Conditional probability",
    "section": "Example",
    "text": "Example\n\nIn Canada, about 0.35% of women over 40 will develop breast cancer in any given year. A common screening test for cancer is the mammogram, but this test is not perfect.\nIn about 11% of patients with breast cancer, the test gives a false negative: it indicates a woman does not have breast cancer when she does have breast cancer.\nIn about 7% of patients who do not have breast cancer, the test gives a false positive: it indicates these patients have breast cancer when they actually do not.\nIf we tested a random Canadian woman over 40 for breast cancer using a mammogram and the test came back positive, what is the probability that the patient actually has breast cancer?"
  },
  {
    "objectID": "slides/slides-10-simpsons.html#study-design",
    "href": "slides/slides-10-simpsons.html#study-design",
    "title": "Simpson’s paradox",
    "section": "Study design",
    "text": "Study design\n\nWhat are the differences between observational studies and experimental studies?\nWhat is a confounding variable?"
  },
  {
    "objectID": "slides/slides-10-simpsons.html#probability",
    "href": "slides/slides-10-simpsons.html#probability",
    "title": "Simpson’s paradox",
    "section": "Probability",
    "text": "Probability\nObservational study on sex bias based on Fall 1973 admissions data to the graduate program at the University of California, Berkeley\n\nRows: applicant gender. Columns: application results.\n\n\n\nAdmit\nDeny\nTotal\n\n\n\n\nMen\n3738\n4704\n8442\n\n\nWomen\n1494\n2827\n4321\n\n\nTotal\n5232\n7531\n12763\n\n\n\n\n\nWhat is the probability of admission for a randomly selected applicant?\nWhat is the probability of admission among men? Among women?\nAre the probabilities you found marginal, joint, or conditional probabilities?\n\n\n\n\nSuppose we want to understand the relationship between gender and admission decision. What sort of visualization might be appropriate for representing this data?"
  },
  {
    "objectID": "slides/slides-10-simpsons.html#dive-into-data",
    "href": "slides/slides-10-simpsons.html#dive-into-data",
    "title": "Simpson’s paradox",
    "section": "Dive into data",
    "text": "Dive into data\nWe have more nuanced data about the graduate admissions: we know the department that each person was applied to.\nWe will consider the six largest departments: A, B, C, D, E, F\n\nThe first six observations in the data frame are as follows:\n\n\n\nhead(admissions)\n\n# A tibble: 6 × 3\n  Decision Gender Dept \n  <chr>    <chr>  <chr>\n1 Admit    Male   B    \n2 Reject   Female C    \n3 Admit    Male   C    \n4 Reject   Female C    \n5 Admit    Male   A    \n6 Reject   Male   F    \n\n\n\n\n\nWhat sort of EDA would be interesting/appropriate for these data?"
  },
  {
    "objectID": "slides/slides-10-simpsons.html#live-code",
    "href": "slides/slides-10-simpsons.html#live-code",
    "title": "Simpson’s paradox",
    "section": "Live code",
    "text": "Live code\n\n\nFemale applicants:\n\n\n\n\n\nDept\nDecision\nn\n\n\n\n\nA\nAdmit\n89\n\n\nA\nReject\n19\n\n\nB\nAdmit\n17\n\n\nB\nReject\n8\n\n\nC\nAdmit\n202\n\n\nC\nReject\n391\n\n\nD\nAdmit\n131\n\n\nD\nReject\n244\n\n\nE\nAdmit\n94\n\n\nE\nReject\n299\n\n\nF\nAdmit\n24\n\n\nF\nReject\n317\n\n\n\n\n\n\nMale applicants:\n\n\n\n\n\nDept\nDecision\nn\n\n\n\n\nA\nAdmit\n512\n\n\nA\nReject\n313\n\n\nB\nAdmit\n353\n\n\nB\nReject\n207\n\n\nC\nAdmit\n120\n\n\nC\nReject\n205\n\n\nD\nAdmit\n138\n\n\nD\nReject\n279\n\n\nE\nAdmit\n53\n\n\nE\nReject\n138\n\n\nF\nAdmit\n22\n\n\nF\nReject\n351"
  },
  {
    "objectID": "slides/slides-10-simpsons.html#visualize",
    "href": "slides/slides-10-simpsons.html#visualize",
    "title": "Simpson’s paradox",
    "section": "Visualize",
    "text": "Visualize"
  },
  {
    "objectID": "slides/slides-10-simpsons.html#closer-look",
    "href": "slides/slides-10-simpsons.html#closer-look",
    "title": "Simpson’s paradox",
    "section": "Closer look",
    "text": "Closer look\nProbability of admission conditioning on gender and department:\n\n\n\n\n\n\n \n  \n    Dept \n    Gender \n    cond_prob_admit \n  \n \n\n  \n    A \n    Female \n    0.82 \n  \n  \n    A \n    Male \n    0.62 \n  \n  \n    B \n    Female \n    0.68 \n  \n  \n    B \n    Male \n    0.63 \n  \n  \n    C \n    Female \n    0.34 \n  \n  \n    C \n    Male \n    0.37 \n  \n  \n    D \n    Female \n    0.35 \n  \n  \n    D \n    Male \n    0.33 \n  \n  \n    E \n    Female \n    0.24 \n  \n  \n    E \n    Male \n    0.28 \n  \n  \n    F \n    Female \n    0.07 \n  \n  \n    F \n    Male \n    0.06 \n  \n\n\n\n\n\n\n\n\nAre all departments uniform in admission rates?\nDo admissions still seem biased against female applicants?"
  },
  {
    "objectID": "slides/slides-10-simpsons.html#whats-going-on",
    "href": "slides/slides-10-simpsons.html#whats-going-on",
    "title": "Simpson’s paradox",
    "section": "What’s going on?",
    "text": "What’s going on?\n\n\n\nBut wait… didn’t we start by noting that men were way more likely to be admitted than women?\nThe first two departments (A and B) are easy to get into\nThe following table shows for each gender, the proportion of applicants each department received.\n\n\n\n\n\n\n\n \n  \n    Gender \n    Dept \n    cond_prop \n  \n \n\n  \n    Female \n    A \n    0.059 \n  \n  \n    Female \n    B \n    0.014 \n  \n  \n    Female \n    C \n    0.323 \n  \n  \n    Female \n    D \n    0.204 \n  \n  \n    Female \n    E \n    0.214 \n  \n  \n    Female \n    F \n    0.186 \n  \n  \n    Male \n    A \n    0.307 \n  \n  \n    Male \n    B \n    0.208 \n  \n  \n    Male \n    C \n    0.121 \n  \n  \n    Male \n    D \n    0.155 \n  \n  \n    Male \n    E \n    0.071 \n  \n  \n    Male \n    F \n    0.139 \n  \n\n\n\n\n\n\n\n\nWhat do you notice?"
  },
  {
    "objectID": "slides/slides-10-simpsons.html#simpsons-paradox",
    "href": "slides/slides-10-simpsons.html#simpsons-paradox",
    "title": "Simpson’s paradox",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox\nThe UC Berkeley admissions observational study is an example of Simpson’s paradox: when omitting one explanatory variable causes the measure/degree of association between another explanatory variable and a response variable to reverse or disappear\n\nIn other words, the inclusion/exclusion of a third variable in the analysis can change the apparent relationship between the other two variables\nWhat was the confounding variable in UC Berkeley study?"
  },
  {
    "objectID": "practice_probs/practice-08-probability.html",
    "href": "practice_probs/practice-08-probability.html",
    "title": "Probability",
    "section": "",
    "text": "If events \\(A\\) and \\(B\\) are disjoint, what is a simple formula for \\(P(A \\cup B)\\)?\n(\\(^*\\)) The American Community Survey (ACS) is an ongoing survey that provides data every year to give communities the current information they need to plan investments and services. The 2010 ACS estimated that 14.6% of Americans live below the poverty line, 20.7% speak a language other than English (i.e. a foreign language) at home, and 4.2% fall into both categories.\n\nAre living below the poverty line and speaking a foreign language at home disjoint?\nDraw a Venn diagram summarizing the probabilities and their associated probabilities. Be sure to complete the diagram by including a ``bounding box”.\nWhat percent of Americans live below the poverty line and only speak English at home?\nWhat percent of Americans live below the poverty line or speak a foreign language at home?\nWhat percent of Americans live below the poverty line and only speak English at home?\nIs the event that someone lives below the poverty line independent of the event that the person speaks a foreign language at home?\n\nIn a multiple choice exam, there are 5 questions and 4 choices for each question. Nancy has not studied for the exam at all and decides to randomly guess the answers. What is the probability that:\n\nthe first question Nancy gets correct is the 5th question? State any assumptions that you make.\nNancy gets all of the questions right?\nNancy gets at least one question right?"
  },
  {
    "objectID": "practice_probs/practice-09-conditional-probability.html",
    "href": "practice_probs/practice-09-conditional-probability.html",
    "title": "Conditional probability",
    "section": "",
    "text": "Suppose we have an event \\(A\\) from one random process and an event \\(B\\) from a second random process such that \\(P(A) = 0.3\\), \\(P(B) = 0.7\\), and \\(P(A \\cap B) = 0.1\\).\n\nAre the random processes independent?\nWhat is \\(P(A|B)\\)?\n\nAssortative mating is a nonrandom mating pattern where individuals with similar genotypes and/or phenotypes mate with one another more frequently than what would be expected under a random mating pattern. Researchers studying this topic collected the following data on eye colors of 204 Scandinavian men and their female partners. For simplicity, we only include heterosexual relationships in this exercise.\n\n\nFind the probability that a randomly chosen male respondent or his partner has blue eyes.\nWhat is the probability that a randomly chosen male respondent with blue eyes has a partner with blue eyes?\nWhat is the probability that a randomly chosen male respondent with brown eyes has a partner with blue eyes? What about the probability of a randomly chosen male respondent with green eyes having a partner with blue eyes?\nDoes it appear that the eye colors of male respondents and their partners are independent? Explain your reasoning.\n\n\\(^*\\) To get to Middlebury College, a professor uses their car 30% of the time, walks 20% of the time, and bikes 50% of the time. They are late 5% when walking, 10% of the time when driving (because this is Vermont and people stop for all pedestrians), and 2% of the time when biking.\n\nWhat is the probability the professor drove to work if they were late?\nWhat is the probability the professor walked to work if they were on time?"
  },
  {
    "objectID": "live_code/data_wrangling_viz.html",
    "href": "live_code/data_wrangling_viz.html",
    "title": "Group data wrangling",
    "section": "",
    "text": "We will now work a larger subset of the Kaggle data science survey data!"
  },
  {
    "objectID": "live_code/data_wrangling_viz.html#warm-up-exercises",
    "href": "live_code/data_wrangling_viz.html#warm-up-exercises",
    "title": "More data wrangling",
    "section": "Warm-up exercises",
    "text": "Warm-up exercises\nHow many different programming languages were recommended in the survey?\n\n\n\nHow many of the respondents who work in academia in the United States are at most 25 years old at the time taking the survey?"
  },
  {
    "objectID": "live_code/data_wrangling_viz.html#group-analysis",
    "href": "live_code/data_wrangling_viz.html#group-analysis",
    "title": "Group data wrangling",
    "section": "Group analysis",
    "text": "Group analysis\nI want your group to generate your own investigation. Using your data-wrangling and plotting skills to do some EDA. After about a half hour, your group will share your process and results with the rest of the class!\nYour final results must include:\n\nA meaningful use of group_by()\nSummary statistics or frequency table\nVisualization with meaningful labels/titles\n\nYou can create more than one visualization and/or more than one table. Whatever speaks to you! The individual components (i.e. table/summary stats vs plot) do not need to use the same set of variables. Feel free to create as many code chunks as you’d like! There is a data dictionary at the bottom of this page that defines all the variables in the data set for you."
  },
  {
    "objectID": "live_code/data_wrangling_viz.html#data-dictionary",
    "href": "live_code/data_wrangling_viz.html#data-dictionary",
    "title": "Group data wrangling",
    "section": "Data dictionary",
    "text": "Data dictionary\nBelow is the data dictionary for the subset of the Kaggle data data.\n\nCountry: home country of employee (character)\nGender: specified gender (character)\nAge: age at time of survey (numeric)\nEmploymentStatus: reported employed status (character)\nEmploymerIndustry: employer’s industry (character)\nMajor: college major (character)\nCompensationAmount: annual compensation (numeric)\nCompensationCurrency: three-letter currency code (character)\nCurrentJobTitle: job title (character)\nTitleFit: assessment of how well the job title fits (“Fine”, “Perfectly”, “Poorly”)\nLanguageRecommendation: recommended programming language (character)\nWorkDataVisualizations: proportion of job dedicated to creating data visualizations, broken into pre-determined categories (character)\nJobSatisfaction: rating of job satisfaction on scale of 1-10, where 1 is not satisfied and 10 is highly satisfied (character)\nJobSatisfaction2: numeric version of JobSatisfaction (numeric)\nConversionUSD: conversion factor from CompensationCurrency to USD (numeric)"
  },
  {
    "objectID": "live_code/data_wrangling.html",
    "href": "live_code/data_wrangling.html",
    "title": "Data wrangling with dplyr",
    "section": "",
    "text": "Recall that we are looking at data provided by Kaggle. In 2017, Kaggle conducted an industry-wide survey to establish a comprehensive view of the state of data science and machine learning. We will be looking at just a subset of the data.\nBy default, all dplyr functions expect the first argument to be a data frame."
  },
  {
    "objectID": "live_code/data_wrangling.html#selecting-columns",
    "href": "live_code/data_wrangling.html#selecting-columns",
    "title": "Data wrangling with dplyr",
    "section": "Selecting columns",
    "text": "Selecting columns\nSometimes, there are a lot of columns in a data frame and we might not want all of them. The select() function gives us an easy way to choose which columns/variables we’d like to work with.\nThe select() function requires by default two arguments: the data frame and the variable names to choose from that data frame.\nThe following code works…\n\nselect(datascience, Age)\n\n# A tibble: 2,288 × 1\n     Age\n   <dbl>\n 1    56\n 2    33\n 3    26\n 4    25\n 5    33\n 6    22\n 7    29\n 8    35\n 9    37\n10    31\n# ℹ 2,278 more rows\n\n\n…but it’s preferable to take advantage of piping in order to make code more readable:\n\ndatascience |>\n  select(Age)\n\n# A tibble: 2,288 × 1\n     Age\n   <dbl>\n 1    56\n 2    33\n 3    26\n 4    25\n 5    33\n 6    22\n 7    29\n 8    35\n 9    37\n10    31\n# ℹ 2,278 more rows\n\n\n\n\nWhat’s going on here?\n\nStart with the data frame datascience\nPipe (|>) the data frame to the select() function and specify that we want the variable Age\nThe result is a data frame with 2288 rows and 1 column with the Age variable\n\n\n\n\n\n\n\nCheck\n\n\n\nWhy do we type Age and not age?\n\n\n\nMultiple variables and excluding\n\n\n\n\n\n\nExpand\n\n\n\n\n\n\ndatascience |>\n  select(Age, Major)\n\n# A tibble: 2,288 × 2\n     Age Major                                                       \n   <dbl> <chr>                                                       \n 1    56 Mathematics or statistics                                   \n 2    33 Other                                                       \n 3    26 Computer Science                                            \n 4    25 Physics                                                     \n 5    33 Electrical Engineering                                      \n 6    22 Information technology, networking, or system administration\n 7    29 Computer Science                                            \n 8    35 Physics                                                     \n 9    37 Electrical Engineering                                      \n10    31 Computer Science                                            \n# ℹ 2,278 more rows\n\n\n\n\n\n\n\n\nCheck\n\n\n\nWhat happens if we swap the order of the variable names?\n\n\n\nA range of variables\n\ndatascience |>\n  select(Gender:EmploymentStatus)\n\n# A tibble: 2,288 × 3\n   Gender   Age EmploymentStatus                                    \n   <chr>  <dbl> <chr>                                               \n 1 Male      56 Independent contractor, freelancer, or self-employed\n 2 Male      33 Employed full-time                                  \n 3 Male      26 Employed full-time                                  \n 4 Male      25 Employed part-time                                  \n 5 Male      33 Employed full-time                                  \n 6 Male      22 Employed full-time                                  \n 7 Male      29 Employed full-time                                  \n 8 Male      35 Employed full-time                                  \n 9 Male      37 Employed full-time                                  \n10 Male      31 Employed part-time                                  \n# ℹ 2,278 more rows\n\n\n\n\nExcluding variables\n\ndatascience |>\n  select(-Country)\n\n# A tibble: 2,288 × 16\n   Gender   Age EmploymentStatus          EmployerIndustry FormalEducation Major\n   <chr>  <dbl> <chr>                     <chr>            <chr>           <chr>\n 1 Male      56 Independent contractor, … Mix of fields    Master's degree Math…\n 2 Male      33 Employed full-time        Internet-based   Bachelor's deg… Other\n 3 Male      26 Employed full-time        Financial        Master's degree Comp…\n 4 Male      25 Employed part-time        Academic         Bachelor's deg… Phys…\n 5 Male      33 Employed full-time        Telecommunicati… Doctoral degree Elec…\n 6 Male      22 Employed full-time        Mix of fields    Bachelor's deg… Info…\n 7 Male      29 Employed full-time        Pharmaceutical   Master's degree Comp…\n 8 Male      35 Employed full-time        Technology       Doctoral degree Phys…\n 9 Male      37 Employed full-time        Technology       Master's degree Elec…\n10 Male      31 Employed part-time        Technology       Doctoral degree Comp…\n# ℹ 2,278 more rows\n# ℹ 10 more variables: CompensationAmount <dbl>, CompensationCurrency <chr>,\n#   CurrentJobTitle <chr>, TitleFit <chr>, LanguageRecommendation <chr>,\n#   DataScienceIdentity <chr>, WorkDataVisualizations <chr>,\n#   JobSatisfaction <chr>, JobSatisfaction2 <dbl>, ConversionUSD <dbl>"
  },
  {
    "objectID": "live_code/data_wrangling.html#arranging-rows",
    "href": "live_code/data_wrangling.html#arranging-rows",
    "title": "Data wrangling with dplyr",
    "section": "Arranging rows",
    "text": "Arranging rows\nWe might want to re-arrange rows in ascending or descending order according to a certain variable. The arrange() function does this, and requires specifying at least one variable to arrange by:\n\ndatascience |>\n  select(Age, Major) |>\n  arrange(Age)\n\n# A tibble: 2,288 × 2\n     Age Major                                                       \n   <dbl> <chr>                                                       \n 1     0 Mathematics or statistics                                   \n 2     1 Other                                                       \n 3    19 Computer Science                                            \n 4    19 Biology                                                     \n 5    20 Information technology, networking, or system administration\n 6    20 Mathematics or statistics                                   \n 7    20 Computer Science                                            \n 8    20 Mathematics or statistics                                   \n 9    21 Other                                                       \n10    21 Computer Science                                            \n# ℹ 2,278 more rows\n\n\n\n\nBy default, arrange() will reorder in ascending order. If we’d like to go in descending order, we can code arrange(desc(Age))."
  },
  {
    "objectID": "live_code/data_wrangling.html#slicing-for-certain-row-numbers",
    "href": "live_code/data_wrangling.html#slicing-for-certain-row-numbers",
    "title": "Data wrangling with dplyr",
    "section": "Slicing for certain row numbers",
    "text": "Slicing for certain row numbers\nRemember, data frames are in tabular format. So each row has a certain index, as does each column. The first row is index 1, the second row index 2, etc.\nThe slice() function expects a vector of row indices to retain:\n\ndatascience |>\n  slice(1:5)\n\n# A tibble: 5 × 17\n  Country   Gender   Age EmploymentStatus EmployerIndustry FormalEducation Major\n  <chr>     <chr>  <dbl> <chr>            <chr>            <chr>           <chr>\n1 United S… Male      56 Independent con… Mix of fields    Master's degree Math…\n2 Russia    Male      33 Employed full-t… Internet-based   Bachelor's deg… Other\n3 Taiwan    Male      26 Employed full-t… Financial        Master's degree Comp…\n4 United S… Male      25 Employed part-t… Academic         Bachelor's deg… Phys…\n5 United S… Male      33 Employed full-t… Telecommunicati… Doctoral degree Elec…\n# ℹ 10 more variables: CompensationAmount <dbl>, CompensationCurrency <chr>,\n#   CurrentJobTitle <chr>, TitleFit <chr>, LanguageRecommendation <chr>,\n#   DataScienceIdentity <chr>, WorkDataVisualizations <chr>,\n#   JobSatisfaction <chr>, JobSatisfaction2 <dbl>, ConversionUSD <dbl>\n\n\n\n\n\n\n\n\nCheck\n\n\n\nWhat is the difference between select() and slice()?"
  },
  {
    "objectID": "live_code/data_wrangling.html#filtering-to-select-a-subset-of-rows",
    "href": "live_code/data_wrangling.html#filtering-to-select-a-subset-of-rows",
    "title": "Data wrangling with dplyr",
    "section": "Filtering to select a subset of rows",
    "text": "Filtering to select a subset of rows\nThe slice() function is nice, but unless the rows of your data frame are ordered meaningfully, its actual utility is limited. We might want to look at a set of the cases in which a certain condition is met.\nIn the following code, we use the filter() function to only retain the observations where the person’s Major was Computer Science. This function requires specifying a logical condition, and keeps observations in which the condition is met (i.e. TRUE).\n\ndatascience |>\n  filter(Major == \"Computer Science\")\n\n# A tibble: 681 × 17\n   Country  Gender   Age EmploymentStatus EmployerIndustry FormalEducation Major\n   <chr>    <chr>  <dbl> <chr>            <chr>            <chr>           <chr>\n 1 Taiwan   Male      26 Employed full-t… Financial        Master's degree Comp…\n 2 Poland   Male      29 Employed full-t… Pharmaceutical   Master's degree Comp…\n 3 Iran     Male      31 Employed part-t… Technology       Doctoral degree Comp…\n 4 Brazil   Male      25 Employed full-t… Academic         Master's degree Comp…\n 5 Brazil   Male      32 Employed full-t… Academic         Master's degree Comp…\n 6 Russia   Male      31 Independent con… CRM/Marketing    Some college/u… Comp…\n 7 India    Male      23 Employed full-t… Technology       Master's degree Comp…\n 8 Canada   Male      52 Employed full-t… Academic         Bachelor's deg… Comp…\n 9 Russia   Male      26 Independent con… Military/Securi… Bachelor's deg… Comp…\n10 Czech R… Male      25 Independent con… Internet-based   Master's degree Comp…\n# ℹ 671 more rows\n# ℹ 10 more variables: CompensationAmount <dbl>, CompensationCurrency <chr>,\n#   CurrentJobTitle <chr>, TitleFit <chr>, LanguageRecommendation <chr>,\n#   DataScienceIdentity <chr>, WorkDataVisualizations <chr>,\n#   JobSatisfaction <chr>, JobSatisfaction2 <dbl>, ConversionUSD <dbl>\n\n\n\nMultiple conditions\n\n\n\n\n\n\nExpand\n\n\n\n\n\nWe can also filter for more than one condition at once. Within filter(), the comma , specifies that all conditions must be true. It can be read as “and”. In the following code, we retain cases where someone’s major was Computer Science and they were 30 years old at the time of filling out the survey.\n\ndatascience |>\n  filter(Major == \"Computer Science\", \n         Age == 30) |>\n  select(Major, Age)\n\n# A tibble: 36 × 2\n   Major              Age\n   <chr>            <dbl>\n 1 Computer Science    30\n 2 Computer Science    30\n 3 Computer Science    30\n 4 Computer Science    30\n 5 Computer Science    30\n 6 Computer Science    30\n 7 Computer Science    30\n 8 Computer Science    30\n 9 Computer Science    30\n10 Computer Science    30\n# ℹ 26 more rows\n\n\nIf we just need at least one of multiple conditions to be true, we can use the | operator which stands for “or”:\n\ndatascience |>\n  filter(Major == \"Computer Science\" | \n         Age == 30) |>\n  select(Major, Age)\n\n# A tibble: 765 × 2\n   Major              Age\n   <chr>            <dbl>\n 1 Computer Science    26\n 2 Computer Science    29\n 3 Computer Science    31\n 4 Computer Science    25\n 5 Computer Science    32\n 6 Computer Science    31\n 7 A social science    30\n 8 Computer Science    23\n 9 Biology             30\n10 Computer Science    52\n# ℹ 755 more rows\n\n\n\ndatascience |>\n  filter(Major == \"Computer Science\" | Major == \"Other\",\n         Age == 30) |>\n  select(Major, Age)\n\n# A tibble: 44 × 2\n   Major              Age\n   <chr>            <dbl>\n 1 Computer Science    30\n 2 Computer Science    30\n 3 Computer Science    30\n 4 Computer Science    30\n 5 Computer Science    30\n 6 Computer Science    30\n 7 Computer Science    30\n 8 Computer Science    30\n 9 Other               30\n10 Computer Science    30\n# ℹ 34 more rows"
  },
  {
    "objectID": "live_code/data_wrangling.html#distinct-to-filter-for-unique-rows",
    "href": "live_code/data_wrangling.html#distinct-to-filter-for-unique-rows",
    "title": "Data wrangling with dplyr",
    "section": "Distinct to filter for unique rows",
    "text": "Distinct to filter for unique rows\nThe distinct() function requires specifying variables in the data frame, and the function will keep only unique/distinct instances of the variable(s). Unless otherwise specified, it will drop all the other variables.\n\ndatascience |>\n  distinct(FormalEducation)\n\n# A tibble: 5 × 1\n  FormalEducation                                                  \n  <chr>                                                            \n1 Master's degree                                                  \n2 Bachelor's degree                                                \n3 Doctoral degree                                                  \n4 Some college/university study without earning a bachelor's degree\n5 I prefer not to answer                                           \n\ndatascience |>\n  distinct(FormalEducation, Major) |>\n  arrange(FormalEducation)\n\n# A tibble: 58 × 2\n   FormalEducation   Major                                                      \n   <chr>             <chr>                                                      \n 1 Bachelor's degree Other                                                      \n 2 Bachelor's degree Physics                                                    \n 3 Bachelor's degree Information technology, networking, or system administrati…\n 4 Bachelor's degree A social science                                           \n 5 Bachelor's degree Electrical Engineering                                     \n 6 Bachelor's degree Mathematics or statistics                                  \n 7 Bachelor's degree Computer Science                                           \n 8 Bachelor's degree Engineering (non-computer focused)                         \n 9 Bachelor's degree A humanities discipline                                    \n10 Bachelor's degree Management information systems                             \n# ℹ 48 more rows\n\n\n\n\n\n\n\n\nCheck\n\n\n\nWhat variables are by default included in the output from distinct()?"
  },
  {
    "objectID": "live_code/data_wrangling.html#mutate-to-add-a-new-variable",
    "href": "live_code/data_wrangling.html#mutate-to-add-a-new-variable",
    "title": "Data wrangling with dplyr",
    "section": "Mutate to add a new variable",
    "text": "Mutate to add a new variable\nIt is typical for us to want to add variables to a given data frame. We do this with the mutate() function. We must specify:\n\nThe name of the new variable and\nHow to calculate the value of that new variable for each observation. This will typically involve operations involving variables already present in the data frame.\n\nWe link the two with an equals sign.\n\ndatascience %>%\n  mutate(compensation_1k = CompensationAmount/1000) |>\n  select(CompensationAmount, compensation_1k)\n\n# A tibble: 2,288 × 2\n   CompensationAmount compensation_1k\n                <dbl>           <dbl>\n 1             250000             250\n 2            1200000            1200\n 3            1100000            1100\n 4              20000              20\n 5             100000             100\n 6             624000             624\n 7             126000             126\n 8             133000             133\n 9              80000              80\n10              15000              15\n# ℹ 2,278 more rows\n\n\n\n\n\n\n\n\nCheck\n\n\n\nWhat exactly is going on in the second line of code?"
  },
  {
    "objectID": "live_code/data_wrangling.html#counting-to-create-frequency-tables",
    "href": "live_code/data_wrangling.html#counting-to-create-frequency-tables",
    "title": "Data wrangling with dplyr",
    "section": "Counting to create frequency tables",
    "text": "Counting to create frequency tables\nWe can count the number of instances we observed each level of a given categorical variable:\n\ndatascience |>\n  count(EmployerIndustry)\n\n# A tibble: 16 × 2\n   EmployerIndustry                     n\n   <chr>                            <int>\n 1 Academic                           478\n 2 CRM/Marketing                       70\n 3 Financial                          211\n 4 Government                         137\n 5 Hospitality/Entertainment/Sports    27\n 6 Insurance                           68\n 7 Internet-based                     134\n 8 Manufacturing                       75\n 9 Military/Security                   35\n10 Mix of fields                      195\n11 Non-profit                          35\n12 Other                              198\n13 Pharmaceutical                      54\n14 Retail                              61\n15 Technology                         445\n16 Telecommunications                  65\n\n\n\n\n\n\n\n\nCheck\n\n\n\nHow does the resulting data frame from count() compare to the original data frame we passed in?\n\n\n\nMaking frequency tables useful\nWe typically want to present the counts in ascending or descending order.\n\n\n\n\n\n\nExpand\n\n\n\n\n\nNote that the following chunks of code do the same thing. One of them takes advantage of an additional argument in count(), whereas the other block of the uses an additional function:\n\ndatascience |>\n  count(EmployerIndustry, sort = T)\n\n# A tibble: 16 × 2\n   EmployerIndustry                     n\n   <chr>                            <int>\n 1 Academic                           478\n 2 Technology                         445\n 3 Financial                          211\n 4 Other                              198\n 5 Mix of fields                      195\n 6 Government                         137\n 7 Internet-based                     134\n 8 Manufacturing                       75\n 9 CRM/Marketing                       70\n10 Insurance                           68\n11 Telecommunications                  65\n12 Retail                              61\n13 Pharmaceutical                      54\n14 Military/Security                   35\n15 Non-profit                          35\n16 Hospitality/Entertainment/Sports    27\n\n\n\ndatascience |>\n  count(EmployerIndustry) |>\n  arrange(desc(n))\n\n# A tibble: 16 × 2\n   EmployerIndustry                     n\n   <chr>                            <int>\n 1 Academic                           478\n 2 Technology                         445\n 3 Financial                          211\n 4 Other                              198\n 5 Mix of fields                      195\n 6 Government                         137\n 7 Internet-based                     134\n 8 Manufacturing                       75\n 9 CRM/Marketing                       70\n10 Insurance                           68\n11 Telecommunications                  65\n12 Retail                              61\n13 Pharmaceutical                      54\n14 Military/Security                   35\n15 Non-profit                          35\n16 Hospitality/Entertainment/Sports    27\n\n\n\n\n\n\n\n\nCheck\n\n\n\nWhat happens if you pass in more than one variable into count()?"
  },
  {
    "objectID": "live_code/data_wrangling.html#practice",
    "href": "live_code/data_wrangling.html#practice",
    "title": "Data wrangling with dplyr",
    "section": "Practice",
    "text": "Practice\nSuppose I want to report a data frame that reports each unique level of Major and the proportion of times each level was observed in the data set in order of most popular to least popular. How might we do that?\n\n\nCode\ndatascience |>\n  count(Major) |>\n  mutate(prop = n/sum(n)) |>\n  select(Major, prop) |>\n  arrange(desc(prop))"
  },
  {
    "objectID": "live_code/data_wrangling.html#summarising-for-summary-statistics",
    "href": "live_code/data_wrangling.html#summarising-for-summary-statistics",
    "title": "Data wrangling with dplyr",
    "section": "Summarising for summary statistics",
    "text": "Summarising for summary statistics\nThe summarise() function gives us an easy way to calculate summary statistics of variables in the data frame! We just need to know the name of the function that will calculate the summary statistic for us.\n\ndatascience |>\n  summarise(mean_age = mean(Age))\n\n# A tibble: 1 × 1\n  mean_age\n     <dbl>\n1     34.4\n\n\n\n\nYou can obtain multiple summary statistics at once by separating the desired summary statistics with commas.\nThe summarise() function changes the data frame entirely. It collapses rows down to a summary statistic, and removes all columns that are irrelevant to the calculation.\n\n\n\n\n\n\nCheck\n\n\n\nWhat happens if you type summarise(mean(Age)) instead? You’ll note that the calculation becomes the column title."
  },
  {
    "objectID": "live_code/data_wrangling.html#grouping-by-grouped-operations",
    "href": "live_code/data_wrangling.html#grouping-by-grouped-operations",
    "title": "Data wrangling with dplyr",
    "section": "Grouping by grouped operations",
    "text": "Grouping by grouped operations\nSometimes, we want to look at a given statistic or create a new variable focusing on each level of a specific categorical variable. The group_by() function tells R to treat each unique level as a separate data set.\n\ndatascience |>\n  group_by(EmploymentStatus) |>\n  summarise(mean_age = mean(Age))\n\n# A tibble: 3 × 2\n  EmploymentStatus                                     mean_age\n  <chr>                                                   <dbl>\n1 Employed full-time                                       34.3\n2 Employed part-time                                       29.5\n3 Independent contractor, freelancer, or self-employed     39.0"
  },
  {
    "objectID": "coding_practice/coding-practice-07-wrangling.html",
    "href": "coding_practice/coding-practice-07-wrangling.html",
    "title": "Data wrangling coding practice",
    "section": "",
    "text": "Load in the tidyverse and openintro packages the in the code chunk below. We will once again work with the starbucks data from the openintro package. Remember that if you want to look at the data, you can type View(data frame name) in the Console.\n\n\n# load packages here\n\n\nWrangle the data to display the five items with the highest amount of calories. Only display the name of the items and their calorie content.\n\n\n\n\n\nWrangle the data to display all the sandwich items in the data set.\n\n\n\n\n\nThree macronutrients serve as sources of calories (i.e. energy) in food: carbohydrates, proteins, and fat. Carbohydrates contain 4 calories per gram, proteins contain 4 calories per gram, and fats contain 9 calories per gram.\n\nWrangle the data to add a new variable called theoretical_cals which represents the number of calories each item in the starbucks data theoretically should have based on its levels of carbohydrates, protein, and fat. Store the resulting data frame as a new data frame called starbucks_new.\n\n\n\n\nUsing your starbucks_new data frame from the previous step, display a summary table/data frame that shows the standard deviation of the differences between the reported calories and theoretical calories. Make sure to specify an informative/nicer name in your summary table.\n\n\n\n\nOnce you’re finished, be sure to knit and submit the outputted HTML file to the corresponding Canvas assignment!"
  },
  {
    "objectID": "slides/slides-00-welcome.html#syllabus-and-website",
    "href": "slides/slides-00-welcome.html#syllabus-and-website",
    "title": "Welcome!",
    "section": "Syllabus and website",
    "text": "Syllabus and website\n\nCourse website: https://midd-stat201-fall2024.github.io/\n\nPlease bookmark this page and visit frequently!\nNote that both sections will use the same website"
  },
  {
    "objectID": "slides/slides-00-welcome.html#what-is-this-course-about",
    "href": "slides/slides-00-welcome.html#what-is-this-course-about",
    "title": "Welcome!",
    "section": "What is this course about?",
    "text": "What is this course about?\n\nWhat is statistics? What is data science?\nBy the end of this course, you will:\n\nProduce and interpret graphical displays and numerical summaries of data\nHave developed confidence and some proficiency in coding in R (and in particular, the tidyverse syntax)\nBetter understand the central role of randomness in designing studies and making conclusions\nHopefully want to pursue another Statistics or Mathematics course!\nAnd much more…"
  },
  {
    "objectID": "slides/slides-00-welcome.html#necessary-background",
    "href": "slides/slides-00-welcome.html#necessary-background",
    "title": "Welcome!",
    "section": "Necessary background",
    "text": "Necessary background\n\nWe assume ZERO background in statistics and data science\nThere is a large computing component, though not as much as in STAT 118\nMATH 121 (Calculus 1) pre-req"
  },
  {
    "objectID": "slides/slides-00-welcome.html#recommendations",
    "href": "slides/slides-00-welcome.html#recommendations",
    "title": "Welcome!",
    "section": "Recommendations",
    "text": "Recommendations\n\nTakes notes! Each day’s slides will be made available on the course website by 10pm the night before. I recommend either:\n\nPrinting out the PDF version of slides to write notes on during class\n\nI recommend 4 or 6 slides per page (demo)\n\nDownloading PDF of slides to iPad/tablet/laptop and write notes on then using device\nTaking supplemental notes on paper/device\n\nRe–visit notes within 24 hours of class"
  },
  {
    "objectID": "slides/slides-00-welcome.html#recommendations-cont.",
    "href": "slides/slides-00-welcome.html#recommendations-cont.",
    "title": "Welcome!",
    "section": "Recommendations (cont.)",
    "text": "Recommendations (cont.)\n\nWe will frequently make use of our laptops. Please bring one with enough charge to last the entire class each day we meet!\n\nPlease let me know as soon as possible if you do not have access to a laptop\n\nTry to resist the temptation to do other tasks (e.g. check email, online shop, watch shows) when your laptop is open\n\nThis can be distracting to those around you\n\nKeep an open mind and don’t be afraid to ask for assistance or tell me to slow down!\nResist the temptation of using ChatGPT or other generative AI tools"
  },
  {
    "objectID": "slides/slides-00-welcome.html#github-username",
    "href": "slides/slides-00-welcome.html#github-username",
    "title": "Welcome!",
    "section": "GitHub username",
    "text": "GitHub username\n\nIf you don’t already have a GitHub account, please make one by visiting https://github.com/ and creating an account.\n\nTips for creating a username: incorporate your actual name, shorter is better than longer, make it timeless\n\nOnce you have an account, please go to this GoogleForm and enter in your GitHub username."
  },
  {
    "objectID": "slides/slides-00-welcome.html#beyoncés-albums",
    "href": "slides/slides-00-welcome.html#beyoncés-albums",
    "title": "Welcome!",
    "section": "Beyoncé’s albums",
    "text": "Beyoncé’s albums\n\nBeyoncé is one of the most famous singers of the 21st century\nBy 2023, Beyoncé had produced eight solo studio albums:\n\nDangerously in Love (2003), B’Day (2006), I Am… Sasha Fierce (2008), 4 (2011), Beyoncé (2013), Lemonade (2016), Renaissance (2022)\nPopular opinion (i.e. Reddit) is that Lemonade is her best album\n\nIs there any difference between the first four albums and Lemonade?\nLet’s consider the average length of words in these albums\n\nWe will discuss “average” in more detail next week, but for today, we will treat “average” as a number that is calculated by summing a bunch of quantities together and dividing by the total number of quantities"
  },
  {
    "objectID": "slides/slides-00-welcome.html#average-word-length",
    "href": "slides/slides-00-welcome.html#average-word-length",
    "title": "Welcome!",
    "section": "Average word length",
    "text": "Average word length\n\n\n\nThe average length of a word in Beyoncé’s first four alums is 3.62. What is the average length of a word in Lemonade?\n\n\nHow might we go about answering this question?\n\nLet’s collect some data!"
  },
  {
    "objectID": "slides/slides-02-toolkit-installation.html#reproducibility",
    "href": "slides/slides-02-toolkit-installation.html#reproducibility",
    "title": "Installation Day",
    "section": "Reproducibility",
    "text": "Reproducibility\n\nAllows your code execution or an experiment to be repeated by another person\nGoals:\n\nAre the tables and figures generated directly from the code?\nDoes the code actually do what you think it does?\nCan your code be used for other data/analyses?"
  },
  {
    "objectID": "slides/slides-02-toolkit-installation.html#toolkit",
    "href": "slides/slides-02-toolkit-installation.html#toolkit",
    "title": "Installation Day",
    "section": "Toolkit",
    "text": "Toolkit\n\n\nWe will use the programming language R to write code\nHow will interact with the R code? In the integrated development environment called RStudio. Helps us be more productive with R\n\nR is like a car engine, and RStudio is like a car’s dashboard\n\nWe will liberate our programming by keeping code, narrative, and output all in the same interface using R Markdown documents"
  },
  {
    "objectID": "slides/slides-02-toolkit-installation.html#r-markdown",
    "href": "slides/slides-02-toolkit-installation.html#r-markdown",
    "title": "Installation Day",
    "section": "R Markdown",
    "text": "R Markdown\n\nAllows us to create fully reproducible reports\nCan code in code chunks and type regular text/narrative outside of these chunks\nHow will we use R Markdown?\n\nYou coding practice problems and some weekly lab assignments will be assigned as an R Markdown document (.Rmd)\nYou will almost always be provided with a template .Rmd to start with (the exception being the end of the semester when you’ve mastered this material!)"
  },
  {
    "objectID": "slides/slides-02-toolkit-installation.html#version-control",
    "href": "slides/slides-02-toolkit-installation.html#version-control",
    "title": "Installation Day",
    "section": "Version control",
    "text": "Version control"
  },
  {
    "objectID": "slides/slides-02-toolkit-installation.html#git-and-github",
    "href": "slides/slides-02-toolkit-installation.html#git-and-github",
    "title": "Installation Day",
    "section": "Git and GitHub",
    "text": "Git and GitHub\n\nGit is a version control system (like “Track Changes” in Microsoft Word)\nGitHub is the home for your Git-based projects (like DropBox)\nWe will work with GitHub Desktop to make working on code on your personal machine and sending it to the cloud for “safe keeping” seamless\n\nAlso makes for great collaboration, because multiple people can be on the same GitHub project and will see all the change you make (kind of like a shared GoogleDoc)"
  },
  {
    "objectID": "slides/slides-02-toolkit-installation.html#installing-r",
    "href": "slides/slides-02-toolkit-installation.html#installing-r",
    "title": "Installation Day",
    "section": "Installing R",
    "text": "Installing R\nPlease be patient! This process may be time-consuming and stressful, but it is necessary for the rest of the course!\n\nWINDOWS/MAC: Go to the CRAN website and click on the appropriate link under “Download and Install R”. Then:\n\nIf you are Windows: click on the blue text that says “install R for the first time”.\nIf you are macOS: check your Mac OS system and if you have a chip (Apple icon -> About this Mac -> Overview)\n\nThen on the website, click the newest release that supports your current OS version. This will most likely be R-4.4.1-arm64.pkg or R-4.4.1-x86_64.pkg.\n\n\n\nLINUX: follow the instructions on for Steps 1 and 2 on this website."
  },
  {
    "objectID": "slides/slides-02-toolkit-installation.html#install-r-cont.",
    "href": "slides/slides-02-toolkit-installation.html#install-r-cont.",
    "title": "Installation Day",
    "section": "Install R (cont.)",
    "text": "Install R (cont.)\n\nA file will download, most likely to your Downloads folder. Run the file by clicking on it. Allow the app to make changes to your device if prompted.\n\nFollow the installation instructions, until you click on “Finish” to exit the installation setup. At this point, R should be successfully installed!"
  },
  {
    "objectID": "slides/slides-02-toolkit-installation.html#installing-rstudio",
    "href": "slides/slides-02-toolkit-installation.html#installing-rstudio",
    "title": "Installation Day",
    "section": "Installing RStudio",
    "text": "Installing RStudio\n\nLINUX: go to step 3 of the same website\nWINDOWS/MACS: Go to the Posit website and scroll down a little until you see two steps. We already did Step 1!\n\nUnder Step 2, click the blue Download RStudio Desktop button recommended for your computer\n\nmacOS users: double check you have an OS that is recent enough! Otherwise, raise your hand!\n\nRun the downloaded RStudio Executable file until you hit the “Finish” button. It may be the case that you don’t have to click anything at all.\nAfter RStudio finishes downloading, a window like this might pop up. If so, go ahead and drag the RStudio icon into the Applications folder."
  },
  {
    "objectID": "slides/slides-02-toolkit-installation.html#opening-rstudio",
    "href": "slides/slides-02-toolkit-installation.html#opening-rstudio",
    "title": "Installation Day",
    "section": "Opening RStudio",
    "text": "Opening RStudio\n\nIn the previous step, we put an RStudio shortcut into your Applications folder.\nYou may find it easier to put a shortcut somewhere else for easier access (e.g. your dock or home screen)\nTo open RStudio, simply double click on the RStudio icon (you do not need to click on the R icon)"
  },
  {
    "objectID": "slides/slides-02-toolkit-installation.html#make-a-folder",
    "href": "slides/slides-02-toolkit-installation.html#make-a-folder",
    "title": "Installation Day",
    "section": "Make a folder",
    "text": "Make a folder\n\nMake a folder that is easy to access (e.g. on your Desktop). Call it STAT 201.\n\nAll of your files for this course should go into this folder!!!!"
  },
  {
    "objectID": "slides/slides-00-welcome.html",
    "href": "slides/slides-00-welcome.html",
    "title": "Welcome!",
    "section": "",
    "text": "Course website: https://midd-stat201-fall2024.github.io/\n\nPlease bookmark this page and visit frequently!\nNote that both sections will use the same website\n\n\n\n\n\n\nWhat is statistics? What is data science?\nBy the end of this course, you will:\n\nHopefully want to pursue another Statistics or Mathematics course!\n\n\n\n\n\n\nWe assume ZERO background in statistics and data science\nThere is a large computing component, though not as much as in STAT 118\nMATH 121 (Calculus 1) pre-req\n\n\n\n\n\nTakes notes! Each day’s slides will be made available on the course website by 10pm the night before. I recommend either:\n\nPrinting out the PDF version of slides to write notes on in during class\n\nI recommend 4 or 6 slides per page (demo)\n\nDownloading PDF of slides to iPad/tablet/laptop and write notes on then using device\nTaking supplemental notes on paper/device\n\nRe–visit notes within 24 hours of class\n\n\n\n\n\nWe will frequently make use of our laptops. Please bring one with enough charge to last the entire class each day we meet!\n\nPlease let me know as soon as possible if you do not have access to a laptop\n\nTry to resist the temptation to do other tasks (e.g. check email, online shop, watch shows) when your laptop is open\n\nThis can be distracting to those around you\n\nKeep an open mind and don’t be afraid to ask for assistance or tell me to slow down!\nResist the temptation of using ChatGPT or other generative AI tools\n\n\n\n\n\nIf you don’t already have a GitHub account, please make one by visiting https://github.com/ and creating an account.\n\nTips for creating a username: incorporate your actual name, shorter is better than longer, make it timeless\n\nOnce you have an account, please go to this GoogleForm and enter in your GitHub username."
  },
  {
    "objectID": "live_code/intro_R.html",
    "href": "live_code/intro_R.html",
    "title": "Intro to R and R Markdown",
    "section": "",
    "text": "In the Console, type the following code: 1 + 1. What happens?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nNothing!\n\n\n\nTo execute/run code in the Console, we simply press Enter. Try it! What happens?\n\n\nNote that spacing doesn’t matter in R code.\nWe can add (+), subtract (-), multiply (*), divide (/), and perform more complicated calculations using R."
  },
  {
    "objectID": "live_code/intro_R.html#introduction-to-r-markdown",
    "href": "live_code/intro_R.html#introduction-to-r-markdown",
    "title": "Untitled",
    "section": "Introduction to R Markdown",
    "text": "Introduction to R Markdown\nThis document that we are working in is called an R markdown document. It allows us to seamlessly move between R code and regular text. How do we tell the document which parts correspond to code, and which parts correspond to text?\nIn the following, you see three back ticks followed by a left curly brace, the letter r, and right curly brace. A few lines down, you will see three more back ticks. The background in between these lines is gray, with some symbols on the right.\nThis defines ______. All of our R code should go into one.\n\n\n\n\n\n\n\nWhat happens if we delete a back tick?\n\n\n\nIn the code chunk above, let’s evaluate \\(\\sqrt{4}\\) (the square root of 4). To do this, type the following code: sqrt(4). Now evaluating the code in a code chunk is different from evaluating code in the Console. There are two ways to do so:\n1.\n2."
  },
  {
    "objectID": "live_code/intro_R.html#saving-progress",
    "href": "live_code/intro_R.html#saving-progress",
    "title": "Intro to R and R Markdown",
    "section": "Saving progress",
    "text": "Saving progress\nAt this point it is a good idea to save our progress. Like most document editors, we need to explicitly save our work. You know your work is not saved when _____.\nTo save our work in R Markdown document, we can do one of the following:\n\nClick on the floppy disk at the top of the panel\nGo to File -> Save\nHit Cmd+S\n____ the document\n\n\nKnitting\nKnitting the document will render your R markdown into its final output form. To knit, simply click on the ball of yarn at the top of the panel. Knitting can take anywhere from a few seconds to a few minutes depending on the amount of code and narrative you have.\nNow, outside of R Studio, go to the Folder where this R Markdown document is located. What do you notice? ______"
  },
  {
    "objectID": "live_code/intro_R.html#r-markdown-basics",
    "href": "live_code/intro_R.html#r-markdown-basics",
    "title": "Intro to R and R Markdown",
    "section": "R Markdown basics",
    "text": "R Markdown basics\nHow do we tell the document which parts correspond to code, and which parts correspond to text?\nIn the following, you see three back ticks followed by a left curly brace, the letter r, and right curly brace. A few lines down, you will see three more back ticks. The background in between these lines is gray, with some symbols on the right. These backs ticks must be aligned on the same tabulation.\nThis defines ______. All of our R code should go into one.\n\n\n\n\n\n\n\nWhat happens if we delete a back tick?\n\n\n\nIn the code chunk above, let’s evaluate \\(\\sqrt{4}\\) (the square root of 4). To do this, type the following code: sqrt(4). Now evaluating the code in a code chunk is different from evaluating code in the Console. There are two ways to do so:\n1.\n2.\n\nsqrt(4)\n\n[1] 2"
  },
  {
    "objectID": "live_code/intro_R.html#coding-in-r",
    "href": "live_code/intro_R.html#coding-in-r",
    "title": "Intro to R and R Markdown",
    "section": "Coding in R",
    "text": "Coding in R\nR is more than just a calculator! It provides lots of functionality for performing tasks related specifically to statistics.\n\nObject types\nEverything in R is an object. Here, we provide a non-exhaustive list of common objects (i.e. structures) you will encounter.\n\nA ____ object contains only a single number\n\n\n\n\n\nA ____ or ____ object is a set of characters within one pair of quotation marks\n\n\n\n\n\nA ____ object is an ordered collection of numbers or strings. We can create vectors using the command c():\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nWhat happened here?\n\nc(1, \"no\")\n\n[1] \"1\"  \"no\"\n\n\n\n\n\n\nA ____ object is either TRUE or FALSE. It is also referred to as a boolean.\nData frames are representations of datasets in R where the rows correspond to observations and columns correspond to variables that describe the observations (more on this later).\n\n\n\nFunctions\nWhen we calculated \\(\\sqrt{4}\\), we used the code sqrt(). This is an example of a function. Functions allow us to automate common tasks in a general way. Functions (just like in math) take in one or more inputs. These inputs are known as _____ or _____. They will almost always return an output. We know a command in R is a function because it has _____.\nIt is possible for us to create our own customized functions (you definitely will if you take STAT 218). However, in STAT 201, we will work with pre-provided functions. All pre-provided functions in R are accompanied by a Help file. To access the Help file, simply type ? followed by the name of the function in the Console. Try opening the Help file for the sqrt() function.\nIf I want to obtain the square root of the values 1, 4, 9, and 16, what code can I type?\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nsqrt(c(1,4,9, 16))\n\n[1] 1 2 3 4\n\n\n\n\n\n\n\nStoring objects as variables\nSuppose I want to calculate the volume of a cube, where each edge is of length 2. The volume is \\(\\text{edge length}^3\\) . We could calculate the area as follows:\n\n2*2*2\n\n[1] 8\n\n\nNow suppose someone tells you that the edge length was actually 2.1 instead of 2. So you modify as follows:\n\n2.1*2.1*2.1\n\n[1] 9.261\n\n\nThen you get told the edge length is actually 2.2. So frustrating! To save ourselves further troubles, we do the following to make our code reproducible:\n\nlength <- 2.2\nlength^3\n\n[1] 10.648\n\n\nWhat did we do in the above code? We _____ or ______ the value 2.2 into the ______ called length using the keys <-.\nNow make a new R code chunk here. In this new code chunk, make a new object called volume and assign to it an appropriate value. Now let’s knit the document.\n\n\n\nAs you should note: when we assign an object a value, its value is not automatically shown as output. In order to display the output, you must _____."
  },
  {
    "objectID": "live_code/intro_R.html#errors-warnings-and-messages",
    "href": "live_code/intro_R.html#errors-warnings-and-messages",
    "title": "Intro to R and R Markdown",
    "section": "Errors, warnings, and messages",
    "text": "Errors, warnings, and messages\nThese are always shown in red text in the Console. Whenever you see them, don’t panic! With practice, you will be able to decipher the messages and de-bug your code with ease.\n\nErrors: prevent code from executing and documents from knitting. It will be prefaced with “Error in…”. Trying typing in the following code in your Console: 1 + a. What happens?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n1+a\n\nError in eval(expr, envir, enclos): object 'a' not found\n\n\n\n\n\n\n\nA good reason to knit often is to ensure your code is error-free!\n\nWarnings: your code will run with some caveats. It will be prefaced with “Warning:”. We will see examples of this later on.\nMessages: messages in red that do not begin with “Error” or “Warning” are simply friendly messages that might provide you more information about the execution of your code."
  },
  {
    "objectID": "live_code/intro_R.html#packages",
    "href": "live_code/intro_R.html#packages",
    "title": "Intro to R and R Markdown",
    "section": "Packages",
    "text": "Packages\nPackages in R extend the functionality by providing additional functions and data. You can view them as analogous to apps you download from the App Store or Google Play on a cell phone. To use an app on a phone, you have to:\n\nDownload the app\nExplicitly open the app\n\nTo use a package, we need to:\n\n____ the package\n____ the package\n\nUnless you update R Studio, you will only need to install a package once. However, you will need to explicitly load in packages every time you work in a new R Markdown document.\nThere are thousands of available packages to work with. Two of the most common packages we will use are the openintro package and the tidyverse package (though, the tidyverse package is actually a giant package that is comprised of several other packages).\n\nPackage installation\nThere are two ways to install a package:\n\nOption 1:\n\nClick on the “Packages” tab in the Files pane of RStudio\nClick on “Install”\nType the name of the package under “Packages (separate multiple with space or comma):”.\nClick “Install”\n\nOption 2:\n\nType install.packages(\"package name\") into the Console. Note that the quotation marks are necessary.\nPress Return/Enter\n\n\nWe will install the openintro package together. Then, try installing the tidyverse package on your own!\n\n\nPackage loading\nTo use the package we have installed, we use the library() command:\n\nlibrary(openintro)\n\nLoading required package: airports\n\n\nLoading required package: cherryblossom\n\n\nLoading required package: usdata\n\n\n\n\nThis is an example of a function that does not return an output\n\n\n\n\n\n\nTip\n\n\n\nYou may have seen some red text in your Console when you loaded in the package above. Was this an error, warning, or regular message?"
  },
  {
    "objectID": "coding_practice/coding-practice-02-intro-r.html",
    "href": "coding_practice/coding-practice-02-intro-r.html",
    "title": "Intro to R coding practice",
    "section": "",
    "text": "Change your name in the YAML. Be sure to keep the quotation marks!\nExecute the following the code in the code chunk. Describe in words what the code is doing.\n\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nAnswer:\n\nWe will learn how to use the important function sample(), which allows us to obtain a random sample. Look at the help file by typing ?sample into your Console. After some experimenting, write code randomly sample two numbers from the set of numbers 1-5, without replacement. Note: it’s okay if you get something different from those around you! We are randomly sampling after all!\n\n\n\n\n\nIn the following code chunk, write code to load in the openintro package. Then run the code in the code chunk.\n\n\n\n\n\nIn the Console, type in ?cherry to open up the Help file for the cherry data frame. Type into the white space below the units for each of the variables.\n\nAnswer:\n\nRemember that rows in data frames represents observations. How many observations are there in the cherry data frame? Answer this by using the nrow() function and passing in the name of data frame of interest. You can confirm your answer by looking at the cherry Help file.\n\n\n\n\nOnce you’re finished, be sure to knit and submit the outputted file to the corresponding Canvas assignment!"
  },
  {
    "objectID": "slides/slides-01-study-design.html#explanatory-vs.-response",
    "href": "slides/slides-01-study-design.html#explanatory-vs.-response",
    "title": "Study design",
    "section": "Explanatory vs. Response",
    "text": "Explanatory vs. Response\n\nLots of scientific questions revolve around asking how \\(x\\) relates to \\(y\\)\nIf \\(y\\) is the primary variable of interest, i.e. the variable whose behavior we want to understand, it is called the response variable\nIf we try to understand how changing \\(x\\) affects \\(y\\), then \\(x\\) is called the explanatory variable\n\nExplanatory variables can often be manipulated/controlled/observed by the researcher ahead of time"
  },
  {
    "objectID": "slides/slides-00-welcome.html#important-course-information",
    "href": "slides/slides-00-welcome.html#important-course-information",
    "title": "Welcome!",
    "section": "Important course information",
    "text": "Important course information\n\nProfessor Becky Tang\n\nOffice: Warner 214\nEmail: btang@middlebury.edu\n\nCourse website: https://midd-stat201-fall2024.github.io/\n\nPlease bookmark this page and visit frequently!\nNote that both sections will use the same website"
  },
  {
    "objectID": "slides/slides-00-welcome.html#example",
    "href": "slides/slides-00-welcome.html#example",
    "title": "Welcome!",
    "section": "Example",
    "text": "Example\nCounties of the U.S. within the bottom 10% of death rates for kidney cancer for white males, 1980-1989.\n\n\nWhat do you notice? What might be the explanation?"
  },
  {
    "objectID": "slides/slides-00-welcome.html#example-cont.",
    "href": "slides/slides-00-welcome.html#example-cont.",
    "title": "Welcome!",
    "section": "Example (cont.)",
    "text": "Example (cont.)\nCounties of the U.S. within the top 10% of death rates for kidney cancer for white males, 1980-1989."
  },
  {
    "objectID": "slides/slides-00-welcome.html#example-cont.-1",
    "href": "slides/slides-00-welcome.html#example-cont.-1",
    "title": "Welcome!",
    "section": "Example (cont.)",
    "text": "Example (cont.)\nWhat’s going on? Let’s do some digging…\n\nDeath rate for kidney cancer: https://seer.cancer.gov/statfacts/html/kidrp.html\nCounty sizes: https://en.wikipedia.org/wiki/County_statistics_of_the_United_States"
  },
  {
    "objectID": "slides/slides-01-study-design.html#example-literary-digest-poll",
    "href": "slides/slides-01-study-design.html#example-literary-digest-poll",
    "title": "Study design",
    "section": "Example: Literary Digest poll",
    "text": "Example: Literary Digest poll\n\n1936 was an election year in the United States. Franklin D. Roosevelt (a Democrat) was completing his first term in office as president.\nRepublican candidate Alfred Landon of Kansas was his competitor\nLiterary Digest magazine conducted a polling survey, which received 2.4 million respondents (largest number of people every replying to a poll at that time)\n\nPrediction: overwhelming victory for Landon (predicted FDR would only get 43% of popular vote)\n\nActual result: FDR won by a landslide! (62% to 38%)\nWhat happened? Selection and non-response bias\n\n\n\nSelection bias: Digest mailed questionnaires to 10 million people. Where did they get the address form? Telephone books and club membership lists –> screened out the poor (only about 25% of households had phones)\n\nThis wouldn’t necessarily be bias, EXCEPT for the fact that poor people overwhelmingly favored FDR and the rich favored Landon\n\nNon-response bias: lots of people didn’t respond to survey (~75%)\n\nOnce again, wouldn’t matter if there wasn’t a difference in the opinions of respondents vs non-respondents. But among the 20% who responded, over half favored Landon\nNon-respondents can be very different from respondents. When there is high non-response rate, look for non-response bias\n\nLesson: not all samples that were done poorly are necessarily biased, but we should always ask how the sample was conducted! Also, when a selection procedure is biased, a large sample does not help; this just replicates the mistake on a larger scale!"
  },
  {
    "objectID": "slides/slides-01-study-design.html#simple-random-sampling-srs",
    "href": "slides/slides-01-study-design.html#simple-random-sampling-srs",
    "title": "Study design",
    "section": "Simple random sampling (SRS)",
    "text": "Simple random sampling (SRS)\n\nIn a simple random sample, each individual is chosen entirely by chance from the population, and each member of the population has an equal chance of being sampled\n\nTypically sampling without replacement\nKnowing that an individual was sampled does not provide useful information about which other cases are included\n\nAny given fixed-size subset of the population is equally likely to be chosen\n\n\n\nConsider again the research question: What proportion of current Middlebury professors attended a liberal arts college?\nHow might I obtain a sample random sample of 25 professors?\n\n\n\nRequires us to list all of the units in the target/survey population –> May be unrealistic!"
  },
  {
    "objectID": "slides/slides-01-study-design.html#multistage-cluster-sampling",
    "href": "slides/slides-01-study-design.html#multistage-cluster-sampling",
    "title": "Study design",
    "section": "Multistage cluster sampling",
    "text": "Multistage cluster sampling\n\nBuilds on the cluster sampling method, but rather than sampling all individuals within the selected clusters, only collect a simple random sample within each selected cluster\n\nCan make more stages/layers if appropriate!\n\nThough seemingly more complicated, why might we prefer multistage sampling over cluster sampling?\nHow might we devise a multistage cluster sample for Literary Digest?"
  },
  {
    "objectID": "slides/slides-01-study-design.html#probability-sampling",
    "href": "slides/slides-01-study-design.html#probability-sampling",
    "title": "Study design",
    "section": "Probability sampling",
    "text": "Probability sampling\n\nAny sampling method where the selection from the target population is based on random selection/chance\n\nAll subjects in the target population have equal chances of being selected as some point in the method\nNo one has discretion about who is included in the sample\n\nRandomly sampling from the population can help reduce bias in our sample\n\nIf we don’t randomly sample, results obtained from the sample will most likely not be representative and will not generalize to the target population\n\nExamples include: simple random, stratified, cluster, systematic"
  },
  {
    "objectID": "live_code/intro_R.html#yaml",
    "href": "live_code/intro_R.html#yaml",
    "title": "Intro to R and R Markdown",
    "section": "YAML",
    "text": "YAML\nAt the top of a markdown topic, you’ll see code between two sets of three dashed lines. This is known as a YAML header, and it contains the “informational” content of a document (e.g. title, author, date). Change these arguments accordingly for each assignment.\n\n\n\n\n\n\n\nNotice the quotation marks! These are extremely important!\nGo ahead and change your name in the author argument of the YAML."
  },
  {
    "objectID": "slides/slides-01-study-design.html",
    "href": "slides/slides-01-study-design.html",
    "title": "Study design",
    "section": "",
    "text": "Please bring your laptops tomorrow! We will be installing R and RStudio!"
  },
  {
    "objectID": "live_code/template_intro_R.html",
    "href": "live_code/template_intro_R.html",
    "title": "Introduction to R and R Markdown",
    "section": "",
    "text": "This document that we are working in is called an R markdown document. It allows us to seamlessly move between R code and regular text. The name of the file is easily found at the top this panel. All R markdown files end in ____.\n\n\nAt the top of a markdown topic, you’ll see code between two sets of three dashed lines. This is known as a YAML header, and it contains the “informational” content of a document (e.g. title, author, date). Change these accordingly for each assignment!\nGo ahead and change your name in the author argument of the YAML.\n\n\n\nHow do we tell the document which parts correspond to code, and which parts correspond to text?\nIn the following, you see three back ticks followed by a left curly brace, the letter r, and right curly brace. A few lines down, you will see three more back ticks. The background in between these lines is gray, with some symbols on the right. These backs ticks must be aligned on the same tabulation.\nThis defines ______. All of our R code should go into one.\n\n\n\nIn the code chunk above, let’s evaluate \\(\\sqrt{4}\\) (the square root of 4). To do this, type the following code: sqrt(4). Now evaluating the code in a code chunk is different from evaluating code in the Console. There are two ways to do so:\n\n\n\n\n\n\n\nAt this point it is a good idea to save our progress. Like most document editors, we need to explicitly save our work. You know your work is not saved when _____.\nTo save our work in R Markdown document, we can do one of the following:\n\nClick on the floppy disk at the top of the panel\nGo to File -> Save\nHit Cmd+S\n____ the document\n\n\n\nKnitting the document will render your R markdown into its final output form. To knit, simply click on the ball of yarn at the top of the panel. Knitting can take anywhere from a few seconds to a few minutes depending on the amount of code and narrative you have.\nNow, outside of R Studio, go to the Folder where this R Markdown document is located. What do you notice? ______\n\n\n\n\nR is more than just a calculator! It provides lots of functionality for performing tasks related specifically to statistics.\n\n\nEverything in R is an object. Here, we provide a non-exhaustive list of common objects (i.e. structures) you will encounter.\n\nA ____ object contains only a single number\n\n\n\n\n\nA ____ or ____ object is a set of characters within one pair of quotation marks\n\n\n\n\n\nA ____ object is an ordered collection of numbers or strings. We can create vectors using the command c():\n\n\n\n\n\nA ____ object is either TRUE or FALSE. It is also referred to as a boolean.\nData frames are representations of datasets in R where the rows correspond to observations and columns correspond to variables that describe the observations (more on this later).\n\n\n\n\nWhen we calculated \\(\\sqrt{4}\\), we used the code sqrt(). This is an example of a function. Functions allow us to automate common tasks in a general way. Functions (just like in math) take in one or more inputs. These inputs are known as _____ or _____. They will almost always return an output. We know a command in R is a function because it has _____.\nIt is possible for us to create our own customized functions (you definitely will if you take STAT 218). However, in STAT 201, we will work with pre-provided functions. All pre-provided functions in R are accompanied by a Help file. To access the Help file, simply type ? followed by the name of the function in the Console. Try opening the Help file for the sqrt() function.\n\n\n\nSuppose I want to calculate the volume of a cube, where each edge is of length 2. The volume is \\(\\text{edge length}^3\\) . We could calculate the area as follows:\n\n\n\nNow suppose someone tells you that the edge length was actually 2.1 instead of 2. So you modify as follows:\n\n\n\nThen you get told the edge length is actually 2.2. So frustrating! To save ourselves further troubles, we do the following to make our code reproducible:\n\n\n\nWhat did we do in the above code? We _____ or ______ the value 2.2 into the ______ called length using the keys <-.\nNow make a new R code chunk here. In this new code chunk, make a new object called volume and assign to it an appropriate value. Now let’s knit the document.\nAs you should note: when we assign an object a value, its value is not automatically shown as output. In order to display the output, you must _____.\n\n\n\n\nThese are always shown in red text in the Console. Whenever you see them, don’t panic! With practice, you will be able to decipher the messages and de-bug your code with ease.\n\nErrors: prevent code from executing and documents from knitting. It will be prefaced with “Error in…”. Trying typing in the following code in your Console: 1 + a. What happens?\nWarnings: your code will run with some caveats. It will be prefaced with “Warning:”. We will see examples of this later on.\nMessages: messages in red that do not begin with “Error” or “Warning” are simply friendly messages that might provide you more information about the execution of your code.\n\n\n\n\nPackages in R extend the functionality by providing additional functions and data. You can view them as analogous to apps you download from the App Store or Google Play on a cell phone. To use an app on a phone, you have to:\n\nDownload the app\nExplicitly open the app\n\nTo use a package, we need to:\n\n____ the package\n____ the package\n\nUnless you update R Studio, you will only need to install a package once. However, you will need to explicitly load in packages every time you work in a new R Markdown document.\nThere are thousands of available packages to work with. Two of the most common packages we will use are the openintro package and the tidyverse package (though, the tidyverse package is actually a giant package that is comprised of several other packages).\n\n\nThere are two ways to install a package:\n\nOption 1:\n\nClick on the “Packages” tab in the Files pane of RStudio\nClick on “Install”\nType the name of the package under “Packages (separate multiple with space or comma):”.\nClick “Install”\n\nOption 2:\n\nType install.packages(\"package name\") into the Console. Note that the quotation marks are necessary.\nPress Return/Enter\n\n\nWe will install the openintro package together. Now, try installing the tidyverse package on your own!\n\n\n\nTo use the package we have installed, we use the library() command:"
  },
  {
    "objectID": "slides/slides-02-toolkit-installation.html",
    "href": "slides/slides-02-toolkit-installation.html",
    "title": "Installation Day",
    "section": "",
    "text": "More homework problems released today. All due to Canvas on Monday, 9/16 by 11:59pm! Feel free to hand-write and then scan your work, or work in a text editor directly.\nOffice hours reminder!"
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#variables-types",
    "href": "slides/slides-03-numerical-pt1.html#variables-types",
    "title": "Numerical data",
    "section": "Variables types",
    "text": "Variables types\n\nVariables can be broadly broken into two categories: numerical (quantitative) or categorical (qualitative)\nNumerical variables take a wide range of numerical values, and it is sensible to add/subtract/do mathematical operations with those values. Two types:\n\nDiscrete if it can only take on finitely many numerical values within a given interval\nContinuous if it can take on any infinitely many values within a given interval\n\nCategorical variables are essentially everything else (more on this next week!)\nExamples and non-examples?"
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#example-1",
    "href": "slides/slides-03-numerical-pt1.html#example-1",
    "title": "Numerical data",
    "section": "Example",
    "text": "Example\nLet’s calculate the sample mean estimated weight from the data we collected today\n\n\nWrite out how you would calculate \\(\\bar{x}\\)\n\nThen I will use R to calculate the sample mean!"
  },
  {
    "objectID": "live_code/stat201_live_code.html",
    "href": "live_code/stat201_live_code.html",
    "title": "STAT 201 Live code",
    "section": "",
    "text": "# url to read data from\nurl_file <- \"https://raw.githubusercontent.com/midd-stat201-fall2024/midd-stat201-fall2024.github.io/main/live_code/data/insurance.csv\"\n\n# if you don't have the readr package, please install it!\nlibrary(readr)\n\n# read data, and assign to variable called insurance\ninsurance <- read_csv(url_file)\n\nRows: 200 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): sex, smoker, region\ndbl (4): age, bmi, children, charges\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "live_code/numerical_pt1_live.html",
    "href": "live_code/numerical_pt1_live.html",
    "title": "9/16/2024 Live code",
    "section": "",
    "text": "# url to read data from\nurl_file <- \"https://raw.githubusercontent.com/midd-stat201-fall2024/midd-stat201-fall2024.github.io/main/live_code/data/insurance.csv\"\n\n# if you don't have the readr package, please install it!\nlibrary(readr)\n\n# read data, and assign to variable called insurance\ninsurance <- read_csv(url_file)\n\nRows: 200 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): sex, smoker, region\ndbl (4): age, bmi, children, charges\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# first argument is x-axis\nplot(insurance$bmi, insurance$charges)\n\n\n\n# make axis labels more informative and add title \nplot(insurance$bmi, insurance$charges, xlab = \"BMI\", ylab = \"Charges ($)\", \n     main = \"Scatterplot of insurance charges by BMI\")"
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#creating-visualizations",
    "href": "slides/slides-03-numerical-pt1.html#creating-visualizations",
    "title": "Numerical data",
    "section": "Creating visualizations",
    "text": "Creating visualizations\nWorking in your groups, create a dot plot and a histogram of the estimated weights from the data we collected today!"
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#live-code-1",
    "href": "slides/slides-03-numerical-pt1.html#live-code-1",
    "title": "Numerical data",
    "section": "Live code",
    "text": "Live code\nFunctions to calculate sample mean, variance, and standard deviation in R:\n\nmean()\nvar()\nsd()"
  },
  {
    "objectID": "live_code/numerical_pt1_live.html#plots-in-base-r",
    "href": "live_code/numerical_pt1_live.html#plots-in-base-r",
    "title": "9/16/2024 Live code",
    "section": "Plots in base R",
    "text": "Plots in base R\n\n# url to read data from\nurl_file <- \"https://raw.githubusercontent.com/midd-stat201-fall2024/midd-stat201-fall2024.github.io/main/live_code/data/insurance.csv\"\n\n# if you don't have the readr package, please install it!\nlibrary(readr)\n\n# read data, and assign to variable called insurance\ninsurance <- read_csv(url_file)\n\nRows: 200 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): sex, smoker, region\ndbl (4): age, bmi, children, charges\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# scatter plot: first argument is x-axis\nplot(insurance$bmi, insurance$charges)\n\n\n\n# make axis labels more informative, add title, add color for fun\nplot(insurance$bmi, insurance$charges, xlab = \"BMI\", ylab = \"Charges ($)\", \n     main = \"Scatterplot of insurance charges by BMI\",\n     col = \"blue\")\n\n\n\n# make histogram\nhist(insurance$bmi)\n\n\n\n# change numbers of bins. Check Help file!\nhist(insurance$bmi, xlab = \"BMI\", ylab = \"Histogram of BMI\", breaks = 15)"
  },
  {
    "objectID": "live_code/numerical_pt1_live.html#summary-statistics",
    "href": "live_code/numerical_pt1_live.html#summary-statistics",
    "title": "9/16/2024 Live code",
    "section": "Summary statistics",
    "text": "Summary statistics\n\nmean(insurance$bmi)\n\n[1] 30.63417\n\nvar(insurance$bmi)\n\n[1] 31.98109\n\nsd(insurance$bmi)\n\n[1] 5.655182\n\n# confirm\nsqrt(var(insurance$bmi))\n\n[1] 5.655182"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html#another-boxplot",
    "href": "slides/slides-04-numerical-pt2.html#another-boxplot",
    "title": "Numerical data",
    "section": "Another boxplot",
    "text": "Another boxplot\nNow a boxplot of the estimated weights from the previous class!"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html#quartiles",
    "href": "slides/slides-04-numerical-pt2.html#quartiles",
    "title": "Numerical data",
    "section": "Quartiles",
    "text": "Quartiles\n\nThe 25th percentile is the value of data with 25% of values below it. Special name: first quartile \\(Q_{1}\\)\nThe 75th percentile is the value of data with 75% of values below it. Special name: third quartile \\(Q_{3}\\)\n\nWhat percent of the data fall between \\(Q_{1}\\) and \\(Q_{3}\\)? What percent of the data fall between \\(Q_{1}\\) and the median?\n\nHow to calculate? Suppose we have \\(2q\\) (even) or \\(2q + 1\\) (odd) number of values\n\n\\(Q_{1}\\) is the median of the \\(q\\) smallest values\n\\(Q_{3}\\) is the median of the \\(q\\) largest values\n\n\nWhat are \\(Q_{1}\\) and \\(Q_{3}\\) of the data \\(\\boldsymbol{x}\\)?"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html#more-visualizations-and-statistics",
    "href": "slides/slides-04-numerical-pt2.html#more-visualizations-and-statistics",
    "title": "Numerical data",
    "section": "More visualizations and statistics",
    "text": "More visualizations and statistics\n\n\n\n\nWe know how to calculate some summary statistics and interpret them alongside the histogram. But wouldn’t it be great if we had a visualization that directly displays some summary statistics?"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#introduction-to-ggplot",
    "href": "slides/slides-05-numerical-data-viz.html#introduction-to-ggplot",
    "title": "Visualizations with ggplot",
    "section": "Introduction to ggplot",
    "text": "Introduction to ggplot\n\nWe will learn how to create histograms, box plots, and scatterplots using the ggplot() function from the ggplot2 library\n\nPlots are constructed in layers\n\nAt a minimum, we need to specify 1) the dataset, 2) variable(s) from the dataset we’d like to plot, and 3) the type of plot\n\nHow does this differ from what we’ve seen in the past?\n\nThis is what the code will generally look like. Values in < > and xxx denote what you as the coder need to specify.\n\n\n\nggplot(data = <dataset>, # specify data frame\n       mapping = aes(x = <x-var>)) + #  specify variables to be used in plot\n  geom_xxx() + # specify plot type\n  <other options>"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#introduction-to-ggplot-.incremental-t",
    "href": "slides/slides-05-numerical-data-viz.html#introduction-to-ggplot-.incremental-t",
    "title": "Visualizations with ggplot",
    "section": "Introduction to ggplot {.incremental = T}",
    "text": "Introduction to ggplot {.incremental = T}\n\nWe will learn how to create histograms, box plots, and scatterplots using the ggplot() function from the ggplot2 library\n\nPlots are constructed in layers\n\nAt a minimum, we need to specify 1) the dataset, 2) variable(s) from the dataset we’d like to plot, and 3) the type of plot\n\nHow does this differ from what we’ve seen in the past?\n\nThis is what the code will generally look like. Values in < > and xxx denote what you as the coder need to specify.\n\n\n\nggplot(data = <dataset>, # <1>\n       mapping = aes(x = <x-var>)) + # <2> \n  geom_xxx() + # <3>\n  <other options>\n\n\nSpecify data frame\nSpecify variables to be use in plot\nSpecify type of plot"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#test",
    "href": "slides/slides-05-numerical-data-viz.html#test",
    "title": "Visualizations with ggplot",
    "section": "Test",
    "text": "Test\nlibrary(tidyverse)\nlibrary(palmerpenguins)\npenguins |>                                      # <1>\n  mutate(                                        # <2>\n    bill_ratio = bill_depth_mm / bill_length_mm, # <2>\n    bill_area  = bill_depth_mm * bill_length_mm  # <2>\n  )                                              # <2>\n\nTake penguins, and then,\nadd new columns for the bill ratio and bill area."
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#aesthetics-color",
    "href": "slides/slides-05-numerical-data-viz.html#aesthetics-color",
    "title": "Visualizations with ggplot",
    "section": "Aesthetics: color",
    "text": "Aesthetics: color\n\n\n\nggplot(data = insurance, \n       mapping = aes(x = bmi, y = charges, \n                     col = age)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nggplot(data = insurance, \n       mapping = aes(x = bmi, y = charges)) +\n  geom_point(aes(col = age))"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#aesthetics-transparency",
    "href": "slides/slides-05-numerical-data-viz.html#aesthetics-transparency",
    "title": "Visualizations with ggplot",
    "section": "Aesthetics: transparency",
    "text": "Aesthetics: transparency\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges, \n                                       alpha = age)) +\n  geom_point()"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#aesthetic-color",
    "href": "slides/slides-05-numerical-data-viz.html#aesthetic-color",
    "title": "Visualizations with ggplot",
    "section": "Aesthetic: color",
    "text": "Aesthetic: color\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges, \n                                       col = smoker)) +\n  geom_point()"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#aesthetic-shape",
    "href": "slides/slides-05-numerical-data-viz.html#aesthetic-shape",
    "title": "Visualizations with ggplot",
    "section": "Aesthetic: shape",
    "text": "Aesthetic: shape\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges, col = age,\n                                       shape = smoker)) +\n  geom_point()"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#specifying-multiple-aesthetics",
    "href": "slides/slides-05-numerical-data-viz.html#specifying-multiple-aesthetics",
    "title": "Visualizations with ggplot",
    "section": "Specifying multiple aesthetics",
    "text": "Specifying multiple aesthetics\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges, col = age, alpha = age)) +\n  geom_point()"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#adding-a-title",
    "href": "slides/slides-05-numerical-data-viz.html#adding-a-title",
    "title": "Visualizations with ggplot",
    "section": "Adding a title",
    "text": "Adding a title\n\nggplot(data = insurance, mapping = aes(x = charges)) +\n  geom_histogram() +\n  ggtitle(\"Histogram of charges\",\n          subtitle = \"In USD\")"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#changing-axis-labels",
    "href": "slides/slides-05-numerical-data-viz.html#changing-axis-labels",
    "title": "Visualizations with ggplot",
    "section": "Changing axis labels",
    "text": "Changing axis labels\nBy default, axis titles are taken from variable name specified in aes(). To change:\n\n\n\nggplot(data = insurance, \n       mapping = aes(x = charges)) +\n  geom_histogram() +\n  ggtitle(\"Histogram of charges\") +\n  xlab(\"Charges ($)\")\n\n\n\n\n\n\n\n\nggplot(data = insurance, \n       mapping = aes(x = charges)) +\n  geom_histogram() +\n  labs(title = \"Histogram of charges\",\n       x = \"Charges ($)\", y = \"Count\")"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#geom_histogram-cont.",
    "href": "slides/slides-05-numerical-data-viz.html#geom_histogram-cont.",
    "title": "Visualizations with ggplot",
    "section": "geom_histogram() cont.",
    "text": "geom_histogram() cont.\nTo improve on histogram we change the bin width.\n\n\n\nggplot(data = insurance, \n       mapping = aes(x = charges)) +\n  geom_histogram(binwidth = 5000)\n\n\n\n\n\n\n\n\nggplot(data = insurance, \n       mapping = aes(x = charges)) +\n  geom_histogram(bins = 20)"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#additional-variables-modifications",
    "href": "slides/slides-05-numerical-data-viz.html#additional-variables-modifications",
    "title": "Visualizations with ggplot",
    "section": "Additional variables + modifications",
    "text": "Additional variables + modifications\n\nWe emphasize making informative and useful visualizations.\n\nInformative titles and labels\nPlot should tell a meaningful story\n\nDepending on the plot and data, we can map additional variables by:\n\nSpecifying visual cues via aesthetics: color, size, shape, alpha (transparency)\nFaceting (will see this next week)"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html",
    "href": "slides/slides-04-numerical-pt2.html",
    "title": "Numerical data",
    "section": "",
    "text": "We learned about the sample mean \\(\\bar{x}\\), the sample variance \\(s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_{i} - \\bar{x})^2\\), and the sample standard deviation \\(s = \\sqrt{s^2}\\)\n\nWhy care about standard deviation (SD)? Describes how far data are distributed from their mean\nUsually (but not always!!) about 70% of the data will be within one SD of the mean, and 95% will be within two SDs\n\nThese percentages are not precise, but are useful for intuition\nWe will come back to this later in semester"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#aesthetic-color",
    "href": "slides/slides-06-categorical-data.html#aesthetic-color",
    "title": "Categorical data",
    "section": "Aesthetic: color",
    "text": "Aesthetic: color\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges, col = smoker)) +\n  geom_point() \n\n\n\nWhat do you notice about the legend for color?"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#aesthetic-shape",
    "href": "slides/slides-06-categorical-data.html#aesthetic-shape",
    "title": "Categorical data",
    "section": "Aesthetic: shape",
    "text": "Aesthetic: shape\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges, shape = smoker)) +\n  geom_point()"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#facet_wrap",
    "href": "slides/slides-06-categorical-data.html#facet_wrap",
    "title": "Categorical data",
    "section": "facet_wrap()",
    "text": "facet_wrap()\nFaceting is used when we want to split a particular visualization by the values of another (categorical) variable\n\n\n\nggplot(data = insurance, \n       mapping = aes(x = bmi)) +\n  geom_histogram() +\n  facet_wrap(~ smoker) \n\n\n\n\n\n\n\n\n\nggplot(data = insurance, \n       mapping = aes(x = bmi)) +\n  geom_histogram() +\n  facet_wrap(~ smoker, scales = \"free_y\")"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#facet_grid",
    "href": "slides/slides-06-categorical-data.html#facet_grid",
    "title": "Categorical data",
    "section": "facet_grid()",
    "text": "facet_grid()\n\nggplot(data = insurance, mapping = aes(x = bmi)) +\n  geom_histogram() +\n  facet_grid(sex ~ smoker)"
  },
  {
    "objectID": "slides/slides-03-numerical-pt1.html#describing-distributions",
    "href": "slides/slides-03-numerical-pt1.html#describing-distributions",
    "title": "Numerical data",
    "section": "Describing distributions",
    "text": "Describing distributions\nA convenient way to describe a variable’s behavior is through the shape of its distribution. Using histograms, we should identify:\n\nIf the distribution is symmetric or skewed\n\nDistributions with long tails to the left are called left-skewed\nDistributions with long tails to the right are right-skewed\nIf not skewed, then the distribution is symmetric\n\nModes which are prominent peaks in the distribution\n\nDistribution may be unimodal (one peak), bimodal (two peaks), or multimodal (more than two peaks)\nPeaks need not be same height"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html#interpreting-sd",
    "href": "slides/slides-04-numerical-pt2.html#interpreting-sd",
    "title": "Numerical data",
    "section": "Interpreting SD",
    "text": "Interpreting SD\nWe learned about the sample mean \\(\\bar{x}\\), the sample variance \\(s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_{i} - \\bar{x})^2\\), and the sample standard deviation \\(s = \\sqrt{s^2}\\)\n\nWhy care about standard deviation (SD)? Describes how far data are distributed from their mean\nUsually (but not always!!) about 70% of the data will be within one SD of the mean, and 95% will be within two SDs\n\nThese percentages are not precise, but are useful for intuition\nWe will come back to this later in semester"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html#visualizing-sd",
    "href": "slides/slides-04-numerical-pt2.html#visualizing-sd",
    "title": "Numerical data",
    "section": "Visualizing SD",
    "text": "Visualizing SD"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html#visualizing-sd-cont.",
    "href": "slides/slides-04-numerical-pt2.html#visualizing-sd-cont.",
    "title": "Numerical data",
    "section": "Visualizing SD (cont.)",
    "text": "Visualizing SD (cont.)\n\n\nWe know how to calculate some summary statistics and interpret them alongside the histogram. But wouldn’t it be great if we had a visualization that directly displays some summary statistics?"
  },
  {
    "objectID": "slides/slides-04-numerical-pt2.html#summary",
    "href": "slides/slides-04-numerical-pt2.html#summary",
    "title": "Numerical data",
    "section": "Summary",
    "text": "Summary\n\nBoxplots are another univariate visualization for numerical data\nMedian and IQR are robust to outliers, whereas mean and standard deviation are sensitive to outliers\nWhen should we prefer median over mean (or vice versa)?\n\n\nMedian is more stable; not affected by one single data point. But mean is easier to compute than median since you do not have sort observations. Also, mean has nice theoretical properties (STAT 311). If possible, always good to calculate both!!\nMedian = what is typical, mean = what you expect. The typical charge amount is around $9000. But if you’re looking at insurance charges, you shouldn’t expect to pay that low amount."
  },
  {
    "objectID": "live_code/numerical_pt2_live.html#boxplot-and-median",
    "href": "live_code/numerical_pt2_live.html#boxplot-and-median",
    "title": "9/18/2024 Live code",
    "section": "Boxplot and median",
    "text": "Boxplot and median\n\n# url to read data from\nurl_file <- \"https://raw.githubusercontent.com/midd-stat201-fall2024/midd-stat201-fall2024.github.io/main/live_code/data/insurance.csv\"\n\n# if you don't have the readr package, please install it!\nlibrary(readr)\n\n# read data, and assign to variable called insurance\ninsurance <- read_csv(url_file)\n\nRows: 200 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): sex, smoker, region\ndbl (4): age, bmi, children, charges\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# boxplot: first argument is x-axis\nboxplot(insurance$bmi)\n\n\n\n# median\nmedian(insurance$bmi)\n\n[1] 30.2075\n\n# making and sorting vectors\nmy_vec <- c(1, 9, 8, -2, 4)\nsort(my_vec)\n\n[1] -2  1  4  8  9"
  },
  {
    "objectID": "slides/slides-05-numerical-data-viz.html#inheriting-arguments",
    "href": "slides/slides-05-numerical-data-viz.html#inheriting-arguments",
    "title": "Visualizations with ggplot",
    "section": "Inheriting arguments",
    "text": "Inheriting arguments\n\nMany functions related to plotting in ggplot take the form geom_xxx()\nThe Help file for these functions show that the first two arguments are mapping and data. These are automatically inherited from the mapping and data arguments in the first layer ggplot() function\n\ni.e. you don’t need to re-specify them, unless you are trying to add a new data frame’s data to your visualization"
  },
  {
    "objectID": "coding_practice/coding-practice-04-numerical-pt2.html",
    "href": "coding_practice/coding-practice-04-numerical-pt2.html",
    "title": "Numerical data coding practice",
    "section": "",
    "text": "Change your name in the YAML. Be sure to keep the quotation marks!\nIn the following code chunk, load in the openintro package. We will once again work with the cherry data set.\n\n\n\n\n\nIn the code chunk below, write code to find the mean and median of the diameter of trees in the cherry data frame.\n\n\n\n\n\nMake a box plot of the height of cherry trees. Can you make an informative axis label for your plot? Try changing the color of your boxplot by specifying col = \"color name\" in the function. Note that the name of the color must be in quotes! If you’re confused, look at the examples in the bottom of the Help file of the appropriate function.\n\n\n\n\n\nMake a scatter plot of the diameter and volume of cherry trees. Put volume on the y-axis. Add some nicer labels!\n\n\n\n\nOnce you’re finished, be sure to knit and submit the output file to the corresponding Canvas assignment!"
  },
  {
    "objectID": "coding_practice/coding-practice-06-ggplot.html",
    "href": "coding_practice/coding-practice-06-ggplot.html",
    "title": "ggplot coding practice",
    "section": "",
    "text": "Load the tidyverse and openintro packages in the code chunk below. We will work with the mammals data from the openintro package. Load the data by typing data(mammals) right underneath where you loaded the packages. Then run the code chunk, and take a quick look at the data by clicking twice on the variable mammals in your Environment.\n\n\n# packages\n\n# data\n\n\nUsing ggplot, make a scatterplot of the gestation time and life span. (Gestation time is how long a baby is in the uterus.) You should notice that a Warning message appears. Describe in common language what the warning is trying to tell you.\n\n\n\n\nAnswer:\n\nNow, let’s make your plot from above more presentable. Using the labs() function, change the axes on your plots to have better titles that include the units. Also add a title. You may copy and paste your from above to get started. Start practicing good coding style by having line breaks between layers of the code!\n\n\n\n\n\nLet’s add color to your plot! Modify your code above to map the color of the points to the total amount of sleep each mammal gets. You might notice that some points are colored gray, instead of blue. Why do you think that is?\n\nAnswer:\nOnce you’re finished, be sure to knit and submit the outputted file to the corresponding Canvas assignment!"
  },
  {
    "objectID": "homework/hw2_r.html",
    "href": "homework/hw2_r.html",
    "title": "Problem Set 2: R",
    "section": "",
    "text": "Throughout this assignment, try to pay attention to practicing good coding style by having each layer of ggplot code be on its own line.\n\nChange your name in the YAML. Then load in the tidyverse and openintro packages in the code chunk below. We will work with the starbucks data from the openintro package.\n\n\n# load packages here\n\n\nNotice in the code chunk above, the code chunk header reads {r message = FALSE} instead of the usual {r}. Knit this file, and briefly study the output. Then delete the message = FALSE and knit again. What is this argument is doing? Once you’ve answered this, put the message = FALSE back!\n\nAnswer:\n\nWrite code that creates a variable for the average number of calories in a Starbucks food item.\n\n\n\n\n\nIn the following code chunk, use ggplot code to create a boxplot of the amount of calories in Starbucks food items. You should note that that one of the axes has tick marks/break points that are meaningless. Add to your plot the appropriate choice of the following two functions scale_x_continuous or scale_y_continuous(), then set the breaks argument to get rid of the breaks (maybe looks at its Help file).\n\n\n\n\n\nUsing ggplot, create a histogram of the calories in Starbucks food items. Change the binwidth to make a more “pleasing” plot. Add an informative title.\n\n\n\n\n\nTo your code above, add the function layer geom_vline() to add a vertical line that displays the mean number of calories (you may want to refer to the Help file). You should use the variable you created in number 3. Make this line a color of your choice. Then, add a caption that provides a brief description of what the line represents.\nUsing the plots you created, along with some summary statistics, describe the distribution of calories in Starbucks food items. Make sure you discuss shape, center, spread, and potential outliers.\n\nAnswer:\n\nNow create a scatterplot in ggplot with calories on the y-axis. For the x-axis variable, choose a variable that displays a strong association with calories. You may have to play around a bit! Add an informative title and change the x-axis variable to include the units (grams).\n\n\n\n\n\nIn the code above, color the points by another numerical variable in the data set. Then, add a layer of code using the function scale_color_viridis_c(). When you run this code, you’ll notice that this function changes the color palette to something called the Viridis color palette. Play around with different palettes by looking to the Help file for this function and looking at the option parameter. Choose one of the eight options!\nBriefly interpret this last plot you created. This may include discussing associations/trends/patterns (or lack thereof)!\n\nAnswer:\nOnce you’re done, knit this file one more time, and submit the outputted HTML file to Canvas alongside the other homework problems."
  },
  {
    "objectID": "coding_practice/coding-practice-05-ggplot.html",
    "href": "coding_practice/coding-practice-05-ggplot.html",
    "title": "ggplot coding practice",
    "section": "",
    "text": "Load the tidyverse and openintro packages in the code chunk below. We will work with the mammals data from the openintro package.\n\nIf you want to see the data frame, type View(mammals) in the Console. That will open up a new tab with the data frame in this Source pane.\n\n# packages\n\n\nUsing ggplot, make a scatterplot of the gestation time and life span. (Gestation time is how long a baby is in the uterus.) You should notice that a Warning message appears. Describe in common language what the warning is trying to tell you.\n\n\n\n\nAnswer:\n\nNow, let’s make your plot from above more presentable. Using the labs() function, change the axes on your plots to have better titles that include the units. Also add a title. You may copy and paste your from above to get started. Start practicing good coding style by having line breaks between layers of the code!\n\n\n\n\n\nLet’s add color to your plot! Modify your code above to map the color of the points to the total amount of sleep each mammal gets. You might notice that some points are colored gray, instead of blue. Why do you think that is?\n\nAnswer:\nOnce you’re finished, be sure to knit and submit the outputted file to the corresponding Canvas assignment!"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#categorical-data",
    "href": "slides/slides-06-categorical-data.html#categorical-data",
    "title": "Categorical data",
    "section": "Categorical data",
    "text": "Categorical data\n\nRecall that a variable is either numerical or categorical\nCategorical variables are variables that can take one of a limited (usually fixed) number of possible values, known as levels\n\nRepresent data that can be divided into groups\n\nTwo types:\n\nOrdinal: the levels have a special ordering\nNominal: the levels don’t have an ordering\n\nWe will almost exclusively treat our categorical variables as nominal in this class\n\n\nExamples and non-examples?"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#insurance-data",
    "href": "slides/slides-06-categorical-data.html#insurance-data",
    "title": "Categorical data",
    "section": "Insurance data",
    "text": "Insurance data"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#univariate",
    "href": "slides/slides-06-categorical-data.html#univariate",
    "title": "Categorical data",
    "section": "Univariate",
    "text": "Univariate\nIf we are interested in understanding the distribution of a single categorical variable, it is common to:\n\n\n\n\nDisplay a frequency table, which is a table of counts of each level\n\n\n\n# A tibble: 2 × 2\n  smoker     n\n  <chr>  <int>\n1 no       155\n2 yes       45\n\n\n\n\n\n\nVisualize the data through a bar plot, where different levels are displayed on one axis and the counts are portrayed on the other"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#univariate-eda",
    "href": "slides/slides-06-categorical-data.html#univariate-eda",
    "title": "Categorical data",
    "section": "Univariate EDA",
    "text": "Univariate EDA\nIf we are interested in understanding the distribution of a single categorical variable, it is common to:\n\n\n\nDisplay a frequency table, which is a table of counts of each level\n\n\n# A tibble: 2 × 2\n  smoker     n\n  <chr>  <int>\n1 no       155\n2 yes       45\n\n\n\n\n\nCreate a bar plot, where different levels are displayed on one axis and the counts are portrayed on the other\n\n\n\n\n\n\n\n\n\nHow do bar plots differ from histograms?"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#bivariate-eda",
    "href": "slides/slides-06-categorical-data.html#bivariate-eda",
    "title": "Categorical data",
    "section": "Bivariate EDA",
    "text": "Bivariate EDA\n\nPerhaps we are interested in examining the distribution of two categorical variables at the same time\nSummarize the distribution using a two-way table known as a contingency table:\n\nEach value in the table counts the number of times a particular combination of variable 1 and variable 2 levels occurred in data\n\n\n\n\n\nContingency table\n \n  \n    smoker \n    female \n    male \n  \n \n\n  \n    no \n    87 \n    68 \n  \n  \n    yes \n    17 \n    28 \n  \n\n\n\n\n\n\n\n\nHow can we use contingency table to obtain the distribution of just one of the variables?"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#visualizing-numerical-and-categorical",
    "href": "slides/slides-06-categorical-data.html#visualizing-numerical-and-categorical",
    "title": "Categorical data",
    "section": "Visualizing numerical and categorical",
    "text": "Visualizing numerical and categorical\n\nggplot(data = insurance, mapping = aes(x = bmi, y = charges, col = smoker)) +\n  geom_point() \n\n\n\nWhat do you notice about the legend for color?"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#bar-plot-univariate",
    "href": "slides/slides-06-categorical-data.html#bar-plot-univariate",
    "title": "Categorical data",
    "section": "Bar plot (univariate)",
    "text": "Bar plot (univariate)\n\nggplot(data = insurance, mapping = aes(x = smoker)) +\n  geom_bar()\n\n\n\nNote: if your data are already in the form of frequency table, we should use geom_col() instead!"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#bivariate-bar-plots",
    "href": "slides/slides-06-categorical-data.html#bivariate-bar-plots",
    "title": "Categorical data",
    "section": "Bivariate bar plots",
    "text": "Bivariate bar plots\n\n\n\nggplot(insurance, aes(x = smoker, fill = sex)) +\n  geom_bar(position = \"dodge\")  \n\n\n\n\n\n\n\n\nggplot(insurance, aes(x = smoker, fill = sex)) +\n  geom_bar(position = \"stack\") # this is default"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#bivariate-bar-plots-cont.",
    "href": "slides/slides-06-categorical-data.html#bivariate-bar-plots-cont.",
    "title": "Categorical data",
    "section": "Bivariate bar plots (cont.)",
    "text": "Bivariate bar plots (cont.)\n\nggplot(insurance, aes(x = smoker, fill = sex)) +\n  geom_bar(position = \"fill\")\n\n\nHow might we make the bars horizontal instead of vertical?"
  },
  {
    "objectID": "slides/slides-06-categorical-data.html#side-by-side-box-plots",
    "href": "slides/slides-06-categorical-data.html#side-by-side-box-plots",
    "title": "Categorical data",
    "section": "Side-by-side box plots",
    "text": "Side-by-side box plots\n\nggplot(data = insurance, \n       mapping = aes(x = smoker, y = bmi)) +\n  geom_boxplot()\n\n\nLike faceting, but only for box plots."
  },
  {
    "objectID": "coding_practice/coding-practice-06-categorical.html",
    "href": "coding_practice/coding-practice-06-categorical.html",
    "title": "Categorical data coding practice",
    "section": "",
    "text": "# load additional packages here\nlibrary(readr)\n\n\nIn the code chunk above, add a line of code to load in the package necessary for working with ggplot. Also change your name in the YAML.\nWe will be working with data you gave me from the start-of-semester study survey! In the following code, I would like you to delete the underscores and replace them with a variable name of your choice. The variable name should be meaningful to you, but also not too long.\n\nThen run the code chunk and take a look at the data! Then delete the argument eval = FALSE in the code chunk header!! That argument tells RStudio to not evaluate that particular code chunk when knitting, but we definitely want to evaluate this as this is how where our data gets loaded in!\n\n# url to read data from\nurl_file <- \"https://raw.githubusercontent.com/midd-stat201-fall2024/midd-stat201-fall2024.github.io/refs/heads/main/data/stat201_f24_data.csv\"\n\n____ <- read_csv(url_file)\n\n\nDo students take the 9:45 section because they want to get more sleep? Create boxplots of the average number of hours of sleep students get per night, split by the different sections. Note there are two ways to do this!\n\nThen interpret your visualization in the context of my “research question”.\n\n\n\nAnswer:\n\nPick two categorical variables of your choice. Using an appropriate type of bar plot, compare the distribution of one variable across the distribution of the second variable. That is, do the frequencies of occurrences of the levels of one variable depend on/change with the levels of the second variable?\n\n\n\n\nAnswer:\n\nWe will change the colors in our bar plot! Look at the Help file for the function scale_fill_brewer(). You should see many options of different palettes. Add this function to your code above (use good coding style!), and set the palette argument to one of the palette names. Play around until you’ve settled on one!\nYou used the function scale_fill_brewer() above, but there is a similar function called scale_color_brewer(). Why do you think we used the first function and not the second? (You can explore this by changing your code above to scale_color_brewer and seeing what happens. Be sure to revert back to scale_fill_brewer before submitting!)\n\nAnswer:\nOnce you’re finished, be sure to knit and submit the outputted file to the corresponding Canvas assignment!"
  },
  {
    "objectID": "homework/hw3_r.html",
    "href": "homework/hw3_r.html",
    "title": "STAT 201: Problem Set 3 (R)",
    "section": "",
    "text": "In January 2017, Buzzfeed published an article titled “These Nobel Prize Winners Show Why Immigration Is So Important For American Science”. In the article they explore where many Nobel laureates in the sciences were born and where they lived when they won their prize.\nIn this homework we will work with the data about Nobel laureates to recreate/update some of their visualizations with new data as well as explore new questions.\n\nlibrary(readr)\n# add more packages here as necessary\n\n\nurl_file <- \"https://raw.githubusercontent.com/midd-stat201-fall2024/midd-stat201-fall2024.github.io/refs/heads/main/data/nobel.csv\"\nnobel <- read_csv(url_file)\n\n\nChange your name in the YAML and add the package(s) necessary for creating ggplots and wrangling data in the code chunk above. Then run the code chunk to load in the data.\n\nA description of the variables in the nobel data are as follows:\n\nid: ID number\nfirstname: first name (and possible middle initial) of laureate\nsurname: last name/surname\nyear: the year the prize was awarded\ncategory: category of prize (Chemistry, Economics, Literature, Peace, Physics, or Medicine)\nborn_year: year laureate was born\ndied_year: year laureate died\naffiliation: affiliation of laureate at time of winning\ncity: city of laureate in prize year\ncountry: country where laureate was based in prize year\ngender: gender or laureate (male, female, or org, where org represents an organization)\nborn_city: city where laureate was born\nborn_country: country where laureate was born\nborn_country_code: two-letter country code of born_country\ndied_city: city were laureate died\ndied_country: country where laureate died\ndied_country_code: two-letter country code of died_country\nshare: reciprocal of the portion of prize awarded to the laureate\nmotivation: motivation for recognition\n\nThere are a few other variables ending in _original which correspond to locations whose names changed after the prize was given. The values in these columns are the original names of the locations at the time of the award. We won’t need to work with these variables.\n\nDisplay a summary table of the sample average and standard deviation of the ages of Nobel laureates at the time of receiving the prize. Do this in a single pipeline by:\n\n\nCreating a new variable that represents the age of the laureate when they one their prize, calculated as the year they received the award minus the year they were born\nFiltering to only retain observations for which your newly calculated age variable is available\nWriting code to actually create the summary statistics\n\nBe sure to explicitly set/define the column titles of your summary table. Then interpret these statistics (particularly the standard deviation) in context.\n\n\n\nAnswer:\n\nCreate a new data frame called nobel_living that filters for the following criteria:\n\n\nlaureates for whom country is available\nlaureates who are people as opposed to organizations\nlaureates who are still alive\n\n\n\n\nUse code to confirm that you have 247 laureates in your new data frame:\n\n\n\n\nBuzzfeed’s Claim #1: “Most living Nobel laureates were based in the US when they won their prizes”. Let’s see if that’s true.\n\nModify (i.e. store/assign over) your nobel_living data frame by creating a new variable called country_us. The variable should equal:\n\n“USA” if the laureate’s country value is indeed the “USA”\n“Other” is the laureate’s country value is not “USA”\n\nYou will have to use the if_else() function. Take a look at its Help file (and in particular, its examples).\n\n\n\nNow would be a good time to knit your work to save the progress and make sure everything is working!\n\nCreate a new data frame called nobel_living_science that only retains observations with laureates from the Physics, Chemistry, Medicine, and Economics categories from the nobel_living data frame.\n\n\n\n\n\nUsing the data frame nobel_living_science, create a faceted bar plot with horizontal bars that visualizes the relationship between 1) the category of prize and 2) whether the laureate was in the US when they won the Nobel prize. Note: Your visualization should be faceted by category. For each facet you should have two bars, one for winners in the US and one for Other.\n\nInterpret your visualization, and say a few words about whether the Buzzfeed Claim #1 is supported by the data.\n\n\n\nAnswer:\nNow would be a good time to knit your work to save the progress and make sure everything is working!\n\nBuzzfeed’s Claim #2: “But of those US-based Nobel laureates, many were born in other countries.” Let’s investigate this second claim!\n\nCreate a new variable called born_country_us that has the value “USA” if the laureate is born in the US, and “Other” if not. Be sure to save the variable to the data frame by storing the output back into nobel_living_science.\n\n\n\n\nLet’s improve on our previous visualization here. Visualize the relationship between where the laureate was based when they won the Nobel Prize and where they were born, split by category. Your final visualization should:\n\n\ncontain a facet for each category\nwithin each facet, have a bar for whether the laureate won the award in the US or not\nwithin each bar, display whether the laureate was born in the US or not\n\nBased on your visualization, do the data appear to support Buzzfeed’s Claim #2? Explain your reasoning in a few sentences.\n\n\n\nAnswer:\n\nWe will explore where the non-US laureates were born. In a single pipeline starting with nobel_living_science, filter for laureates who were living in the US when they won their prize but where born outside of the US. Then create a frequency table for their birth country, only displaying the top eight countries.\n\n\n\n\n\nFinally, go to the Buzzfeed website linked at the top (copy and paste the URL into Chrome or Safari) and compare your frequency table from the previous exercise to their bar plot. The numbers and ordering might not be the same. Why do you think that is?\n\nAnswer:\nOnce you’re finished, knit once more. Be sure to submitted the outputted HTML file to Canvas."
  },
  {
    "objectID": "live_code/data_wrangling.html#remember-to-save-over-data-frames-for-future-use",
    "href": "live_code/data_wrangling.html#remember-to-save-over-data-frames-for-future-use",
    "title": "Data wrangling with dplyr",
    "section": "Remember to save over data frames for future use!",
    "text": "Remember to save over data frames for future use!\nOften, we wrangle a data frame because we want certain variables to exist for future analyses. When we have modify a data frame for several future analyses, we should save over/store back into the same data frame with our new operations. As an example, in the following code, I am creating a new variable called age_months that represents the age in months.\n\ndatascience |>\n  mutate(age_months = Age * 12)\n\nNow suppose I want to visualize this variable:\n\nggplot(data = datascience, mapping = aes(x = age_months))+\n  geom_histogram()\n\nError in `geom_histogram()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error:\n! object 'age_months' not found\n\n\nIt’s complaining because the data frame datascience does not have a variable called age_months! We need to store over the previous version to make sure we “save our work”:\n\ndatascience <- datascience |>\n  mutate(age_months = Age * 12)\nggplot(data = datascience, mapping = aes(x = age_months))+\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "live_code/data_wrangling_pt1.html",
    "href": "live_code/data_wrangling_pt1.html",
    "title": "Data wrangling with dplyr",
    "section": "",
    "text": "Don’t forget to load the tidyverse package, which includes dplyr!\nRecall that we are looking at data provided by Kaggle. In 2017, Kaggle conducted an industry-wide survey to establish a comprehensive view of the state of data science and machine learning. We will be looking at just a subset of the data.\nBy default, all dplyr functions expect the first argument to be a data frame."
  },
  {
    "objectID": "live_code/data_wrangling_pt1.html#selecting-columns",
    "href": "live_code/data_wrangling_pt1.html#selecting-columns",
    "title": "Data wrangling with dplyr",
    "section": "Selecting columns",
    "text": "Selecting columns\nSometimes, there are a lot of columns in a data frame and we might not want all of them. The select() function gives us an easy way to choose which columns/variables we’d like to work with.\nThe select() function requires by default two arguments: the data frame and the variable names to choose from that data frame.\nThe following code works…\n\nselect(datascience, Age)\n\n# A tibble: 102 × 1\n     Age\n   <dbl>\n 1    56\n 2    33\n 3    26\n 4    25\n 5    33\n 6    22\n 7    29\n 8    35\n 9    37\n10    36\n# ℹ 92 more rows\n\n\n…but it’s preferable to take advantage of piping in order to make code more readable:\n\ndatascience |>\n  select(Age)\n\n# A tibble: 102 × 1\n     Age\n   <dbl>\n 1    56\n 2    33\n 3    26\n 4    25\n 5    33\n 6    22\n 7    29\n 8    35\n 9    37\n10    36\n# ℹ 92 more rows\n\n\n\n\nWhat’s going on here?\n\nStart with the data frame datascience\nPipe (|>) the data frame to the select() function and specify that we want the variable Age\nThe result is a data frame with 102 rows and 1 column with the Age variable\n\n\n\n\n\n\n\nCheck\n\n\n\nWhy do we type Age and not age?\n\n\n\nMultiple variables and excluding\n\n\n\n\n\n\nExpand\n\n\n\n\n\n\ndatascience |>\n  select(Age, Major)\n\n# A tibble: 102 × 2\n     Age Major                                                       \n   <dbl> <chr>                                                       \n 1    56 Mathematics or statistics                                   \n 2    33 Other                                                       \n 3    26 Computer Science                                            \n 4    25 Physics                                                     \n 5    33 Electrical Engineering                                      \n 6    22 Information technology, networking, or system administration\n 7    29 Computer Science                                            \n 8    35 Physics                                                     \n 9    37 Electrical Engineering                                      \n10    36 Information technology, networking, or system administration\n# ℹ 92 more rows\n\n\n\n\n\n\n\n\nCheck\n\n\n\nWhat happens if we swap the order of the variable names?\n\n\n\nA range of variables\n\ndatascience |>\n  select(Gender:Major)\n\n# A tibble: 102 × 3\n   Gender   Age Major                                                       \n   <chr>  <dbl> <chr>                                                       \n 1 Male      56 Mathematics or statistics                                   \n 2 Male      33 Other                                                       \n 3 Male      26 Computer Science                                            \n 4 Male      25 Physics                                                     \n 5 Male      33 Electrical Engineering                                      \n 6 Male      22 Information technology, networking, or system administration\n 7 Male      29 Computer Science                                            \n 8 Male      35 Physics                                                     \n 9 Male      37 Electrical Engineering                                      \n10 Male      36 Information technology, networking, or system administration\n# ℹ 92 more rows\n\n\n\n\nExcluding variables\n\ndatascience |>\n  select(-Country)\n\n# A tibble: 102 × 7\n   Gender   Age Major    FormalEducation CompensationAmount CompensationCurrency\n   <chr>  <dbl> <chr>    <chr>                        <dbl> <chr>               \n 1 Male      56 Mathema… Master's degree             250000 USD                 \n 2 Male      33 Other    Bachelor's deg…            1200000 RUB                 \n 3 Male      26 Compute… Master's degree            1100000 TWD                 \n 4 Male      25 Physics  Bachelor's deg…              20000 USD                 \n 5 Male      33 Electri… Doctoral degree             100000 USD                 \n 6 Male      22 Informa… Bachelor's deg…             624000 RUB                 \n 7 Male      29 Compute… Master's degree             126000 PLN                 \n 8 Male      35 Physics  Doctoral degree             133000 USD                 \n 9 Male      37 Electri… Master's degree              80000 USD                 \n10 Male      36 Informa… Master's degree              80000 AUD                 \n# ℹ 92 more rows\n# ℹ 1 more variable: LanguageRecommendation <chr>"
  },
  {
    "objectID": "live_code/data_wrangling_pt1.html#arranging-rows",
    "href": "live_code/data_wrangling_pt1.html#arranging-rows",
    "title": "Data wrangling with dplyr",
    "section": "Arranging rows",
    "text": "Arranging rows\nWe might want to re-arrange rows in ascending or descending order according to a certain variable. The arrange() function does this, and requires specifying at least one variable to arrange by:\n\ndatascience |>\n  select(Age, Major) |>\n  arrange(Age)\n\n# A tibble: 102 × 2\n     Age Major                                                       \n   <dbl> <chr>                                                       \n 1    21 Computer Science                                            \n 2    21 Computer Science                                            \n 3    22 Information technology, networking, or system administration\n 4    22 Computer Science                                            \n 5    22 Computer Science                                            \n 6    22 Computer Science                                            \n 7    22 Computer Science                                            \n 8    22 Computer Science                                            \n 9    22 Computer Science                                            \n10    23 Biology                                                     \n# ℹ 92 more rows\n\n\n\n\nBy default, arrange() will reorder in ascending order. If we’d like to go in descending order, we can code arrange(desc(Age))."
  },
  {
    "objectID": "live_code/data_wrangling_pt1.html#slicing-for-certain-row-numbers",
    "href": "live_code/data_wrangling_pt1.html#slicing-for-certain-row-numbers",
    "title": "Data wrangling with dplyr",
    "section": "Slicing for certain row numbers",
    "text": "Slicing for certain row numbers\nRemember, data frames are in tabular format. So each row has a certain index, as does each column. The first row is index 1, the second row index 2, etc.\nThe slice() function expects a vector of row indices to retain:\n\ndatascience |>\n  slice(1:5)\n\n# A tibble: 5 × 8\n  Country       Gender   Age Major            FormalEducation CompensationAmount\n  <chr>         <chr>  <dbl> <chr>            <chr>                        <dbl>\n1 United States Male      56 Mathematics or … Master's degree             250000\n2 Russia        Male      33 Other            Bachelor's deg…            1200000\n3 Taiwan        Male      26 Computer Science Master's degree            1100000\n4 United States Male      25 Physics          Bachelor's deg…              20000\n5 United States Male      33 Electrical Engi… Doctoral degree             100000\n# ℹ 2 more variables: CompensationCurrency <chr>, LanguageRecommendation <chr>\n\n\n\n\n\n\n\n\nCheck\n\n\n\nWhat is the difference between select() and slice()?"
  },
  {
    "objectID": "live_code/data_wrangling_pt1.html#filtering-to-select-a-subset-of-rows",
    "href": "live_code/data_wrangling_pt1.html#filtering-to-select-a-subset-of-rows",
    "title": "Data wrangling with dplyr",
    "section": "Filtering to select a subset of rows",
    "text": "Filtering to select a subset of rows\nThe slice() function is nice, but unless the rows of your data frame are ordered meaningfully, its actual utility is limited. We might want to look at a set of the cases in which a certain condition is met.\nIn the following code, we use the filter() function to only retain the observations where the person’s Major was Computer Science. This function requires specifying a logical condition, and keeps observations in which the condition is met (i.e. TRUE).\n\ndatascience |>\n  filter(Major == \"Computer Science\")\n\n# A tibble: 35 × 8\n   Country                 Gender   Age Major FormalEducation CompensationAmount\n   <chr>                   <chr>  <dbl> <chr> <chr>                        <dbl>\n 1 Taiwan                  Male      26 Comp… Master's degree            1100000\n 2 Poland                  Male      29 Comp… Master's degree             126000\n 3 India                   Male      34 Comp… Master's degree            2300000\n 4 Russia                  Male      22 Comp… Bachelor's deg…             528000\n 5 People 's Republic of … Male      28 Comp… Master's degree              70000\n 6 Russia                  Male      34 Comp… Some college/u…            1500000\n 7 Russia                  Male      22 Comp… Master's degree              70000\n 8 Italy                   Male      22 Comp… Bachelor's deg…              10000\n 9 India                   Male      23 Comp… Bachelor's deg…             400000\n10 Other                   Male      32 Comp… Master's degree            1200000\n# ℹ 25 more rows\n# ℹ 2 more variables: CompensationCurrency <chr>, LanguageRecommendation <chr>\n\n\n\nMultiple conditions\n\n\n\n\n\n\nExpand\n\n\n\n\n\nWe can also filter for more than one condition at once. Within filter(), the comma , specifies that all conditions must be true. It can be read as “and”. In the following code, we retain cases where someone’s major was Computer Science and they were 30 years old at the time of filling out the survey.\n\ndatascience |>\n  filter(Major == \"Computer Science\", \n         Age == 30) |>\n  select(Major, Age)\n\n# A tibble: 3 × 2\n  Major              Age\n  <chr>            <dbl>\n1 Computer Science    30\n2 Computer Science    30\n3 Computer Science    30\n\n\nIf we just need at least one of multiple conditions to be true, we can use the | operator which stands for “or”:\n\ndatascience |>\n  filter(Major == \"Computer Science\" | \n         Age == 30) |>\n  select(Major, Age)\n\n# A tibble: 36 × 2\n   Major              Age\n   <chr>            <dbl>\n 1 Computer Science    26\n 2 Computer Science    29\n 3 Computer Science    34\n 4 Computer Science    22\n 5 Computer Science    28\n 6 Computer Science    34\n 7 Computer Science    22\n 8 Computer Science    22\n 9 Computer Science    23\n10 Computer Science    32\n# ℹ 26 more rows\n\n\n\ndatascience |>\n  filter(Major == \"Computer Science\" | Major == \"Other\",\n         Age == 30) |>\n  select(Major, Age)\n\n# A tibble: 3 × 2\n  Major              Age\n  <chr>            <dbl>\n1 Computer Science    30\n2 Computer Science    30\n3 Computer Science    30"
  },
  {
    "objectID": "live_code/data_wrangling_pt1.html#distinct-to-filter-for-unique-rows",
    "href": "live_code/data_wrangling_pt1.html#distinct-to-filter-for-unique-rows",
    "title": "Data wrangling with dplyr",
    "section": "Distinct to filter for unique rows",
    "text": "Distinct to filter for unique rows\nThe distinct() function requires specifying variables in the data frame, and the function will keep only unique/distinct instances of the variable(s). Unless otherwise specified, it will drop all the other variables.\n\ndatascience |>\n  distinct(FormalEducation)\n\n# A tibble: 4 × 1\n  FormalEducation                                                  \n  <chr>                                                            \n1 Master's degree                                                  \n2 Bachelor's degree                                                \n3 Doctoral degree                                                  \n4 Some college/university study without earning a bachelor's degree\n\ndatascience |>\n  distinct(FormalEducation, Major) |>\n  arrange(FormalEducation)\n\n# A tibble: 29 × 2\n   FormalEducation   Major                                                      \n   <chr>             <chr>                                                      \n 1 Bachelor's degree Other                                                      \n 2 Bachelor's degree Physics                                                    \n 3 Bachelor's degree Information technology, networking, or system administrati…\n 4 Bachelor's degree Computer Science                                           \n 5 Bachelor's degree Biology                                                    \n 6 Bachelor's degree A humanities discipline                                    \n 7 Bachelor's degree Electrical Engineering                                     \n 8 Bachelor's degree Engineering (non-computer focused)                         \n 9 Bachelor's degree Mathematics or statistics                                  \n10 Doctoral degree   Electrical Engineering                                     \n# ℹ 19 more rows\n\n\n\n\n\n\n\n\nCheck\n\n\n\nWhat variables are by default included in the output from distinct()?"
  },
  {
    "objectID": "live_code/data_wrangling_pt1.html#mutate-to-add-a-new-variable",
    "href": "live_code/data_wrangling_pt1.html#mutate-to-add-a-new-variable",
    "title": "Data wrangling with dplyr",
    "section": "Mutate to add a new variable",
    "text": "Mutate to add a new variable\nIt is typical for us to want to add variables to a given data frame. We do this with the mutate() function. We must specify:\n\nThe name of the new variable and\nHow to calculate the value of that new variable for each observation. This will typically involve operations involving variables already present in the data frame.\n\nWe link the two with an equals sign.\n\ndatascience %>%\n  mutate(compensation_1k = CompensationAmount/1000) |>\n  select(CompensationAmount, compensation_1k)\n\n# A tibble: 102 × 2\n   CompensationAmount compensation_1k\n                <dbl>           <dbl>\n 1             250000             250\n 2            1200000            1200\n 3            1100000            1100\n 4              20000              20\n 5             100000             100\n 6             624000             624\n 7             126000             126\n 8             133000             133\n 9              80000              80\n10              80000              80\n# ℹ 92 more rows\n\n\n\n\n\n\n\n\nCheck\n\n\n\nWhat exactly is going on in the second line of code?"
  },
  {
    "objectID": "live_code/data_wrangling_pt1.html#counting-to-create-frequency-tables",
    "href": "live_code/data_wrangling_pt1.html#counting-to-create-frequency-tables",
    "title": "Data wrangling with dplyr",
    "section": "Counting to create frequency tables",
    "text": "Counting to create frequency tables\nWe can count the number of instances we observed each level of a given categorical variable:\n\ndatascience |>\n  count(LanguageRecommendation)\n\n# A tibble: 8 × 2\n  LanguageRecommendation     n\n  <chr>                  <int>\n1 C/C++/C#                   3\n2 Haskell                    1\n3 Matlab                     3\n4 Other                      1\n5 Python                    66\n6 R                         19\n7 SQL                        6\n8 Scala                      3\n\n\n\n\n\n\n\n\nCheck\n\n\n\nHow does the resulting data frame from count() compare to the original data frame we passed in?\n\n\n\nMaking frequency tables useful\nWe typically want to present the counts in ascending or descending order.\n\n\n\n\n\n\nExpand\n\n\n\n\n\nNote that the following chunks of code do the same thing. One of them takes advantage of an additional argument in count(), whereas the other block of the uses an additional function:\n\ndatascience |>\n  count(LanguageRecommendation, sort = T)\n\n# A tibble: 8 × 2\n  LanguageRecommendation     n\n  <chr>                  <int>\n1 Python                    66\n2 R                         19\n3 SQL                        6\n4 C/C++/C#                   3\n5 Matlab                     3\n6 Scala                      3\n7 Haskell                    1\n8 Other                      1\n\n\n\ndatascience |>\n  count(LanguageRecommendation) |>\n  arrange(desc(n))\n\n# A tibble: 8 × 2\n  LanguageRecommendation     n\n  <chr>                  <int>\n1 Python                    66\n2 R                         19\n3 SQL                        6\n4 C/C++/C#                   3\n5 Matlab                     3\n6 Scala                      3\n7 Haskell                    1\n8 Other                      1\n\n\n\n\n\n\n\n\nCheck\n\n\n\nWhat happens if you pass in more than one variable into count()?"
  },
  {
    "objectID": "live_code/data_wrangling_pt1.html#practice",
    "href": "live_code/data_wrangling_pt1.html#practice",
    "title": "Data wrangling with dplyr",
    "section": "Practice",
    "text": "Practice\nSuppose I want to report a data frame that reports each unique level of Major and the proportion of times each level was observed in the data set in order of most popular to least popular. How might we do that?\n\n\nCode\ndatascience |>\n  count(Major) |>\n  mutate(prop = n/sum(n)) |>\n  select(Major, prop) |>\n  arrange(desc(prop))"
  },
  {
    "objectID": "live_code/data_wrangling_pt1.html#summarising-for-summary-statistics",
    "href": "live_code/data_wrangling_pt1.html#summarising-for-summary-statistics",
    "title": "Data wrangling with dplyr",
    "section": "Summarising for summary statistics",
    "text": "Summarising for summary statistics\nThe summarise() function gives us an easy way to calculate summary statistics of variables in the data frame! We just need to know the name of the function that will calculate the summary statistic for us.\n\ndatascience |>\n  summarise(mean_age = mean(Age))\n\n# A tibble: 1 × 1\n  mean_age\n     <dbl>\n1     32.8\n\n\n\n\nYou can obtain multiple summary statistics at once by separating the desired summary statistics with commas.\nThe summarise() function changes the data frame entirely. It collapses rows down to a summary statistic, and removes all columns that are irrelevant to the calculation.\n\n\n\n\n\n\nCheck\n\n\n\nWhat happens if you type summarise(mean(Age)) instead? You’ll note that the calculation becomes the column title."
  },
  {
    "objectID": "live_code/data_wrangling_pt2.html",
    "href": "live_code/data_wrangling_pt2.html",
    "title": "Data wrangling with dplyr (cont.)",
    "section": "",
    "text": "Don’t forget to load the tidyverse package!\nRecall that we are looking at data provided by Kaggle. In 2017, Kaggle conducted an industry-wide survey to establish a comprehensive view of the state of data science and machine learning. We will be looking at just a subset of the data."
  },
  {
    "objectID": "live_code/data_wrangling_pt2.html#selecting-columns",
    "href": "live_code/data_wrangling_pt2.html#selecting-columns",
    "title": "Data wrangling with dplyr",
    "section": "Selecting columns",
    "text": "Selecting columns\nSometimes, there are a lot of columns in a data frame and we might not want all of them. The select() function gives us an easy way to choose which columns/variables we’d like to work with.\nThe select() function requires by default two arguments: the data frame and the variable names to choose from that data frame.\nThe following code works…\n\nselect(datascience, Age)\n\n# A tibble: 2,288 × 1\n     Age\n   <dbl>\n 1    56\n 2    33\n 3    26\n 4    25\n 5    33\n 6    22\n 7    29\n 8    35\n 9    37\n10    31\n# ℹ 2,278 more rows\n\n\n…but it’s preferable to take advantage of piping in order to make code more readable:\n\ndatascience |>\n  select(Age)\n\n# A tibble: 2,288 × 1\n     Age\n   <dbl>\n 1    56\n 2    33\n 3    26\n 4    25\n 5    33\n 6    22\n 7    29\n 8    35\n 9    37\n10    31\n# ℹ 2,278 more rows\n\n\n\n\nWhat’s going on here?\n\nStart with the data frame datascience\nPipe (|>) the data frame to the select() function and specify that we want the variable Age\nThe result is a data frame with 2288 rows and 1 column with the Age variable\n\n\n\n\n\n\n\nCheck\n\n\n\nWhy do we type Age and not age?\n\n\n\nMultiple variables and excluding\n\n\n\n\n\n\nExpand\n\n\n\n\n\n\ndatascience |>\n  select(Age, Major)\n\n# A tibble: 2,288 × 2\n     Age Major                                                       \n   <dbl> <chr>                                                       \n 1    56 Mathematics or statistics                                   \n 2    33 Other                                                       \n 3    26 Computer Science                                            \n 4    25 Physics                                                     \n 5    33 Electrical Engineering                                      \n 6    22 Information technology, networking, or system administration\n 7    29 Computer Science                                            \n 8    35 Physics                                                     \n 9    37 Electrical Engineering                                      \n10    31 Computer Science                                            \n# ℹ 2,278 more rows\n\n\n\n\n\n\n\n\nCheck\n\n\n\nWhat happens if we swap the order of the variable names?\n\n\n\nA range of variables\n\ndatascience |>\n  select(Gender:EmploymentStatus)\n\n# A tibble: 2,288 × 3\n   Gender   Age EmploymentStatus                                    \n   <chr>  <dbl> <chr>                                               \n 1 Male      56 Independent contractor, freelancer, or self-employed\n 2 Male      33 Employed full-time                                  \n 3 Male      26 Employed full-time                                  \n 4 Male      25 Employed part-time                                  \n 5 Male      33 Employed full-time                                  \n 6 Male      22 Employed full-time                                  \n 7 Male      29 Employed full-time                                  \n 8 Male      35 Employed full-time                                  \n 9 Male      37 Employed full-time                                  \n10 Male      31 Employed part-time                                  \n# ℹ 2,278 more rows\n\n\n\n\nExcluding variables\n\ndatascience |>\n  select(-Country)\n\n# A tibble: 2,288 × 16\n   Gender   Age EmploymentStatus          EmployerIndustry FormalEducation Major\n   <chr>  <dbl> <chr>                     <chr>            <chr>           <chr>\n 1 Male      56 Independent contractor, … Mix of fields    Master's degree Math…\n 2 Male      33 Employed full-time        Internet-based   Bachelor's deg… Other\n 3 Male      26 Employed full-time        Financial        Master's degree Comp…\n 4 Male      25 Employed part-time        Academic         Bachelor's deg… Phys…\n 5 Male      33 Employed full-time        Telecommunicati… Doctoral degree Elec…\n 6 Male      22 Employed full-time        Mix of fields    Bachelor's deg… Info…\n 7 Male      29 Employed full-time        Pharmaceutical   Master's degree Comp…\n 8 Male      35 Employed full-time        Technology       Doctoral degree Phys…\n 9 Male      37 Employed full-time        Technology       Master's degree Elec…\n10 Male      31 Employed part-time        Technology       Doctoral degree Comp…\n# ℹ 2,278 more rows\n# ℹ 10 more variables: CompensationAmount <dbl>, CompensationCurrency <chr>,\n#   CurrentJobTitle <chr>, TitleFit <chr>, LanguageRecommendation <chr>,\n#   DataScienceIdentity <chr>, WorkDataVisualizations <chr>,\n#   JobSatisfaction <chr>, JobSatisfaction2 <dbl>, ConversionUSD <dbl>"
  },
  {
    "objectID": "live_code/data_wrangling_pt2.html#arranging-rows",
    "href": "live_code/data_wrangling_pt2.html#arranging-rows",
    "title": "Data wrangling with dplyr",
    "section": "Arranging rows",
    "text": "Arranging rows\nWe might want to re-arrange rows in ascending or descending order according to a certain variable. The arrange() function does this, and requires specifying at least one variable to arrange by:\n\ndatascience |>\n  select(Age, Major) |>\n  arrange(Age)\n\n# A tibble: 2,288 × 2\n     Age Major                                                       \n   <dbl> <chr>                                                       \n 1     0 Mathematics or statistics                                   \n 2     1 Other                                                       \n 3    19 Computer Science                                            \n 4    19 Biology                                                     \n 5    20 Information technology, networking, or system administration\n 6    20 Mathematics or statistics                                   \n 7    20 Computer Science                                            \n 8    20 Mathematics or statistics                                   \n 9    21 Other                                                       \n10    21 Computer Science                                            \n# ℹ 2,278 more rows\n\n\n\n\nBy default, arrange() will reorder in ascending order. If we’d like to go in descending order, we can code arrange(desc(Age))."
  },
  {
    "objectID": "live_code/data_wrangling_pt2.html#slicing-for-certain-row-numbers",
    "href": "live_code/data_wrangling_pt2.html#slicing-for-certain-row-numbers",
    "title": "Data wrangling with dplyr",
    "section": "Slicing for certain row numbers",
    "text": "Slicing for certain row numbers\nRemember, data frames are in tabular format. So each row has a certain index, as does each column. The first row is index 1, the second row index 2, etc.\nThe slice() function expects a vector of row indices to retain:\n\ndatascience |>\n  slice(1:5)\n\n# A tibble: 5 × 17\n  Country   Gender   Age EmploymentStatus EmployerIndustry FormalEducation Major\n  <chr>     <chr>  <dbl> <chr>            <chr>            <chr>           <chr>\n1 United S… Male      56 Independent con… Mix of fields    Master's degree Math…\n2 Russia    Male      33 Employed full-t… Internet-based   Bachelor's deg… Other\n3 Taiwan    Male      26 Employed full-t… Financial        Master's degree Comp…\n4 United S… Male      25 Employed part-t… Academic         Bachelor's deg… Phys…\n5 United S… Male      33 Employed full-t… Telecommunicati… Doctoral degree Elec…\n# ℹ 10 more variables: CompensationAmount <dbl>, CompensationCurrency <chr>,\n#   CurrentJobTitle <chr>, TitleFit <chr>, LanguageRecommendation <chr>,\n#   DataScienceIdentity <chr>, WorkDataVisualizations <chr>,\n#   JobSatisfaction <chr>, JobSatisfaction2 <dbl>, ConversionUSD <dbl>\n\n\n\n\n\n\n\n\nCheck\n\n\n\nWhat is the difference between select() and slice()?"
  },
  {
    "objectID": "live_code/data_wrangling_pt2.html#filtering-to-select-a-subset-of-rows",
    "href": "live_code/data_wrangling_pt2.html#filtering-to-select-a-subset-of-rows",
    "title": "Data wrangling with dplyr",
    "section": "Filtering to select a subset of rows",
    "text": "Filtering to select a subset of rows\nThe slice() function is nice, but unless the rows of your data frame are ordered meaningfully, its actual utility is limited. We might want to look at a set of the cases in which a certain condition is met.\nIn the following code, we use the filter() function to only retain the observations where the person’s Major was Computer Science. This function requires specifying a logical condition, and keeps observations in which the condition is met (i.e. TRUE).\n\ndatascience |>\n  filter(Major == \"Computer Science\")\n\n# A tibble: 681 × 17\n   Country  Gender   Age EmploymentStatus EmployerIndustry FormalEducation Major\n   <chr>    <chr>  <dbl> <chr>            <chr>            <chr>           <chr>\n 1 Taiwan   Male      26 Employed full-t… Financial        Master's degree Comp…\n 2 Poland   Male      29 Employed full-t… Pharmaceutical   Master's degree Comp…\n 3 Iran     Male      31 Employed part-t… Technology       Doctoral degree Comp…\n 4 Brazil   Male      25 Employed full-t… Academic         Master's degree Comp…\n 5 Brazil   Male      32 Employed full-t… Academic         Master's degree Comp…\n 6 Russia   Male      31 Independent con… CRM/Marketing    Some college/u… Comp…\n 7 India    Male      23 Employed full-t… Technology       Master's degree Comp…\n 8 Canada   Male      52 Employed full-t… Academic         Bachelor's deg… Comp…\n 9 Russia   Male      26 Independent con… Military/Securi… Bachelor's deg… Comp…\n10 Czech R… Male      25 Independent con… Internet-based   Master's degree Comp…\n# ℹ 671 more rows\n# ℹ 10 more variables: CompensationAmount <dbl>, CompensationCurrency <chr>,\n#   CurrentJobTitle <chr>, TitleFit <chr>, LanguageRecommendation <chr>,\n#   DataScienceIdentity <chr>, WorkDataVisualizations <chr>,\n#   JobSatisfaction <chr>, JobSatisfaction2 <dbl>, ConversionUSD <dbl>\n\n\n\nMultiple conditions\n\n\n\n\n\n\nExpand\n\n\n\n\n\nWe can also filter for more than one condition at once. Within filter(), the comma , specifies that all conditions must be true. It can be read as “and”. In the following code, we retain cases where someone’s major was Computer Science and they were 30 years old at the time of filling out the survey.\n\ndatascience |>\n  filter(Major == \"Computer Science\", \n         Age == 30) |>\n  select(Major, Age)\n\n# A tibble: 36 × 2\n   Major              Age\n   <chr>            <dbl>\n 1 Computer Science    30\n 2 Computer Science    30\n 3 Computer Science    30\n 4 Computer Science    30\n 5 Computer Science    30\n 6 Computer Science    30\n 7 Computer Science    30\n 8 Computer Science    30\n 9 Computer Science    30\n10 Computer Science    30\n# ℹ 26 more rows\n\n\nIf we just need at least one of multiple conditions to be true, we can use the | operator which stands for “or”:\n\ndatascience |>\n  filter(Major == \"Computer Science\" | \n         Age == 30) |>\n  select(Major, Age)\n\n# A tibble: 765 × 2\n   Major              Age\n   <chr>            <dbl>\n 1 Computer Science    26\n 2 Computer Science    29\n 3 Computer Science    31\n 4 Computer Science    25\n 5 Computer Science    32\n 6 Computer Science    31\n 7 A social science    30\n 8 Computer Science    23\n 9 Biology             30\n10 Computer Science    52\n# ℹ 755 more rows\n\n\n\ndatascience |>\n  filter(Major == \"Computer Science\" | Major == \"Other\",\n         Age == 30) |>\n  select(Major, Age)\n\n# A tibble: 44 × 2\n   Major              Age\n   <chr>            <dbl>\n 1 Computer Science    30\n 2 Computer Science    30\n 3 Computer Science    30\n 4 Computer Science    30\n 5 Computer Science    30\n 6 Computer Science    30\n 7 Computer Science    30\n 8 Computer Science    30\n 9 Other               30\n10 Computer Science    30\n# ℹ 34 more rows"
  },
  {
    "objectID": "live_code/data_wrangling_pt2.html#distinct-to-filter-for-unique-rows",
    "href": "live_code/data_wrangling_pt2.html#distinct-to-filter-for-unique-rows",
    "title": "Data wrangling with dplyr",
    "section": "Distinct to filter for unique rows",
    "text": "Distinct to filter for unique rows\nThe distinct() function requires specifying variables in the data frame, and the function will keep only unique/distinct instances of the variable(s). Unless otherwise specified, it will drop all the other variables.\n\ndatascience |>\n  distinct(FormalEducation)\n\n# A tibble: 5 × 1\n  FormalEducation                                                  \n  <chr>                                                            \n1 Master's degree                                                  \n2 Bachelor's degree                                                \n3 Doctoral degree                                                  \n4 Some college/university study without earning a bachelor's degree\n5 I prefer not to answer                                           \n\ndatascience |>\n  distinct(FormalEducation, Major) |>\n  arrange(FormalEducation)\n\n# A tibble: 58 × 2\n   FormalEducation   Major                                                      \n   <chr>             <chr>                                                      \n 1 Bachelor's degree Other                                                      \n 2 Bachelor's degree Physics                                                    \n 3 Bachelor's degree Information technology, networking, or system administrati…\n 4 Bachelor's degree A social science                                           \n 5 Bachelor's degree Electrical Engineering                                     \n 6 Bachelor's degree Mathematics or statistics                                  \n 7 Bachelor's degree Computer Science                                           \n 8 Bachelor's degree Engineering (non-computer focused)                         \n 9 Bachelor's degree A humanities discipline                                    \n10 Bachelor's degree Management information systems                             \n# ℹ 48 more rows\n\n\n\n\n\n\n\n\nCheck\n\n\n\nWhat variables are by default included in the output from distinct()?"
  },
  {
    "objectID": "live_code/data_wrangling_pt2.html#mutate-to-add-a-new-variable",
    "href": "live_code/data_wrangling_pt2.html#mutate-to-add-a-new-variable",
    "title": "Data wrangling with dplyr",
    "section": "Mutate to add a new variable",
    "text": "Mutate to add a new variable\nIt is typical for us to want to add variables to a given data frame. We do this with the mutate() function. We must specify:\n\nThe name of the new variable and\nHow to calculate the value of that new variable for each observation. This will typically involve operations involving variables already present in the data frame.\n\nWe link the two with an equals sign.\n\ndatascience %>%\n  mutate(compensation_1k = CompensationAmount/1000) |>\n  select(CompensationAmount, compensation_1k)\n\n# A tibble: 2,288 × 2\n   CompensationAmount compensation_1k\n                <dbl>           <dbl>\n 1             250000             250\n 2            1200000            1200\n 3            1100000            1100\n 4              20000              20\n 5             100000             100\n 6             624000             624\n 7             126000             126\n 8             133000             133\n 9              80000              80\n10              15000              15\n# ℹ 2,278 more rows\n\n\n\n\n\n\n\n\nCheck\n\n\n\nWhat exactly is going on in the second line of code?"
  },
  {
    "objectID": "live_code/data_wrangling_pt2.html#counting-to-create-frequency-tables",
    "href": "live_code/data_wrangling_pt2.html#counting-to-create-frequency-tables",
    "title": "Data wrangling with dplyr",
    "section": "Counting to create frequency tables",
    "text": "Counting to create frequency tables\nWe can count the number of instances we observed each level of a given categorical variable:\n\ndatascience |>\n  count(EmployerIndustry)\n\n# A tibble: 16 × 2\n   EmployerIndustry                     n\n   <chr>                            <int>\n 1 Academic                           478\n 2 CRM/Marketing                       70\n 3 Financial                          211\n 4 Government                         137\n 5 Hospitality/Entertainment/Sports    27\n 6 Insurance                           68\n 7 Internet-based                     134\n 8 Manufacturing                       75\n 9 Military/Security                   35\n10 Mix of fields                      195\n11 Non-profit                          35\n12 Other                              198\n13 Pharmaceutical                      54\n14 Retail                              61\n15 Technology                         445\n16 Telecommunications                  65\n\n\n\n\n\n\n\n\nCheck\n\n\n\nHow does the resulting data frame from count() compare to the original data frame we passed in?\n\n\n\nMaking frequency tables useful\nWe typically want to present the counts in ascending or descending order.\n\n\n\n\n\n\nExpand\n\n\n\n\n\nNote that the following chunks of code do the same thing. One of them takes advantage of an additional argument in count(), whereas the other block of the uses an additional function:\n\ndatascience |>\n  count(EmployerIndustry, sort = T)\n\n# A tibble: 16 × 2\n   EmployerIndustry                     n\n   <chr>                            <int>\n 1 Academic                           478\n 2 Technology                         445\n 3 Financial                          211\n 4 Other                              198\n 5 Mix of fields                      195\n 6 Government                         137\n 7 Internet-based                     134\n 8 Manufacturing                       75\n 9 CRM/Marketing                       70\n10 Insurance                           68\n11 Telecommunications                  65\n12 Retail                              61\n13 Pharmaceutical                      54\n14 Military/Security                   35\n15 Non-profit                          35\n16 Hospitality/Entertainment/Sports    27\n\n\n\ndatascience |>\n  count(EmployerIndustry) |>\n  arrange(desc(n))\n\n# A tibble: 16 × 2\n   EmployerIndustry                     n\n   <chr>                            <int>\n 1 Academic                           478\n 2 Technology                         445\n 3 Financial                          211\n 4 Other                              198\n 5 Mix of fields                      195\n 6 Government                         137\n 7 Internet-based                     134\n 8 Manufacturing                       75\n 9 CRM/Marketing                       70\n10 Insurance                           68\n11 Telecommunications                  65\n12 Retail                              61\n13 Pharmaceutical                      54\n14 Military/Security                   35\n15 Non-profit                          35\n16 Hospitality/Entertainment/Sports    27\n\n\n\n\n\n\n\n\nCheck\n\n\n\nWhat happens if you pass in more than one variable into count()?"
  },
  {
    "objectID": "live_code/data_wrangling_pt2.html#practice",
    "href": "live_code/data_wrangling_pt2.html#practice",
    "title": "Data wrangling with dplyr (cont.)",
    "section": "Practice",
    "text": "Practice\nWrite code to create a summary table reporting the average age of survey respondents for each category of formal education, in descending order.\n\n\nCode\ndatascience |>\n  group_by(FormalEducation) |>\n  summarise(avg_age = mean(Age)) |>\n  arrange(desc(avg_age))"
  },
  {
    "objectID": "live_code/data_wrangling_pt2.html#summarising-for-summary-statistics",
    "href": "live_code/data_wrangling_pt2.html#summarising-for-summary-statistics",
    "title": "Data wrangling with dplyr",
    "section": "Summarising for summary statistics",
    "text": "Summarising for summary statistics\nThe summarise() function gives us an easy way to calculate summary statistics of variables in the data frame! We just need to know the name of the function that will calculate the summary statistic for us.\n\ndatascience |>\n  summarise(mean_age = mean(Age))\n\n# A tibble: 1 × 1\n  mean_age\n     <dbl>\n1     34.4\n\n\n\n\nYou can obtain multiple summary statistics at once by separating the desired summary statistics with commas.\nThe summarise() function changes the data frame entirely. It collapses rows down to a summary statistic, and removes all columns that are irrelevant to the calculation.\n\n\n\n\n\n\nCheck\n\n\n\nWhat happens if you type summarise(mean(Age)) instead? You’ll note that the calculation becomes the column title."
  },
  {
    "objectID": "live_code/data_wrangling_pt2.html#remember-to-save-over-data-frames-for-future-use",
    "href": "live_code/data_wrangling_pt2.html#remember-to-save-over-data-frames-for-future-use",
    "title": "Data wrangling with dplyr (cont.)",
    "section": "Remember to save over data frames for future use!",
    "text": "Remember to save over data frames for future use!\nOften, we wrangle a data frame because we want certain variables to exist for future analyses. When we have modify a data frame for several future analyses, we should save over/store back into the same data frame with our new operations. As an example, in the following code, I am creating a new variable called age_months that represents the age in months.\n\ndatascience |>\n  mutate(age_months = Age * 12)\n\nNow suppose I want to work with this variable:\n\ndatascience |>\n  summarise(mean_age_months = mean(age_months))\n\nError in `summarise()`:\nℹ In argument: `mean_age_months = mean(age_months)`.\nCaused by error:\n! object 'age_months' not found\n\n\nIt’s complaining because the data frame datascience does not have a variable called age_months! We need to store over the previous version to make sure we “save our work”:\n\ndatascience <- datascience |>\n  mutate(age_months = Age * 12)"
  },
  {
    "objectID": "live_code/data_wrangling_pt2.html#grouping-by-grouped-operations",
    "href": "live_code/data_wrangling_pt2.html#grouping-by-grouped-operations",
    "title": "Data wrangling with dplyr (cont.)",
    "section": "Grouping by grouped operations",
    "text": "Grouping by grouped operations\nSometimes, we want to look at a given statistic or create a new variable focusing on each level of a specific categorical variable. The group_by() function tells R to treat each unique level as a separate data set.\n\ndatascience |>\n  group_by(FormalEducation) |>\n  summarise(mean_age = mean(Age))\n\n# A tibble: 4 × 2\n  FormalEducation                                                   mean_age\n  <chr>                                                                <dbl>\n1 Bachelor's degree                                                     31.9\n2 Doctoral degree                                                       35.9\n3 Master's degree                                                       33.0\n4 Some college/university study without earning a bachelor's degree     25.8\n\n\nIt’s always important to ungroup() after using group_by()! Otherwise, the grouping with carry on and could lead to potential errors in your future wrangling! Notice the differences in the outputs in the following examples:\n\ndatascience |>\n  group_by(Major) |>\n  mutate(mean_age = mean(Age))|>\n  mutate(mean_comp = mean(CompensationAmount)) |>\n  ungroup() |>\n  select(Major, mean_age, mean_comp) |>\n  arrange(Major)\n\n# A tibble: 102 × 3\n   Major                   mean_age mean_comp\n   <chr>                      <dbl>     <dbl>\n 1 A humanities discipline     60      55000 \n 2 A social science            34      50000 \n 3 Biology                     28.3    18550 \n 4 Biology                     28.3    18550 \n 5 Biology                     28.3    18550 \n 6 Computer Science            28.4  1130131.\n 7 Computer Science            28.4  1130131.\n 8 Computer Science            28.4  1130131.\n 9 Computer Science            28.4  1130131.\n10 Computer Science            28.4  1130131.\n# ℹ 92 more rows\n\ndatascience |>\n  group_by(Major) |>\n  mutate(mean_age = mean(Age)) |>\n  ungroup() |>\n  mutate(mean_comp = mean(CompensationAmount)) |>\n  select(Major, mean_age, mean_comp) |>\n  arrange(Major)\n\n# A tibble: 102 × 3\n   Major                   mean_age mean_comp\n   <chr>                      <dbl>     <dbl>\n 1 A humanities discipline     60     604512.\n 2 A social science            34     604512.\n 3 Biology                     28.3   604512.\n 4 Biology                     28.3   604512.\n 5 Biology                     28.3   604512.\n 6 Computer Science            28.4   604512.\n 7 Computer Science            28.4   604512.\n 8 Computer Science            28.4   604512.\n 9 Computer Science            28.4   604512.\n10 Computer Science            28.4   604512.\n# ℹ 92 more rows"
  },
  {
    "objectID": "live_code/data_wrangling_pt2.html#piping-to-ggplot",
    "href": "live_code/data_wrangling_pt2.html#piping-to-ggplot",
    "title": "Data wrangling with dplyr (cont.)",
    "section": "Piping to ggplot()",
    "text": "Piping to ggplot()\nRemember that when creating plots, ggplot() expects a data frame as its first argument.\nWe may sometimes need to wrangle data prior to visualizing it. We have two options (both have pros and cons):\n\nWrangle the original data, store the resulting data frame as a new object or overwrite the previous one, and then refer to that data frame with ggplot()\n\n\ndatascience_india <- datascience |>\n  filter(Country == \"India\")\n\nggplot(data = datascience_india, mapping = aes(x = Age)) +\n  geom_histogram(bins = 20)\n\n\n\n\n\nWrangle the original data, and then directly pipe the result into ggplot(), which knows to expect a data frame as its first argument:\n\n\n# Notice that we don't specify the data parameter in ggplot()!\ndatascience |>\n  filter(Country == \"India\") |>\n  ggplot(mapping = aes(x = Age)) + \n  geom_histogram(bins = 20)\n\n\n\n\n\n\n\n\nWhen do we use |> and when do we use + to connect lines of code?"
  },
  {
    "objectID": "live_code/data_wrangling_pt2.html#warm-uprecap",
    "href": "live_code/data_wrangling_pt2.html#warm-uprecap",
    "title": "Data wrangling with dplyr (cont.)",
    "section": "Warm-up/Recap:",
    "text": "Warm-up/Recap:\nWrite code to determine many different programming languages were recommended in the data:\n\n\nCode\ndatascience |>\n  distinct(LanguageRecommendation) |>\n  nrow()\n\n\nDisplay a data frame of the respondents who were living in the United States and were at most 35 years old at the time of taking the survey.\n\n\nCode\ndatascience |>\n  filter(Country == \"United States\", Age <= 35)"
  },
  {
    "objectID": "live_code/kaggle_survey_analysis.html",
    "href": "live_code/kaggle_survey_analysis.html",
    "title": "Kaggle survey: group data wrangling",
    "section": "",
    "text": "We will now work a larger subset of the Kaggle data science survey data!"
  },
  {
    "objectID": "live_code/kaggle_survey_analysis.html#group-analysis",
    "href": "live_code/kaggle_survey_analysis.html#group-analysis",
    "title": "Kaggle survey: group data wrangling",
    "section": "Group analysis",
    "text": "Group analysis\nI want your group to generate your own investigation. Using your data-wrangling and plotting skills to do some EDA. After about a half hour, your group will share your process and results with the rest of the class!\nYour final results must include:\n\nA meaningful use of group_by()\nSummary statistics or frequency table\nVisualization with meaningful labels/titles\n\nYou can create more than one visualization and/or more than one table. Whatever speaks to you! The individual components (i.e. table/summary stats vs plot) do not need to use the same set of variables. Feel free to create as many code chunks as you’d like! There is a data dictionary at the bottom of this page that defines all the variables in the data set for you."
  },
  {
    "objectID": "live_code/kaggle_survey_analysis.html#data-dictionary",
    "href": "live_code/kaggle_survey_analysis.html#data-dictionary",
    "title": "Kaggle survey: group data wrangling",
    "section": "Data dictionary",
    "text": "Data dictionary\nBelow is the data dictionary for the subset of the Kaggle data data.\n\nCountry: home country of employee (character)\nGender: specified gender (character)\nAge: age at time of survey (numeric)\nEmploymentStatus: reported employed status (character)\nEmployerIndustry: employer’s industry (character)\nMajor: college major (character)\nCompensationAmount: annual compensation (numeric)\nCompensationCurrency: three-letter currency code (character)\nCurrentJobTitle: job title (character)\nTitleFit: assessment of how well the job title fits (“Fine”, “Perfectly”, “Poorly”)\nLanguageRecommendation: recommended programming language (character)\nDataScienceIdentity: does the respondent identify as a data scientist (character)\nWorkDataVisualizations: proportion of job dedicated to creating data visualizations, broken into pre-determined categories (character)\nJobSatisfaction: rating of job satisfaction on scale of 1-10, where 1 is not satisfied and 10 is highly satisfied (character)\nJobSatisfaction2: numeric version of JobSatisfaction (numeric)\nConversionUSD: conversion factor from CompensationCurrency to USD (numeric)"
  },
  {
    "objectID": "slides/slides-08-probability.html#random-variable",
    "href": "slides/slides-08-probability.html#random-variable",
    "title": "Probability basics",
    "section": "Random variable",
    "text": "Random variable\n\nA random variable is a variable whose value is unknown and depends on random events\n\nOften denoted with a capital letter like \\(X\\) or \\(Y\\)\n\nThere are two types: discrete and continuous (just like in numeric variables)\n\nDiscrete: represents random process where sample space is countable (i.e. finite, or distinct counts)\nContinuous: sample space is uncountable (i.e. can take on any value within a specified interval with infinite number of possible values)\n\nNOTE: we will focus on discrete random variables for now"
  },
  {
    "objectID": "slides/slides-10-simpsons.html#misc.-live-code",
    "href": "slides/slides-10-simpsons.html#misc.-live-code",
    "title": "Simpson’s paradox",
    "section": "Misc. live code",
    "text": "Misc. live code\n\nlibrary(readr)\nurl_file <- \"https://raw.githubusercontent.com/midd-stat201-fall2024/midd-stat201-fall2024.github.io/main/live_code/data/insurance.csv\"\ninsurance <- read_csv(url_file)\n\nWe will return to insurance data to learn about:\n\nUsing wrangling to obtain probabilities\ncase_when() to create more complex categorical variables"
  },
  {
    "objectID": "slides/slides-10-simpsons.html#wrangling-for-probabilities",
    "href": "slides/slides-10-simpsons.html#wrangling-for-probabilities",
    "title": "Simpson’s paradox",
    "section": "Wrangling for probabilities",
    "text": "Wrangling for probabilities\n\n\nWhat is probability that someone is a smoker?\n\ninsurance |>\n  count(smoker) |>\n  mutate(prob = n/sum(n)) |>\n  select(-n)\n\n# A tibble: 2 × 2\n  smoker  prob\n  <chr>  <dbl>\n1 no     0.775\n2 yes    0.225\n\n\n\n\nWhat is the probability that someone is a smoker, conditioned on sex?\n\ninsurance |>\n  count(smoker, sex) |>\n  group_by(sex) |>\n  mutate(cond_prob = n/sum(n)) |>\n  select(-n)\n\n# A tibble: 4 × 3\n# Groups:   sex [2]\n  smoker sex    cond_prob\n  <chr>  <chr>      <dbl>\n1 no     female     0.837\n2 no     male       0.708\n3 yes    female     0.163\n4 yes    male       0.292"
  },
  {
    "objectID": "slides/slides-10-simpsons.html#case_when",
    "href": "slides/slides-10-simpsons.html#case_when",
    "title": "Simpson’s paradox",
    "section": "case_when()",
    "text": "case_when()\nWe will use the case_when() function which generalizes if_else(). We use the following notation: <logical condition> ~ <value of variable>. Different “ifs” are separated by commas, and the logical conditions are checked sequentially.\n\n\n\ninsurance |>\n  mutate(bmi_cat = case_when(\n    bmi < 18.5 ~ \"under\",\n    bmi >= 18.5 & bmi < 25 ~ \"healthy\",\n    bmi >= 25 & bmi < 30 ~ \"over\",\n    bmi >= 30 ~ \"obese\"\n  )) |>\n  select(bmi, bmi_cat) |>\n  slice(1:5)\n\n# A tibble: 5 × 2\n    bmi bmi_cat\n  <dbl> <chr>  \n1  27.9 over   \n2  33.8 obese  \n3  33   obese  \n4  22.7 healthy\n5  28.9 over   \n\n\n\n\n# The following is also acceptable, but \n# relies on sequential ordering:\ninsurance |>\n  mutate(bmi_cat = case_when(\n    bmi < 18.5 ~ \"under\",\n    bmi >= 18.5 & bmi < 25 ~ \"healthy\",\n    bmi >= 25 & bmi < 30 ~ \"over\",\n    T ~ \"obese\" \n  )) |>\n  select(bmi, bmi_cat) |>\n  slice(1:5)\n\n# A tibble: 5 × 2\n    bmi bmi_cat\n  <dbl> <chr>  \n1  27.9 over   \n2  33.8 obese  \n3  33   obese  \n4  22.7 healthy\n5  28.9 over"
  },
  {
    "objectID": "slides/slides-10-simpsons.html#more-complex-categorical-variables",
    "href": "slides/slides-10-simpsons.html#more-complex-categorical-variables",
    "title": "Simpson’s paradox",
    "section": "More complex categorical variables",
    "text": "More complex categorical variables\nSuppose I want to create a new variable representing the categories of BMI, constructed as follows:\n\nunderweight if someone’s BMI is less than 18.5\nhealthy if BMI is 18.5 to less than 25\noverweight if BMI is 25 to less than 30\nobese if BMI is 30 or greater\n\n\n\n# option 1 (awful): nested if_else()\ninsurance |>\n  mutate(bmi_cat = if_else(bmi < 18.5, \"under\",\n                           if_else(bmi >= 18.5 & bmi < 25, \"healthy\",\n                                   if_else(...))))"
  },
  {
    "objectID": "coding-practice-10-simpsons.html",
    "href": "coding-practice-10-simpsons.html",
    "title": "Conditional probabilities coding practice",
    "section": "",
    "text": "Today’s data comes from a study of conducted in Whickham, England. In this study, the researchers recorded each participant’s age, smoking status at the start of the study, and their health outcome 20 years later.\nThe data is in the mosaicData package. You will have to install the package first! Then run the following code:\n\nlibrary(tidyverse)\nlibrary(mosaicData)\n\nWe will work with the Whickham data. You should open its Help file and take a view of the data before proceeding. Note that “factor” can be though of as a categorical variable. Make sure you understand the data before proceeding!\nDiscuss with your group: What would you expect the relationship between smoking status and health outcome to be?\n\nCreate an appropriate visualization depicting the relationship between smoking status and health outcome.\n\n\n\n\n\nUsing wrangling code, calculate the conditional probabilities of death of each smoking status. Please report only the probabilities for when outcome is Dead.\n\n\n\n\nWith your group, briefly describe the relationship and whether or not it is what you expected. You may want to discuss the visualization from the previous exercise as well.\n\nCreate a new variable for future use called age_cat that takes the values as follows:\n\n\n“18-44”: if someone is less than or equal to 44 years old\n“45-64”: if someone is between 45 and 64 years old, inclusive\n“65+”: if someone is older than 64\n\n\n\n\n\nRe-create your first visualization, this time faceting by age_cat. You are welcome to copy-paste code if that is helpful.\n\n\n\n\n\nExtend your table from above by breaking it down by age category. You are welcome to copy-paste code if that is helpful.\n\n\n\n\nWith your group, compare the two visualizations and the two summary tables. What changed, and what might explain the change?"
  },
  {
    "objectID": "slides/slides-09-conditional-probability.html#general-multiplication-rule",
    "href": "slides/slides-09-conditional-probability.html#general-multiplication-rule",
    "title": "Conditional probability",
    "section": "General multiplication rule",
    "text": "General multiplication rule\nConditional, joint, and marginal probabilities are related via the general multiplication rule:\n\n\\[\n\\text{P}(A \\cap B) =\n\\]\n\n\nLet’s see this in the coffee example!\nVery useful for finding probability that two events will happen in sequence.\n\nExample: A box has three tickets, colored red, orange, yellow. We will draw two tickets randomly one-at-a-time without replacement. What is the probability of drawing the red ticket first and then the orange ticket?"
  },
  {
    "objectID": "slides/slides-09-conditional-probability.html#tree-diagram",
    "href": "slides/slides-09-conditional-probability.html#tree-diagram",
    "title": "Conditional probability",
    "section": "Tree diagram",
    "text": "Tree diagram\nTool to organize outcomes and probabilities around the structure of the data. Useful when outcomes occur sequentially, and outcomes are conditioned on predecessors. Let’s do an example:\n\nA class has a midterm and a final exam. 13% of students earned an A on the midterm. Of those students who earned an A on the midterm, 47% received an A on the final. Of those student who earned below an A on the midterm, 11% received an A on the final. You randomly pick up a final exam and notice the student received an A. What is the probability that they earned an A on the midterm?\n\nUsing \\(\\text{P}()\\) notation, what probability are we interested in? What probabilities do we need to calculate along the way?\n\nLet’s construct our tree!\n\nIn the tree diagram, where are the three types of probabilities appearing?"
  },
  {
    "objectID": "midterms.html",
    "href": "midterms.html",
    "title": "Midterms",
    "section": "",
    "text": "When and where: Wednesday, Nov. 20 and Thursday, Nov. 21 during class (75 minutes each)\n\nWednesday will be the written portion\nThursday will be coding-focused (i.e. the majority of questions will require you to use R in some way)\n\nA starter .Rmd template will be made available via the Canvas assignment associated with the midterm.\nBe sure to type your name in the Honor Code statement provided at the top of the document\nYou will be required to submit both the completed .Rmd and the knitted output to Canvas by the end of the allotted time. Be sure to budget this time in appropriately.\n\nImportant: Prof. Tang will be proctoring the exam. On Thursday, you can expect Prof. Tang to walk around the room several times in order discourage the temptation of using Google/ChatGPT/other AI tools\n\nWhat: focusing on content from Week 5 through the beginning of Week 10 (i.e. all inference topics, Normal distribution, and SLR)\n\nLightly cumulative (but no probability problems like those on Midterm 1)\nFeel comfortable making plots and wrangling data\n\n\n\n\n\n\nFor Wednesday: you can bring a single-sided 8.5”x11” formula sheet. This sheet should include formulas only! No worked out examples, pictures, descriptions, or definitions are allowed. You will be asked to submit the formula sheet alongside the midterm on Wednesday. The formula sheet will be returned to you for your midterm on Thursday, so be sure to write your name on your sheet.\nFor Thursday: during the coding component of the midterm, you are welcome to use/access your own code and any code posted on the course website. Aside from accessing the course website, the midterm is not open internet. Once again, use of Google/ChatGPT/other AI Tools during the midterm is strictly forbidden and would be in violation of the Honor Code.\n\nYou should bring a calculator to day 1 of the midterm. If you do not have one, the department can provide very basic calculators.\n\nYou will be provided the following two tables alongside your exam: a table of z-score probabilities and table of \\(t\\)-distribution percentiles. We will go over how to read them in class, but make sure you feel comfortable with them before the midterm!\n\nYou should bring a fully charged laptop to day 2 of the midterm.\n\n\n\n\n\n\nThe best preparation you can do for the midterm is to go back through your notes and homework and be honest with yourself about what you do/don’t understand. This means going through the painful process of looking at feedback on Canvas. For the concepts that you need to practice more, try more problems (see below)!\nTry answering some of the comprehension questions at the end of the slides. You should expect some questions on the midterm that assess your “big picture” understanding of the material.\nAs you’re studying, try making a rough draft of your formula sheet. Then use that formula sheet (rather than your notes) when attempting new practice problems. This will more accurately simulate the test-taking experience.\n\nBased on your experiences and practice, you should make a new version of the formula sheet. Keep iterating until you’ve found one that works for you.\nPractice the problems using the provided tables of z-score probabilities and t-distribution percentiles.\n\nFor coding, go back through your old .Rmds and make sure you understand everything that is happening! You might also consider re-doing a coding problem without looking at your previous code.\nMore practice problems. These problems are representative of what you can expect on the midterm:\n\nPractice problems involving coding\nPractice problems written"
  },
  {
    "objectID": "midterms.html#preparation",
    "href": "midterms.html#preparation",
    "title": "Midterms",
    "section": "Preparation",
    "text": "Preparation\n\nThe best preparation you can do for the midterm is to go back through your notes and homework and be honest with yourself about what you do/don’t understand. This means going through the painful process of looking at feedback on Canvas. For the concepts that you need to practice more, try more problems (see below)!\nAnother way to prepare is to create your own study guide with summaries and examples of important concepts. As you study, it would be a good diea to compile a list of questions that you might have.\nWork on the practice problems that were distributed at the end of classes but not assigned.\nFor extra practice, additional review problems will soon be made available below. These questions are not necessarily representative of the typical scope and difficulty of individual exam questions. This review is not comprehensive, nor does it represent the expected amount of time for it will take for you to complete the midterm.\n\nExtra problems here and here\nVideo recap of probability"
  },
  {
    "objectID": "index.html#announcements",
    "href": "index.html#announcements",
    "title": "Advanced Introduction to Statistics and Data Science",
    "section": "Announcements",
    "text": "Announcements\n\nSee this document for knitting more beautiful tables.\nIf your group would like focused time with Prof. Tang, sign up for 15-minute meetings here: Calendly link\n\nFeel free to book back-to-back meetings if you think that is necessary\n\nMidterm 2 revisions (written portion only)\n\nRevisions are due to Prof. Tang by Friday 12:00pm at the latest. There will be a box outside the office for you to place the revisions if she is unavailable.\nPlease submit your original midterm alongside your revisions. Ensure the problem numbers are clearly indicated, and write and sign the Honor Code statement on your revision.\nYou are not allowed to consult other students nor TAs, but you may ask Prof. Tang for clarifications and use your notes.\nQuestions that are available for revision are: 2a, 2d, 3a, and any two multiple choice questions of your choice\n\nFor multiple choice questions, you must explain/justify/show work for why your new choice is correct\nFor all other questions, you should completely re-do the part you are revising. The exception is if your only revision is to check conditions. In this case, you do not need to re-do the whole part\n\nUp to 25% of points back are available (depending on quality of revision)"
  },
  {
    "objectID": "slides/slides-09-conditional-probability.html#recap",
    "href": "slides/slides-09-conditional-probability.html#recap",
    "title": "Conditional probability",
    "section": "Recap",
    "text": "Recap\n\nTwo events are disjoint/mutually exclusive if they do not have any overlapping outcomes\nAddition rule: \\(\\text{Pr}(A \\cup B) =\\)\nComplement rule: \\(\\text{Pr}(A^c) =\\)"
  },
  {
    "objectID": "slides/slides-09-conditional-probability.html#law-of-total-probability",
    "href": "slides/slides-09-conditional-probability.html#law-of-total-probability",
    "title": "Conditional probability",
    "section": "Law of Total Probability",
    "text": "Law of Total Probability\n\nLet \\(A\\) be an event, then let \\(\\{B_{1},B_{2},\\ldots, B_{k}\\}\\) be a set of mutually exclusive events whose union comprises their entire sample space \\(S\\)\nThen Law of Total Probability (LoTP) says:\n\n\n\\[\n\\text{Pr}(A) = \\text{Pr}(A \\cap B_{1} ) + \\text{Pr}(A \\cap B_{2}) + \\ldots + \\text{Pr}(A \\cap B_{k})\n\\]\n\n\nBlob picture"
  },
  {
    "objectID": "slides/slides-10-simpsons.html#uc-berkeley-admissions",
    "href": "slides/slides-10-simpsons.html#uc-berkeley-admissions",
    "title": "Simpson’s paradox",
    "section": "UC Berkeley admissions",
    "text": "UC Berkeley admissions\nObservational study on sex bias based on Fall 1973 admissions data to the graduate program at the University of California, Berkeley\n\n\n\n\nAdmit\nDeny\nTotal\n\n\n\n\nMen\n3738\n4704\n8442\n\n\nWomen\n1494\n2827\n4321\n\n\nTotal\n5232\n7531\n12763\n\n\n\n\n\nWhat is the probability of admission for a randomly selected applicant?\nWhat is the probability of admission among men? Among women?\nAre the probabilities you found marginal, joint, or conditional probabilities?\n\n\n\n\nSuppose we want to understand the relationship between gender and admission decision. What sort of visualization might be appropriate for representing this data?"
  },
  {
    "objectID": "slides/slides-10-simpsons.html#section",
    "href": "slides/slides-10-simpsons.html#section",
    "title": "Simpson’s paradox",
    "section": "",
    "text": "Dept\nFemale: Admit\nMale: Admit\nFemale: Reject\nMale: Reject\n\n\n\n\nA\n89\n512\n19\n313\n\n\nB\n17\n353\n8\n207\n\n\nC\n202\n120\n391\n205\n\n\nD\n131\n138\n244\n279\n\n\nE\n94\n53\n299\n138\n\n\nF\n24\n22\n317\n351"
  },
  {
    "objectID": "slides/slides-10-simpsons.html#frequency-tables",
    "href": "slides/slides-10-simpsons.html#frequency-tables",
    "title": "Simpson’s paradox",
    "section": "Frequency tables",
    "text": "Frequency tables\nNumber of applicants by department:\n\n\nFemale applicants:\n\n\n\n\n\nDept\nn\n\n\n\n\nA\n108\n\n\nB\n25\n\n\nC\n593\n\n\nD\n375\n\n\nE\n393\n\n\nF\n341\n\n\n\n\n\n\nMale applicants:\n\n\n\n\n\nDept\nn\n\n\n\n\nA\n825\n\n\nB\n560\n\n\nC\n325\n\n\nD\n417\n\n\nE\n191\n\n\nF\n373"
  },
  {
    "objectID": "slides/slides-10-simpsons.html#more-detailed-frequency-tables",
    "href": "slides/slides-10-simpsons.html#more-detailed-frequency-tables",
    "title": "Simpson’s paradox",
    "section": "More-detailed frequency tables",
    "text": "More-detailed frequency tables\nNumber of applicants by department and admission status:\n\n\nFemale applicants:\n\n\n\n\n \n  \n    Dept \n    Decision \n    n \n  \n \n\n  \n    A \n    Admit \n    89 \n  \n  \n    A \n    Reject \n    19 \n  \n  \n    B \n    Admit \n    17 \n  \n  \n    B \n    Reject \n    8 \n  \n  \n    C \n    Admit \n    202 \n  \n  \n    C \n    Reject \n    391 \n  \n  \n    D \n    Admit \n    131 \n  \n  \n    D \n    Reject \n    244 \n  \n  \n    E \n    Admit \n    94 \n  \n  \n    E \n    Reject \n    299 \n  \n  \n    F \n    Admit \n    24 \n  \n  \n    F \n    Reject \n    317 \n  \n\n\n\n\n\n\nMale applicants:\n\n\n\n\n \n  \n    Dept \n    Decision \n    n \n  \n \n\n  \n    A \n    Admit \n    512 \n  \n  \n    A \n    Reject \n    313 \n  \n  \n    B \n    Admit \n    353 \n  \n  \n    B \n    Reject \n    207 \n  \n  \n    C \n    Admit \n    120 \n  \n  \n    C \n    Reject \n    205 \n  \n  \n    D \n    Admit \n    138 \n  \n  \n    D \n    Reject \n    279 \n  \n  \n    E \n    Admit \n    53 \n  \n  \n    E \n    Reject \n    138 \n  \n  \n    F \n    Admit \n    22 \n  \n  \n    F \n    Reject \n    351"
  },
  {
    "objectID": "coding_practice/coding-practice-10-simpsons.html",
    "href": "coding_practice/coding-practice-10-simpsons.html",
    "title": "Conditional probabilities coding practice",
    "section": "",
    "text": "Today’s data comes from a study of conducted in Whickham, England. In this study, the researchers recorded each participant’s age, smoking status at the start of the study, and their health outcome 20 years later.\nThe data is in the mosaicData package. You may have to install the package first! Then run the following code:\n\nlibrary(tidyverse)\nlibrary(mosaicData)\n\nWe will work with the Whickham data. You should open its Help file and take a view of the data before proceeding. Note that “factor” can be though of as a categorical variable. Make sure you understand the data before proceeding!\nDiscuss with your group: What would you expect the relationship between smoking status and health outcome to be?\n\nCreate an appropriate visualization depicting the relationship between smoking status and health outcome. Make sure you have informative labels and titles.\n\n\n\n\n\nUsing wrangling code, calculate the conditional probabilities of death of each smoking status. Your resulting table/data frame should only retain the variables for smoke status, outcome, and the conditional probabilities in a meaningful order. Also, please report only the probabilities for when outcome is Dead.\n\n\n\n\nWith your group, briefly describe the relationship and whether or not it is what you expected. You may want to discuss the visualization from the previous exercise as well.\n\nUsing case_when(), create a new variable for future use called age_cat that takes the values as follows:\n\n\n“18-44”: if someone is less than or equal to 44 years old\n“45-64”: if someone is between 45 and 64 years old, inclusive\n“65+”: if someone is older than 64\n\n\n\n\n\nRe-create your first visualization from Exercise 1, this time faceting by age_cat. Make sure you have informative labels and titles.\n\n\n\n\n\nElaborate on your table from Exercise 2 above by breaking it down by age category. Your resulting table/data frame should only retain the variables for smoke status, age category, outcome, and the conditional probabilities in a meaningful order. Once again, please report only the probabilities for when outcome is Dead.\n\n\n\n\nWith your group, compare the two visualizations and the two summary tables. What changed, and what might explain the change?\nAnswer:\nWhen finished, knit one more time and submit the HTML to Canvas!"
  },
  {
    "objectID": "slides/slides-10-simpsons.html#uc-berkeley-admissions-cont.",
    "href": "slides/slides-10-simpsons.html#uc-berkeley-admissions-cont.",
    "title": "Simpson’s paradox",
    "section": "UC Berkeley admissions (cont.)",
    "text": "UC Berkeley admissions (cont.)"
  },
  {
    "objectID": "slides/slides-11-bootstrap.html#where-we-are-going",
    "href": "slides/slides-11-bootstrap.html#where-we-are-going",
    "title": "Variability of statistic + Intro to Bootstrap",
    "section": "Where we are going",
    "text": "Where we are going\nWe are leaving the world of EDA and beginning to enter the world of inference and modeling!\n\nWant to answer questions about a population, but must rely on a sample\nCollect data from sample –> calculate statistics\nWhat can we say about the statistics?\nData are random! So how sure are we about our conclusions?\n\n\nStatistics starts here!"
  },
  {
    "objectID": "slides/slides-11-bootstrap.html#inferential-questions",
    "href": "slides/slides-11-bootstrap.html#inferential-questions",
    "title": "Variability of statistic + Intro to Bootstrap",
    "section": "Inferential questions",
    "text": "Inferential questions\n\nI want to know the true average number of hours of sleep Middlebury students get a night. Based on a sample of students, what might be a “good estimate” of the true average?\nIs the true average number of of hours of Middlebury students get a night less than 7 hours?\n\nQuestions here are about population parameter (in this case, \\(\\mu\\))\nAll we have access to is the data \\(x_{1}, x_{2},\\ldots, x_{n}\\) from which we can calculate some statistics"
  },
  {
    "objectID": "slides/slides-11-bootstrap.html#activity",
    "href": "slides/slides-11-bootstrap.html#activity",
    "title": "Variability of statistic + Intro to Bootstrap",
    "section": "Activity",
    "text": "Activity\nWhile you’re coming into the room, please take:\n\n1 pink card\n1 white card\n\n\nOn the pink card, write down an estimate of the average number of hours of sleep you received this past week.\nOn white card, write a 1 if this number you wrote down on the pink card is greater than or equal to 7, and a 0 otherwise\nThen bring these to Prof. Tang"
  },
  {
    "objectID": "slides/slides-11-bootstrap.html#variability-of-statistic",
    "href": "slides/slides-11-bootstrap.html#variability-of-statistic",
    "title": "Variability of statistic + Intro to Bootstrap",
    "section": "Variability of statistic",
    "text": "Variability of statistic\n\nTwo datasets collected under identical procedures will differ. As a result, value of the point estimate we obtain are also different\n\nActivity cont.\n\nThus, there exists the notion of a sampling distribution of the statistic: how the statistic behaves under repeated random samples obtained via the same sampling procedure\n\nThe variability associated with the sampling distribution of the statistic is called the standard error\n\nNote: “error” \\(\\neq\\) bad\n\nThis is in contrast to the standard deviation, which describes variability in the individual data points and not the statistic\n\nPopulation distribution vs. sample distribution vs. sampling distribution"
  },
  {
    "objectID": "slides/slides-11-bootstrap.html#sampling-distribution",
    "href": "slides/slides-11-bootstrap.html#sampling-distribution",
    "title": "Variability of statistic + Intro to Bootstrap",
    "section": "Sampling distribution",
    "text": "Sampling distribution\n\nOf course, sampling distribution of the statistic depends on underlying distribution of the population\nSometimes, we assume that the population/data have a very specific behavior, and this allows us to exactly define/quantify the sampling distribution\n\nWe will see this in a couple of weeks\n\nIf we don’t want to make assumptions, what do we do?\n\nCould conduct a census! That way we can answer any questions we want about the population. But that’s impractical…\nHow to obtain more samples cheaply and quickly?"
  },
  {
    "objectID": "slides/slides-11-bootstrap.html#procedure",
    "href": "slides/slides-11-bootstrap.html#procedure",
    "title": "Variability of statistic + Intro to Bootstrap",
    "section": "Procedure",
    "text": "Procedure\n\nAssume we have a sample \\(x_{1}, x_{2}, \\ldots, x_{n}\\) from the population. Call this sample \\(\\vec{x}\\). Note the sample size is \\(n\\)\nChoose a large number \\(B\\). For \\(b\\) in \\(1,2, \\ldots, B\\):\n\nResample: take a sample of size \\(n\\) with replacement from \\(\\vec{x}\\). Call this set of resampled data \\(\\vec{x}^*_{b}\\)\nCalculate: calculate and record the statistic of interest from \\(\\vec{x}^{*}_{b}\\)\n\n\n\nAt the end of this procedure, we will have a distribution of resample or bootstrap statistics"
  },
  {
    "objectID": "slides/slides-11-bootstrap.html#activity-1",
    "href": "slides/slides-11-bootstrap.html#activity-1",
    "title": "Variability of statistic + Intro to Bootstrap",
    "section": "Activity",
    "text": "Activity\n\nTarget population:\nSampling method:\nPopulation parameter:\nStatistics we can calculate:"
  },
  {
    "objectID": "slides/slides-11-bootstrap.html#why-resample-with-replacement",
    "href": "slides/slides-11-bootstrap.html#why-resample-with-replacement",
    "title": "Variability of statistic + Intro to Bootstrap",
    "section": "Why resample with replacement?",
    "text": "Why resample with replacement?\n\nWe want to understand the sampling error of the sampling distribution!\n\nWhat would the bootstrap samples \\(\\vec{x}^*_b\\) look like if we sampled without replacement?\n\n\nSampling without replacement -> zero variation in the resampled statistics\n\nResampling with replacement will give us “new” datasets that are similar to original sample distribution but not exactly the same!"
  },
  {
    "objectID": "slides/slides-11-bootstrap.html#remarks",
    "href": "slides/slides-11-bootstrap.html#remarks",
    "title": "Variability of statistic + Intro to Bootstrap",
    "section": "Remarks",
    "text": "Remarks\n\n\nRelies on having a representative original sample!\n\n\nResampling from initial sample should be roughly equivalent to sampling directly from the population\n\nRequires computational tools!\n\nWe need \\(B\\) to be large enough to accurately capture variability. \\(B=5000\\) or \\(B=10000\\) sufficient in this class\nMore complex problems will require larger \\(B\\)\n\nBootstrapping can fail!\nBootstrapping is not a solution to small sample sizes!!"
  },
  {
    "objectID": "slides/slides-11-bootstrap.html#point-estimate",
    "href": "slides/slides-11-bootstrap.html#point-estimate",
    "title": "Variability of statistic + Intro to Bootstrap",
    "section": "Point estimate",
    "text": "Point estimate\n\n\\(\\bar{x}\\) is often times a sensible estimate for \\(\\mu\\)\n\\(\\bar{x}\\) is an example of a point estimate: a single number used to estimate a true but unknown population parameter\n\ni.e. a point estimate is a statistic with a specific purpose\nOther examples include \\(s\\) for \\(\\sigma\\), observed proportion \\(\\hat{p}\\) for true proportion \\(p\\),\n\n\nWhat are desirable characteristics of a “good” point estimate?\n\n\nDo we believe that \\(\\bar{x} = \\mu\\) or \\(\\hat{p} = p\\)?"
  },
  {
    "objectID": "slides/slides-11-bootstrap.html#demonstration",
    "href": "slides/slides-11-bootstrap.html#demonstration",
    "title": "Variability of statistic + Intro to Bootstrap",
    "section": "Demonstration",
    "text": "Demonstration\n\nActivity cont.\nLive code demonstration"
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html#housekeeping",
    "href": "slides/slides-12-bootstrap_ci.html#housekeeping",
    "title": "Bootstrap Confidence Intervals",
    "section": "Housekeeping",
    "text": "Housekeeping\n\nOffice hours 3-4pm\nMidterm tomorrows! Bring a calculator."
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html#bootstrap-recap",
    "href": "slides/slides-12-bootstrap_ci.html#bootstrap-recap",
    "title": "Bootstrap Confidence Intervals",
    "section": "Bootstrap recap",
    "text": "Bootstrap recap\nTaking new samples each time is costly! Bootstrap distribution is an approximation of the sampling distribution of the statistic!\nProcedure:\n\nAssume we have a sample \\(x_{1}, x_{2}, \\ldots, x_{n}\\) from the population. Call this sample \\(\\vec{x}\\). Note the sample size is \\(n\\)\nChoose a large number \\(B\\). For \\(b\\) in \\(1,2, \\ldots, B\\):\n\nResample: take a sample of size \\(n\\) with replacement from \\(\\vec{x}\\). Call this set of resampled data \\(\\vec{x}^*_{b}\\)\nCalculate: calculate and record the statistic of interest from \\(\\vec{x}^{*}_{b}\\)\n\n\n\nAt the end of this procedure, we will have a distribution of resample or bootstrap statistics"
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html#confidence-intervals",
    "href": "slides/slides-12-bootstrap_ci.html#confidence-intervals",
    "title": "Bootstrap Confidence Intervals",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nAnalogy: would you rather go fishing with a single pole or a large net?\n\nA range of plausible values gives us a better chance at capturing the parameter\n\nA confidence interval provides such a range of values (more rigorous definition coming soon)\n\n“Interval” = we specify a lower bound and an upper bound\nConfidence intervals are not unique! Depending on the method you use, you might get different intervals"
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html#bootstrap-confidence-intervals",
    "href": "slides/slides-12-bootstrap_ci.html#bootstrap-confidence-intervals",
    "title": "Bootstrap Confidence Intervals",
    "section": "Bootstrap confidence intervals",
    "text": "Bootstrap confidence intervals\n\n\n\nLet’s continue with the data collected from our activity. We have the following bootstrap distribution of sample means, obtained from \\(B=\\) 5000 iterations:"
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html#bootstrap-distribution",
    "href": "slides/slides-12-bootstrap_ci.html#bootstrap-distribution",
    "title": "Bootstrap Confidence Intervals",
    "section": "Bootstrap distribution",
    "text": "Bootstrap distribution\n\n\n\nLet’s continue with the data collected from our activity. We have the following bootstrap distribution of sample proportions, obtained from \\(B=\\) 5000 iterations:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhere is the bootstrap distribution centered?"
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html#bootstrap-percentile-interval",
    "href": "slides/slides-12-bootstrap_ci.html#bootstrap-percentile-interval",
    "title": "Bootstrap Confidence Intervals",
    "section": "Bootstrap percentile interval",
    "text": "Bootstrap percentile interval\n\nThe \\(\\gamma \\times 100\\)% bootstrap percentile interval is obtained by finding the bounds of the middle \\(\\gamma \\times 100\\)% of the bootstrap distribution\n\n\ne.g. If I want a 90% bootstrap percentile interval, where would the bounds be?\n\n\nCalled “percentile interval” because the bounds are the \\((1-\\gamma)/2\\) and \\((1+\\gamma)/2\\) percentiles of the bootstrap distribution\n\ne.g. if \\(\\gamma = 0.80\\), then the bounds would be \\((1-0.80)/2 = 0.10\\) and \\((1+0.80)/2 = 0.90\\) percentiles\n\nFor our purposes, “bootstrap confidence interval” will be equivalent to “bootstrap percentile interval”"
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html#interpreting-a-confidence-interval",
    "href": "slides/slides-12-bootstrap_ci.html#interpreting-a-confidence-interval",
    "title": "Bootstrap Confidence Intervals",
    "section": "Interpreting a confidence interval",
    "text": "Interpreting a confidence interval\n\nOur 90% confidence interval is: (0.3, 0.8) or (0.5, 0.9). Does this mean there is a 90% chance/probability that the true proportion lies in the interval?\n\n\nAnswer: NO\n\n\nRemember: bootstrap distribution is based on our original sample\n\nIf we started with a different original sample \\(\\vec{x}\\), then our estimated 90% confidence interval would also be different\n\n\nWhat a confidence interval (CI) represents: if we take many independent repeated samples from this population using the same method and calculate a \\(\\gamma \\times 100\\) % CI for the parameter in the exact same way, then in theory, \\(\\gamma \\times 100\\) % of these intervals should capture/contain the parameter\n\n\n\\(\\gamma\\) represents the long-run proportion of CIs that theoretically contain the true parameter\nHowever, we never know if any particular interval(s) actually do!"
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html#interpreting-a-confidence-interval-cont.",
    "href": "slides/slides-12-bootstrap_ci.html#interpreting-a-confidence-interval-cont.",
    "title": "Bootstrap Confidence Intervals",
    "section": "Interpreting a confidence interval (cont.)",
    "text": "Interpreting a confidence interval (cont.)\n\nCorrect interpretation (generic): We are \\(\\gamma \\times 100\\) % confident that the population parameter is between the lower bound and upper bound.\n\n\nInterpret our bootstrap CI in context\n\nAgain: why is this interpretation incorrect? “There is a 90% chance/probability that the true parameter value lies in the interval.”"
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html#obtaining-bootstrap-confidence-interval",
    "href": "slides/slides-12-bootstrap_ci.html#obtaining-bootstrap-confidence-interval",
    "title": "Bootstrap Confidence Intervals",
    "section": "Obtaining bootstrap confidence interval",
    "text": "Obtaining bootstrap confidence interval\n\n\nSection A 90% confidence interval for \\(p_{A}\\): (0.3, 0.8)\nSection B 90% confidence interval for \\(p_{B}\\): (0.5, 0.9)"
  },
  {
    "objectID": "slides/slides-11-bootstrap.html",
    "href": "slides/slides-11-bootstrap.html",
    "title": "Variability of statistic + Intro to Bootstrap",
    "section": "",
    "text": "Midterm review problems released\nWednesday office hours 3-4pm\nMidterm this Thursday in class! Bring a calculator.\n\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nRows: 4526 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Decision, Gender, Dept\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nWe are leaving the world of EDA and beginning to enter the world of inference and modeling!\n\nWant to answer questions about a population, but must rely on a sample\nCollect data from sample –> calculate statistics\nWhat can we say about the statistics?\nData are random! So how sure are we about our conclusions?\n\n\nStatistics starts here!\n\n\n\n\nStatistical inference is the process of using sample data to make conclusions about the underlying population the sample came from\n\nEstimation: using the sample to estimate a plausible values for the unknown parameter\nTesting: evaluating whether our observed sample provides evidence for or against some claim about the population\n\n\n\n\n\nI want to know the true average number of hours of sleep Middlebury students get a night. Based on a sample of students, what might be a “good estimate” of the true average?\nIs the true average number of of hours of Middlebury students get a night less than 7 hours?\n\nQuestions here are about population parameter (in this case, \\(\\mu\\))\nAll we have access to is the data \\(x_{1}, x_{2},\\ldots, x_{n}\\) from which we can calculate some statistics\n\n\n\n\n\n\nTarget population:\nSampling method:\nPopulation parameter:\nStatistics we can calculate:\n\n\n\n\n\n\\(\\bar{x}\\) is often times a sensible estimate for \\(\\mu\\)\n\\(\\bar{x}\\) is an example of a point estimate: a single number used to estimate a true but unknown population parameter\n\ni.e. a point estimate is a statistic with a specific purpose\nOther examples include \\(s\\) for \\(\\sigma\\), observed proportion \\(\\hat{p}\\) for true proportion \\(p\\),\n\n\nWhat are desirable characteristics of a “good” point estimate?\n\n\nDo we believe that \\(\\bar{x} = \\mu\\)?\n\n\n\n\n\n\nTwo datasets collected under identical procedures will differ. As a result, value of the point estimate we obtain are also different\n\nActivity cont.\n\nThus, there exists the notion of a sampling distribution of the statistic: how the statistic behaves under repeated random samples obtained via the same sampling procedure\n\nThe variability associated with the sampling distribution of the statistic is called the standard error\n\nNote: “error” \\(\\neq\\) bad\n\nThis is in contrast to the standard deviation, which describes variability in the individual data points and not the statistic\n\nPopulation distribution vs. sample distribution vs. sampling distribution\n\n\n\n\n\nOf course, sampling distribution of the statistic depends on underlying distribution of the population\nSometimes, we assume that the population/data have a very specific behavior, and this allows us to exactly define/quantify the sampling distribution\n\nWe will see this in a couple of weeks\n\nIf we don’t want to make assumptions, what do we do?\n\nCould conduct a census! That way we can answer any questions we want about the population. But that’s impractical…\nHow to obtain more samples cheaply and quickly?"
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html",
    "href": "slides/slides-12-bootstrap_ci.html",
    "title": "Bootstrap Confidence Intervals",
    "section": "",
    "text": "Office hours 3-4pm\nMidterm tomorrows! Bring a calculator."
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html#remarks",
    "href": "slides/slides-12-bootstrap_ci.html#remarks",
    "title": "Bootstrap Confidence Intervals",
    "section": "Remarks",
    "text": "Remarks\n\n\nWhat is a virtue of a “good” confidence interval?\n\n\nHow do you expect the interval to change as the original sample size \\(n\\) changes?\nHow do you expect the interval to change as level of confidence \\(\\gamma\\) changes?\n\nOnce again, relies on a representative original sample!"
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html#your-turn",
    "href": "slides/slides-12-bootstrap_ci.html#your-turn",
    "title": "Bootstrap Confidence Intervals",
    "section": "Your turn!",
    "text": "Your turn!"
  },
  {
    "objectID": "slides/slides-11-bootstrap.html#inference",
    "href": "slides/slides-11-bootstrap.html#inference",
    "title": "Variability of statistic + Intro to Bootstrap",
    "section": "Inference",
    "text": "Inference\nStatistical inference is the process of using sample data to make conclusions about the underlying population the sample came from\n\nEstimation: using the sample to estimate a plausible values for the unknown parameter\nTesting: evaluating whether our observed sample provides evidence for or against some claim about the population"
  },
  {
    "objectID": "slides/slides-11-bootstrap.html#estimation-questions",
    "href": "slides/slides-11-bootstrap.html#estimation-questions",
    "title": "Variability of statistic + Intro to Bootstrap",
    "section": "Estimation questions",
    "text": "Estimation questions\n\nI want to know the true average number of hours of sleep Middlebury students get a night.\n\nBased on a sample of students, what might be a “good estimate” of the true average?\n\nWhat proportion of Middlebury students get a night less than 7 hours?\n\nBased on a sample of students, what might be a “good estimate” of the true proportion?\n\nQuestions here are about population parameter\n\nAll we have access to is the data \\(x_{1}, x_{2},\\ldots, x_{n}\\) from which we can calculate some statistics"
  },
  {
    "objectID": "live_code/bootstrap_dist.html",
    "href": "live_code/bootstrap_dist.html",
    "title": "Bootstrap distribution",
    "section": "",
    "text": "library(tidyverse)\n# our original sample\nx_orig <- c(1, 1, 1, 0, 1)\n\n# sample size stored as variable for reproducibility\nn <- length(x_orig)\n\n# number of bootstrap samples to take\nB <- 1000\n\n# vector to store bootstrap statistics. Starts off as vector full of NAs of length B\nbootstrap_props <- rep(NA, B)\n\nfor(b in 1:B){\n  # resample\n  x_boot <- sample(x = x_orig, size = n, replace = TRUE)\n  \n  # calculate and store bootstrap statistic\n  bootstrap_props[b] <- mean(x_boot)\n}\n\n# visualize\ndata.frame(props = bootstrap_props) |>\n  ggplot(aes(x = props))+\n  geom_histogram(binwidth =  0.2 ) +\n  labs(title = \"Bootstrap distribution of sample proportions\")"
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html#why-bootstrap",
    "href": "slides/slides-12-bootstrap_ci.html#why-bootstrap",
    "title": "Bootstrap Confidence Intervals",
    "section": "Why bootstrap?",
    "text": "Why bootstrap?\n\nSample distribution describes how statistic behaves under repeated sampling from population\nLet’s continue with the data collected from our activity.\n\nI repeatedly take SRS of \\(n=5\\) values from the population and calculate \\(\\hat{p}\\). Sampling distribution of \\(\\hat{p}\\) is as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTaking new samples each time is costly! Bootstrap distribution is an approximation of the sampling distribution of the statistic"
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html#bootstrap-procedure-recap",
    "href": "slides/slides-12-bootstrap_ci.html#bootstrap-procedure-recap",
    "title": "Bootstrap Confidence Intervals",
    "section": "Bootstrap procedure recap",
    "text": "Bootstrap procedure recap\n\nAssume we have a sample \\(x_{1}, x_{2}, \\ldots, x_{n}\\) from the population. Call this sample \\(\\vec{x}\\). Note the sample size is \\(n\\)\nChoose a large number \\(B\\). For \\(b\\) in \\(1,2, \\ldots, B\\):\n\nResample: take a sample of size \\(n\\) with replacement from \\(\\vec{x}\\). Call this set of resampled data \\(\\vec{x}^*_{b}\\)\nCalculate: calculate and record the statistic of interest from \\(\\vec{x}^{*}_{b}\\)\n\n\n\nAt the end of this procedure, we will have a distribution of resample or bootstrap statistics"
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html#bootstrap-distribution-from-activity",
    "href": "slides/slides-12-bootstrap_ci.html#bootstrap-distribution-from-activity",
    "title": "Bootstrap Confidence Intervals",
    "section": "Bootstrap distribution from activity",
    "text": "Bootstrap distribution from activity\n\n\n\nWe have the following bootstrap distribution of sample proportions, obtained from \\(B=\\) 5000 iterations:"
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html#bootstrap-dist.-continued",
    "href": "slides/slides-12-bootstrap_ci.html#bootstrap-dist.-continued",
    "title": "Bootstrap Confidence Intervals",
    "section": "Bootstrap dist. continued",
    "text": "Bootstrap dist. continued\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice where the bootstrap distribution is centered\nWhat do we do with the bootstrap distribution?"
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html#answering-estimation-question",
    "href": "slides/slides-12-bootstrap_ci.html#answering-estimation-question",
    "title": "Bootstrap Confidence Intervals",
    "section": "Answering estimation question",
    "text": "Answering estimation question\nRecall our research question: What proportion of STAT 201A/STAT 201B students get at least 7 hours of sleep a night?\n\nCould respond using our single point estimate: \\(\\hat{p}_{A} = 0.6\\) or \\(\\hat{p}_{B} = 0.7\\)\nBut due to variability, we recognize that the point estimate will rarely (if ever) equal population parameter\nRather than report a single number, why not report a range of values?\n\n\nThis is possible only if we have a distribution to work with!!"
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html#recap",
    "href": "slides/slides-12-bootstrap_ci.html#recap",
    "title": "Bootstrap Confidence Intervals",
    "section": "Recap",
    "text": "Recap\n\nSampling distribution describes how statistic behaves under repeated sampling from population\nLet’s return to the data collected from our activity.\n\nI will repeatedly take SRS of \\(n=10\\) values from the population (call this \\(\\vec{x}\\)) and calculate \\(\\hat{p}\\). Sampling distribution of \\(\\hat{p}\\) is as follows:"
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html#comparing-confidence-intervals",
    "href": "slides/slides-12-bootstrap_ci.html#comparing-confidence-intervals",
    "title": "Bootstrap Confidence Intervals",
    "section": "Comparing confidence intervals",
    "text": "Comparing confidence intervals\nComparing changes in \\(\\gamma \\times 100\\) % CI for sample sizes: \\(n = 5\\), \\(n = 10\\), and \\(n = 17\\):\n\n\n\n\n\n\n\n\n\n\n\nWhat do you notice?\n\n\n\n\n\nSection A\n\n\nn\ninterval\n\n\n\n\nn = 5\n(0.2, 1)\n\n\nn = 10\n(0.3, 0.8)\n\n\nn = 17\n(0.41, 0.76)\n\n\n\n\n\n\nSection B\n\n\nn\ninterval\n\n\n\n\nn = 5\n(0.4, 1)\n\n\nn = 10\n(0.5, 0.9)\n\n\nn = 17\n(0.53, 0.88)"
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html#comparing-confidence-intervals-1",
    "href": "slides/slides-12-bootstrap_ci.html#comparing-confidence-intervals-1",
    "title": "Bootstrap Confidence Intervals",
    "section": "Comparing confidence intervals",
    "text": "Comparing confidence intervals\nWhat do you think happens as we increase \\(\\gamma\\) from \\(0\\) to \\(1\\)?\n\nYour turn to try!"
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html#comparing-confidence-intervals-cont.",
    "href": "slides/slides-12-bootstrap_ci.html#comparing-confidence-intervals-cont.",
    "title": "Bootstrap Confidence Intervals",
    "section": "Comparing confidence intervals (cont.)",
    "text": "Comparing confidence intervals (cont.)\nYou will investigate what happens as we move \\(\\gamma\\) between \\(0\\) to \\(1\\)!"
  },
  {
    "objectID": "slides/slides-12-bootstrap_ci.html#live-code-your-turn",
    "href": "slides/slides-12-bootstrap_ci.html#live-code-your-turn",
    "title": "Bootstrap Confidence Intervals",
    "section": "Live code + your turn!",
    "text": "Live code + your turn!\n\nLive code:\n\nin-line code\n\nYou will investigate what happens as we move \\(\\gamma\\) between \\(0\\) to \\(1\\)!"
  },
  {
    "objectID": "coding_practice/coding-practice-12-bootstrap.html",
    "href": "coding_practice/coding-practice-12-bootstrap.html",
    "title": "Bootstrap confidence intervals",
    "section": "",
    "text": "We will work with the average hours of sleep each class reported. In the following code chunk, please delete the line of code that does not correspond to your section.\n\nlibrary(tidyverse)\nlibrary(readr)\n\n# SECTION A DATA\nurl_file <- \"https://raw.githubusercontent.com/midd-stat201-fall2024/midd-stat201-fall2024.github.io/refs/heads/main/data/sectionA_week4_sleep.csv\"\n\n# SECTION B DATA \nurl_file <- \"https://raw.githubusercontent.com/midd-stat201-fall2024/midd-stat201-fall2024.github.io/refs/heads/main/data/sectionB_week4_sleep.csv\"\n\n\nsleep <- read_csv(url_file)\n\nWe will intialize a pseudo-random-number-generator using the set.seed() function. You can choose any whole number to input as the parameter to this function. If I have the same seed and code and you, then I can reproduce the same random results.\n\nset.seed(201)\n\n\nCreate a variable that represents and stores the target sample size: 10. Then, take a random sample of size 10 of sleep hours from our population. Store your sample into a variable called x.\n\n\n# create a variable for sample size\n\n# obtain and store our sample\n\n\nWe will take 5000 bootstrap iterations. Store this value as a variable for reproducibility.\n\n\n# store number of bootstrap iterations\n\n\nObtain a bootstrap distribution of sample means using your original sample. Remember to store the bootstrap statistics somewhere! Please use a meaningful variable name. It may be useful to look at and modify the live code from previous class (on website)\n\n\n\n\n\nThe quantile() function obtains percentiles for us. It requires two arguments: a numeric vector and a percentile level (between 0 and 1). For example, quantile(x, 0.5) finds the 50-th percentile of the vector x.\n\nUsing this function, obtain the bounds for a 80%, 90%, and 99% bootstrap confidence interval, respectively. Store these bounds as variables.\n\n# 80%\n\n# 90%\n\n# 99%\n\n\nReport the three confidence intervals here in the format of (lower, upper) using in-line code.\n\n80% CI:\n90%: CI:\n99% CI:\n\nHow do the confidence interval widths change as the level of confidence increases?\n\nAnswer:\n\nInterpret one of your confidence intervals in context. Use in-line code in your answer.\n\nAnswer:"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#housekeeping",
    "href": "slides/slides-13-intro-testing.html#housekeeping",
    "title": "Introduction to Hypothesis Testing",
    "section": "Housekeeping",
    "text": "Housekeeping\n\nOffice hours change this week\nMid-semester feedback survey results"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#testing",
    "href": "slides/slides-13-intro-testing.html#testing",
    "title": "Introduction to Hypothesis Testing",
    "section": "Testing",
    "text": "Testing\nWe are now entering into second branch of inference-related tasks: testing.\n\nWe have some “claim”/question about the target population, and we use sampled data to provide evidence for or against the claim/question.\nEspecially important in experiments where we want to learn the effect of some new drug\nWe will use the hypothesis testing framework to formalize the process of making decisions about research claims.\n\nBecause claim is about target population, we will almost always formulate claims in terms of population parameters\nThen we use sample statistics to provide the evidence for/against"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#running-example-sex-discrimination-study",
    "href": "slides/slides-13-intro-testing.html#running-example-sex-discrimination-study",
    "title": "Introduction to Hypothesis Testing",
    "section": "Running example: sex discrimination study",
    "text": "Running example: sex discrimination study\n\nNote: this study considered sex as binary “male” or “female”, and did not take into consideration gender identities\nParticipants in the study were 48 bank supervisors who identified as male and were attending a management institute at UNC in 1972\n\nEach supervisor was asked to assume the role of personnel director of a bank\nThey were each given a file to judge whether the person in the file should be promoted\nThe files were identical, except half of them indicated that the candidate was male, and the other half were indicated as female\nFiles were randomly assigned to bank managers\n\nExperiment or observational study?\n\n\n\nResearch question: Are individuals who identify their sex as female discriminated against in promotion decisions made by their managers who identify as male?"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#data",
    "href": "slides/slides-13-intro-testing.html#data",
    "title": "Introduction to Hypothesis Testing",
    "section": "Data",
    "text": "Data\nFor each of the 48 supervisors, the following were recorded:\n\nThe sex of the candidate in the file (male/female)\nThe decision (promote/not promote)\n\n\n\n\n\n \n  \n    sex \n    not promote \n    promote \n    total \n  \n \n\n  \n    female \n    10 \n    14 \n    24 \n  \n  \n    male \n    3 \n    21 \n    24 \n  \n  \n    total \n    13 \n    35 \n    48 \n  \n\n\n\n\n\n\nAre we prepared to answer our research question: Are individuals who identify their sex as female discriminated against in promotion decisions made by their managers who identify as male?\n\n\nWhat evidence do we have?"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#evidence",
    "href": "slides/slides-13-intro-testing.html#evidence",
    "title": "Introduction to Hypothesis Testing",
    "section": "Evidence",
    "text": "Evidence\nConditional probability of getting promoted by sex:\n\n\n# A tibble: 2 × 3\n# Groups:   sex [2]\n  sex    decision cond_prob\n  <chr>  <chr>        <dbl>\n1 female promote      0.583\n2 male   promote      0.875\n\n\n\nThe key question: does the difference we found provide convincing evidence that individuals who identify as female are unfairly discriminated against?"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#step-1-define-hypotheses",
    "href": "slides/slides-13-intro-testing.html#step-1-define-hypotheses",
    "title": "Introduction to Hypothesis Testing",
    "section": "Step 1: Define hypotheses",
    "text": "Step 1: Define hypotheses\nA hypothesis test is a statistical technique used to evaluate competing claims using data\n\nWe define hypotheses to translate our research question/claim into statistical notation\nWe always define two hypotheses in context: a null hypothesis and an alternative hypothesis\nNull hypothesis \\(H_{0}\\): the hypothesis that represents “business as usual”/status quo/nothing unusual or noteworthy\nAlternative hypothesis \\(H_{A}\\): claim the researchers want to demonstrate\n\n\nIt will not always be obvious what \\(H_{0}\\) should be, but you will develop intuition for this over time!"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#defining-hypotheses-in-context",
    "href": "slides/slides-13-intro-testing.html#defining-hypotheses-in-context",
    "title": "Introduction to Hypothesis Testing",
    "section": "Defining hypotheses in context",
    "text": "Defining hypotheses in context\nResearch question: do the majority of STAT 201A/STAT 201B students get at least 7 hours of sleep?\n\nDefine \\(p\\) as the true proportion of STAT 201A/STAT 201B who get at least 7 hours of sleep on average\n\n\n\n\\(H_{0}\\): \\(p \\leq 0.5\\)\n\\(H_{A}\\): \\(p > 0.5\\)"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#step-2-collect-data-and-calculate-sample-statistics",
    "href": "slides/slides-13-intro-testing.html#step-2-collect-data-and-calculate-sample-statistics",
    "title": "Introduction to Hypothesis Testing",
    "section": "Step 2: Collect data and calculate sample statistics",
    "text": "Step 2: Collect data and calculate sample statistics\nSuppose I collect a sample of \\(n= 10\\) students from each class:\n\n\n\n\n\nIn STAT 201A sample: 6 students received at least 7 hours of sleep, and 4 received less than 7 hours\n\nSample statistic: \\(\\hat{p}\\): 0.6\n\n\nIn STAT 201B sample: 7 students received at least 7 hours of sleep, and 3 received less than 7 hours\n\nSample statistic: \\(\\hat{p}\\): 0.7\n\n\n\n\n\nAre we prepared to answer our research question based on this evidence?\n\nDue to variability in data and \\(\\hat{p}\\) we should ask: do the data provide convincing evidence that the majority of students get at least 7 hours of sleep?"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#running-example-2-sex-discrimination-study",
    "href": "slides/slides-13-intro-testing.html#running-example-2-sex-discrimination-study",
    "title": "Introduction to Hypothesis Testing",
    "section": "Running example 2: sex discrimination study",
    "text": "Running example 2: sex discrimination study\n\nNote: this study considered sex as binary “male” or “female”, and did not take into consideration gender identities\nParticipants in the study were 48 bank supervisors who identified as male and were attending a management institute at UNC in 1972\n\nEach supervisor was asked to assume the role of personnel director of a bank\nThey were each given a file to judge whether the person in the file should be promoted\nThe files were identical, except half of them indicated that the candidate was male, and the other half were indicated as female\nFiles were randomly assigned to bank managers\n\nExperiment or observational study?\n\n\n\nResearch question: Are individuals who identify their sex as female discriminated against in promotion decisions made by their managers who identify as male?"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#defining-hypotheses-here",
    "href": "slides/slides-13-intro-testing.html#defining-hypotheses-here",
    "title": "Introduction to Hypothesis Testing",
    "section": "Defining hypotheses here",
    "text": "Defining hypotheses here\n\nResearch question: Are individuals who identify their sex as female discriminated against in promotion decisions made by their managers who identify as male?"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#step-3-determine-if-we-have-convincing-evidence",
    "href": "slides/slides-13-intro-testing.html#step-3-determine-if-we-have-convincing-evidence",
    "title": "Introduction to Hypothesis Testing",
    "section": "Step 3: Determine if we have “convincing evidence”",
    "text": "Step 3: Determine if we have “convincing evidence”\n“Convincing evidence” for us means that it would be highly unlikely to observe the data we did (or data even more extreme) if \\(H_{0}\\) were true!\n\nWe will calculate a p-value: the probability of observing data as or more extreme than we did assuming \\(H_{0}\\) true\n\nNote: p is not the same as true proportion \\(p\\)!\n\nHighly unlikely is vague and needs to defined by the researcher, ideally before seeing data.\n\nIf we want to answer the research question with a binary yes/no, we need some threshold to compare the p-value to. This is called a significance level \\(\\alpha\\)\nCommon choices are \\(\\alpha = 0.05\\), \\(\\alpha = 0.01\\) (more on this later)!\n\nFor our example, we will choose \\(\\alpha = 0.05\\)"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#simulating-under-h_0",
    "href": "slides/slides-13-intro-testing.html#simulating-under-h_0",
    "title": "Introduction to Hypothesis Testing",
    "section": "Simulating under \\(H_{0}\\)",
    "text": "Simulating under \\(H_{0}\\)\n-"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#simulating-under-h_0-step-3-cont.",
    "href": "slides/slides-13-intro-testing.html#simulating-under-h_0-step-3-cont.",
    "title": "Introduction to Hypothesis Testing",
    "section": "Simulating under \\(H_{0}\\) (step 3 cont.)",
    "text": "Simulating under \\(H_{0}\\) (step 3 cont.)\n\nWe have to simulate our data under the assumption that \\(H_{0}\\) is true (recall \\(H_0\\): \\(p \\leq 0.5\\))\nImagine a big bag with pink and purple slips of paper\n\nPink = people who got at least 7 hours of sleep\nPurple = people who got less than 7 hours\n\nWhat proportion of the slips in the bowl should be pink vs purple?\n\nTo simulate under \\(H_{0}\\), no more than 50% of the slip should be pink\n\nWe want convincing evidence even in the most “borderline” case, so we will choose 50% of the slips to be pink."
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#simulating-under-h_0-step-3-cont.-1",
    "href": "slides/slides-13-intro-testing.html#simulating-under-h_0-step-3-cont.-1",
    "title": "Introduction to Hypothesis Testing",
    "section": "Simulating under \\(H_{0}\\) (step 3 cont.)",
    "text": "Simulating under \\(H_{0}\\) (step 3 cont.)\n\nActivity: we now replicate our original sample, this time sampling from this bag of paper slips\n\nWe repeatedly take samples from the null distribution, using original sample size \\(n =\\) 10\nFor each sample, calculate the simulated proportion of pink slips, \\(\\hat{p}_{sim}\\)\n\nLive code?\n\n\n\nset.seed(2) # reproducibility\nB <- 5000 # number of simulations to do to gather enough evidence\nn <- 10 # size of our original sample\np_null_vec <- rep(NA, B) # vector to store the simulated proportions\nfor(b in 1:B){\n  # sample() takes a random sample\n  null_samp <- sample(x = c(\"pink\", \"purple\"), # pink and purple slips\n                      size = n, # sample of size n\n                      replace = T, # tell R that my bowl has infinitely many marbles \n                      prob = c(0.5, 0.5)) # 50% of slips are pink and 50% are purple\n  \n  # calculate and store the proportion of pink slips in this simulation\n  p_null_vec[b] <- sum(null_samp == \"pink\")/n\n}"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#null-distribution-of-statistic",
    "href": "slides/slides-13-intro-testing.html#null-distribution-of-statistic",
    "title": "Introduction to Hypothesis Testing",
    "section": "Null distribution of statistic",
    "text": "Null distribution of statistic\nWe can visualize the distribution of \\(\\hat{p}_{sim}\\) assuming \\(H_{0}\\) true:\n\n\nThis is called the null distribution of the sample statistic, which is the distribution of the statistic assuming \\(H_{0}\\) is true\n\nWhere is the null distribution of \\(\\hat{p}\\) centered? Why does that “make sense”?"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#obtain-p-value-step-3-cont.",
    "href": "slides/slides-13-intro-testing.html#obtain-p-value-step-3-cont.",
    "title": "Introduction to Hypothesis Testing",
    "section": "Obtain p-value (step 3 cont.)",
    "text": "Obtain p-value (step 3 cont.)\nWe can directly obtain (technically estimate) the p-value using our null distribution and our observed \\(\\hat{p}\\)!\n\n\n\n\n\n\n\n\n\nOut of 5000 replications, we saw 1946 instances of \\(\\hat{p}_{sim} \\geq \\hat{p}\\)\np-value is \\(\\frac{ 1946}{5000} \\approx\\) 0.39\n\n\n\n\n\n\n\n\nOut of 5000 replications, we saw 853 instances of \\(\\hat{p}_{sim} \\geq \\hat{p}\\)\np-value is \\(\\frac{ 853}{5000} \\approx\\) 0.17"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#comparing-null-to-observed",
    "href": "slides/slides-13-intro-testing.html#comparing-null-to-observed",
    "title": "Introduction to Hypothesis Testing",
    "section": "Comparing null to observed",
    "text": "Comparing null to observed\nLet’s return to our original goal of Step 3! We need to find the p-value: the probability of observing data as or more extreme as ours, assuming \\(H_{0}\\) were true.\n\nOur observed data were \\(\\hat{p} =\\) 0.6 (STAT 201A) or \\(\\hat{p} =\\) 0.7 (STAT 201B)\n\\(H_{0}\\): \\(p \\leq 0.5\\) and \\(H_{A}\\): \\(p > 0.5\\)\n\nWhat does “as or more extreme” mean in this context?\nHow can we use the null distribution to obtain this probability?"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#how-to-obtain-p-value",
    "href": "slides/slides-13-intro-testing.html#how-to-obtain-p-value",
    "title": "Introduction to Hypothesis Testing",
    "section": "How to obtain p-value?",
    "text": "How to obtain p-value?\n\nHow to obtain this probability? It depends!\n\nOption 1: if we have assumptions about how our data behave, we can obtain this probability using theory/math (next week)\nOption 2: if we don’t want to make assumptions, why not simulate?\n\nWe will call this option “simulating under \\(H_{0}\\)”\n\n\n\nThis is the step that requires the most “work”, and what exactly you do will depend on the the type of data and the research question/claim you have\n\nRemark: hypothesis tests, like confidence intervals, are not unique!"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#step-4-interpret-p-value-and-make-decision",
    "href": "slides/slides-13-intro-testing.html#step-4-interpret-p-value-and-make-decision",
    "title": "Introduction to Hypothesis Testing",
    "section": "Step 4: Interpret p-value and make decision",
    "text": "Step 4: Interpret p-value and make decision\n\nInterpret the p-value in context\n\n\nAssuming \\(H_{0}\\) true, the probability of observing a sample proportion as or more extreme as ours (0.6 or 0.7) is 0.39 or 0.17\n\n\nMake a decision about research claim/question by comparing p-value to significance level \\(\\alpha\\)\n\nIf p-value \\(< \\alpha\\), we reject \\(H_{0}\\) (it was highly unlikely to observe our data given our selected threshold)\nIf p-value \\(\\geq \\alpha\\), we fail to reject \\(H_{0}\\) (we did not have enough evidence against the null)\n\n\nNote: we never “accept \\(H_{A}\\)”!\n\n\n\nSince our p value is greater than \\(\\alpha = 0.05\\), we fail to reject \\(H_{0}\\). The data do not provide sufficient evidence to suggest that the majority of STAT 201A/STAT 201B students get more than 7 hours of sleep."
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#summary",
    "href": "slides/slides-13-intro-testing.html#summary",
    "title": "Introduction to Hypothesis Testing",
    "section": "Summary",
    "text": "Summary\nFour steps for hypothesis test:\n\nDefine null and alternative hypotheses \\(H_{0}\\) and \\(H_{A}\\) in context\nCollect data and set significance level \\(\\alpha\\)\nObtain p-value\n\nWe did this using by simulating under the null distribution\n\nInterpret p-value and make a decision in context"
  },
  {
    "objectID": "slides/slides-14-randomization.html#housekeeping",
    "href": "slides/slides-14-randomization.html#housekeeping",
    "title": "Hypothesis Testing with Randomization",
    "section": "Housekeeping",
    "text": "Housekeeping\n\nOffice hours change this week"
  },
  {
    "objectID": "slides/slides-14-randomization.html#recap",
    "href": "slides/slides-14-randomization.html#recap",
    "title": "Hypothesis Testing with Randomization",
    "section": "Recap",
    "text": "Recap\nFour steps for hypothesis test:\n\nDefine null and alternative hypotheses \\(H_{0}\\) and \\(H_{A}\\) in context\nCollect data and set significance level \\(\\alpha\\)\nObtain p-value by modeling randomness that would occur if \\(H_{0}\\) was true\n\nWe did this using by simulating under the null distribution\n\nReport p-value and make a decision and conclusion in context"
  },
  {
    "objectID": "slides/slides-14-randomization.html#running-example-sex-discrimination-study",
    "href": "slides/slides-14-randomization.html#running-example-sex-discrimination-study",
    "title": "Hypothesis Testing with Randomization",
    "section": "Running example: sex discrimination study",
    "text": "Running example: sex discrimination study\n\nNote: this study considered sex as binary “male” or “female”, and did not take into consideration gender identities\nParticipants in the study were 48 bank supervisors who identified as male and were attending a management institute at UNC in 1972\n\nEach supervisor was asked to assume the role of personnel director of a bank\nThey were each given a file to judge whether the person in the file should be promoted\nThe files were identical, except half of them indicated that the candidate was male, and the other half were indicated as female\nFiles were randomly assigned to bank managers\n\nExperiment or observational study?\n\n\n\nResearch question: Are individuals who identify their sex as female discriminated against in promotion decisions made by their managers who identify as male?"
  },
  {
    "objectID": "slides/slides-14-randomization.html#defining-hypotheses",
    "href": "slides/slides-14-randomization.html#defining-hypotheses",
    "title": "Hypothesis Testing with Randomization",
    "section": "Defining hypotheses",
    "text": "Defining hypotheses\n\nResearch question: Are individuals who identify their sex as female discriminated against in promotion decisions made by their managers who identify as male?\n\n\n\nWhat is/are the variables(s) here? What types of variables are they?\n\nWe need to construct hypotheses where \\(H_{0}\\) is “status quo” and \\(H_{A}\\) is the claim researchers have\n\\(H_{0}\\): the variables sex and decision are independent.\n\ni.e. any observed difference in promotion rates is due to variability\n\n\\(H_{A}\\): the variables sex and decision are not independent, and equally-qualified female personnel are less likely to be promoted than male personnel"
  },
  {
    "objectID": "slides/slides-14-randomization.html#data",
    "href": "slides/slides-14-randomization.html#data",
    "title": "Hypothesis Testing with Randomization",
    "section": "Data",
    "text": "Data\nFor each of the 48 supervisors, the following were recorded:\n\nThe sex of the candidate in the file (male/female)\nThe decision (promote/not promote)\n\n\n\n\n\n\n \n  \n    sex \n    not promote \n    promote \n    total \n  \n \n\n  \n    female \n    10 \n    14 \n    24 \n  \n  \n    male \n    3 \n    21 \n    24 \n  \n  \n    total \n    13 \n    35 \n    48 \n  \n\n\n\n\n\n\n\nAre we prepared to answer our research question: Are individuals who identify their sex as female discriminated against in promotion decisions made by their managers who identify as male?\n\n\nWhat evidence do we have?"
  },
  {
    "objectID": "slides/slides-14-randomization.html#data-cont.",
    "href": "slides/slides-14-randomization.html#data-cont.",
    "title": "Hypothesis Testing with Randomization",
    "section": "Data (cont.)",
    "text": "Data (cont.)\nConditional probability of getting promoted by sex:\n\ndiscrimination |>\n  count(sex, decision) |>\n  group_by(sex) |>\n  mutate(cond_prob = n/sum(n)) |>\n  filter(decision == \"promote\") |>\n  select(-n)\n\n\n\n# A tibble: 2 × 3\n# Groups:   sex [2]\n  sex    decision cond_prob\n  <chr>  <chr>        <dbl>\n1 female promote      0.583\n2 male   promote      0.875\n\n\n\nIs the observed difference -0.2916667 convincing evidence? We need to examine variability in the data, assuming \\(H_{0}\\) true.\nLet’s set \\(\\alpha = 0.05\\)"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#comprehension-questions",
    "href": "slides/slides-13-intro-testing.html#comprehension-questions",
    "title": "Introduction to Hypothesis Testing",
    "section": "Comprehension questions",
    "text": "Comprehension questions\n\nWhat are the similarities/differences between the bootstrap distribution of a sample statistic and the simulated null distribution?\nDo you understand what a p-value represents, and how we obtain it from the null distribution?\nWhat role does \\(\\alpha\\) play? Why is it important to set \\(\\alpha\\) early on?"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#step-2-collect-and-summarize-data",
    "href": "slides/slides-13-intro-testing.html#step-2-collect-and-summarize-data",
    "title": "Introduction to Hypothesis Testing",
    "section": "Step 2: Collect and summarize data",
    "text": "Step 2: Collect and summarize data\nSuppose I collect a sample of \\(n= 10\\) students from each class:\n\n\n\n\n\nIn STAT 201A sample: 6 students received at least 7 hours of sleep, and 4 received less than 7 hours\n\nSample statistic: \\(\\hat{p}\\): 0.6\n\n\nIn STAT 201B sample: 7 students received at least 7 hours of sleep, and 3 received less than 7 hours\n\nSample statistic: \\(\\hat{p}\\): 0.7\n\n\n\n\n\nAre we prepared to answer our research question based on this evidence?\n\nDue to variability in data and \\(\\hat{p}\\) we should ask: do the data provide convincing evidence that the majority of students get at least 7 hours of sleep?"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#summary-of-testing-framework",
    "href": "slides/slides-13-intro-testing.html#summary-of-testing-framework",
    "title": "Introduction to Hypothesis Testing",
    "section": "Summary of testing framework",
    "text": "Summary of testing framework\nFour steps for hypothesis test:\n\nDefine null and alternative hypotheses \\(H_{0}\\) and \\(H_{A}\\) in context\nCollect data and set significance level \\(\\alpha\\)\nObtain/estimate p-value by modeling randomness that would occur if the \\(H_{0}\\) were true\n\nWe did this using by simulating under the null distribution\n\nInterpret p-value and make a decision in context"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#errors-in-decision",
    "href": "slides/slides-13-intro-testing.html#errors-in-decision",
    "title": "Introduction to Hypothesis Testing",
    "section": "Errors in decision",
    "text": "Errors in decision\n\nIn Step 4, we make a decision but it could be wrong! (Unfortunately, we will never know)\nWe always fall into one of the following four scenarios:\n\n\n\n\n\n\n\n\n\n\nIdentify which cells are good scenarios, and which are bad"
  },
  {
    "objectID": "slides/slides-13-intro-testing.html#errors-in-decision-1",
    "href": "slides/slides-13-intro-testing.html#errors-in-decision-1",
    "title": "Introduction to Hypothesis Testing",
    "section": "Errors in decision",
    "text": "Errors in decision\n\n\n\nWhat kind of error could we have made in our example?\n\nIt is important to weight the consequences of making each type of error!\n\nWe have some control in this - how? Through \\(\\alpha\\)!"
  },
  {
    "objectID": "slides/slides-14-randomization.html#running-example-cpr",
    "href": "slides/slides-14-randomization.html#running-example-cpr",
    "title": "Hypothesis Testing with Randomization",
    "section": "Running example: CPR",
    "text": "Running example: CPR\nAn experiment was conducted, consisting of two treatments on 90 patients who underwent CPR for a heart attack and subsequently went to the hospital. Each patient was randomly assigned to either:\n\ntreatment group: received a blood thinner\ncontrol group: did not receive a blood thinner\n\n\nFor each patient, the outcome recorded was whether they survived for at least 24 hours.\n\n\n\n\n \n  \n    group \n    died \n    survived \n    total \n  \n \n\n  \n    control \n    39 \n    11 \n    50 \n  \n  \n    treatment \n    26 \n    14 \n    40 \n  \n  \n    total \n    65 \n    25 \n    90 \n  \n\n\n\n\n\n\n\n\nWhat is/are the variables(s) here? What types of variables are they?"
  },
  {
    "objectID": "slides/slides-14-randomization.html#collect-data",
    "href": "slides/slides-14-randomization.html#collect-data",
    "title": "Hypothesis Testing with Randomization",
    "section": "Collect data",
    "text": "Collect data\nUsing the data, obtain the observed difference in sample proportions.\n\n\n\n\n \n  \n    group \n    died \n    survived \n    total \n  \n \n\n  \n    control \n    39 \n    11 \n    50 \n  \n  \n    treatment \n    26 \n    14 \n    40 \n  \n  \n    total \n    65 \n    25 \n    90 \n  \n\n\n\n\n\n\n\n\n\np_hat_c <- cpr |>\n  filter(group == \"control\") |>\n  summarise(p = mean(outcome == \"survived\")) |>\n  pull()\np_hat_t <- cpr |>\n  filter(group == \"treatment\") |>\n  summarise(p = mean(outcome == \"survived\")) |>\n  pull()\nobs_diff <- p_hat_t - p_hat_c\n\n\n\n\n\\(\\hat{p}_{C} = \\frac{11}{50} = 0.22\\)\n\\(\\hat{p}_{T} = \\frac{14}{40} = 0.35\\)\nObserved difference: \\(\\hat{p}_{T} - \\hat{p}_{C} = 0.13\\)\n\n\n\n\nIs this “convincing evidence” that blood thinner usage after CPR is effective?\nSet \\(\\alpha = 0.05\\)"
  },
  {
    "objectID": "slides/slides-14-randomization.html#defining-hypotheses-1",
    "href": "slides/slides-14-randomization.html#defining-hypotheses-1",
    "title": "Hypothesis Testing with Randomization",
    "section": "Defining hypotheses",
    "text": "Defining hypotheses\nThe researchers are interested in learning if the blood thinner treatment was effective.\n\nIn words, try to determine \\(H_{0}\\) and \\(H_{A}\\).\n\n\nLet \\(p_{T}\\) and \\(p_{C}\\) denote the proportion of patients who survive when receiving the thinner (Treatment) and when not receiving the treatment (Control), respectively\n\n\n\n\nOption 1\n\n\\(H_{0}\\): \\(p_{T} \\leq p_{C}\\)\n\\(H_{A}\\): \\(p_{T} > p_{C}\\)\n\n\n\n\nOption 2 (preferred)\n\n\\(H_{0}\\): \\(p_{T} - p_{C} \\leq 0\\)\n\\(H_{A}\\): \\(p_{T} - p_{C}> 0\\)"
  },
  {
    "objectID": "slides/slides-14-randomization.html#simulate-under-null",
    "href": "slides/slides-14-randomization.html#simulate-under-null",
    "title": "Hypothesis Testing with Randomization",
    "section": "Simulate under null",
    "text": "Simulate under null\n\nSimulating under \\(H_{0}\\) means operating in a hypothetical word where sex and decision are independent.\n\nThis means that knowing the sex of the candidate should have no bearing on the decision to promote or not\n\nWe will perform a simulation called a randomization test where we randomly re-assign decision and sex outcome pairs to see what would have happened if the bankers’ decision had been independent of candidate’s sex (i.e. if \\(H_{0}\\) true)"
  },
  {
    "objectID": "slides/slides-14-randomization.html#visualizing-null-distribution",
    "href": "slides/slides-14-randomization.html#visualizing-null-distribution",
    "title": "Hypothesis Testing with Randomization",
    "section": "Visualizing null distribution",
    "text": "Visualizing null distribution\n\n\nHow would we obtain the p-value in this problem?"
  },
  {
    "objectID": "slides/slides-14-randomization.html#calculate-p-value",
    "href": "slides/slides-14-randomization.html#calculate-p-value",
    "title": "Hypothesis Testing with Randomization",
    "section": "Calculate p-value",
    "text": "Calculate p-value\n\n\n\n\n\n\n\n\n\nWe observed 148 out of 1000 simulations where the difference in proportions under \\(H_{0}\\) was as or more extreme than our observed difference of 0.13\nSo p-value is 0.148"
  },
  {
    "objectID": "slides/slides-14-randomization.html#interpret-and-make-conclusion",
    "href": "slides/slides-14-randomization.html#interpret-and-make-conclusion",
    "title": "Hypothesis Testing with Randomization",
    "section": "Interpret and make conclusion",
    "text": "Interpret and make conclusion\nThe researchers are interested in learning if the blood thinner treatment was effective.\nOur p-value is 0.148 and our selected significance level was \\(\\alpha = 0.05\\).\n\n\nMake a decision and conclusion about the research question in context."
  },
  {
    "objectID": "slides/slides-14-randomization.html#simulate-under-null-code",
    "href": "slides/slides-14-randomization.html#simulate-under-null-code",
    "title": "Hypothesis Testing with Randomization",
    "section": "Simulate under null (code)",
    "text": "Simulate under null (code)\nLive code"
  },
  {
    "objectID": "slides/slides-14-randomization.html#where-were-going-today",
    "href": "slides/slides-14-randomization.html#where-were-going-today",
    "title": "Hypothesis Testing with Randomization",
    "section": "Where we’re going today",
    "text": "Where we’re going today\n\nWe will see another kinds of hypotheses for different types of research questions\nHypothesis testing framework is the same, but will change how we obtain null distribution\nTry to see the big picture"
  },
  {
    "objectID": "slides/slides-14-randomization.html#randomization-test",
    "href": "slides/slides-14-randomization.html#randomization-test",
    "title": "Hypothesis Testing with Randomization",
    "section": "Randomization test",
    "text": "Randomization test\n\n\n\n\n \n  \n    sex \n    not promote \n    promote \n    total \n  \n \n\n  \n    female \n    10 \n    14 \n    24 \n  \n  \n    male \n    3 \n    21 \n    24 \n  \n  \n    total \n    13 \n    35 \n    48 \n  \n\n\n\n\n\n\nWrite down “promote” on 35 cards and “not promote” on 13 cards. Repeat the following:\n\nThoroughly shuffle these 48 cards.\nDeal out a stack of 24 cards to represent males, and the remaining 24 cards to represent females\n\nThis is how we simulate independence under \\(H_{0}\\)\n\nCalculate the proportion of “promote” cards in each stack, \\(\\hat{p}_{m, sim}\\) and \\(\\hat{p}_{f, sim}\\)\nCalculate and record the difference \\(\\hat{p}_{f,sim} - \\hat{p}_{m,sim}\\) (order of difference doesn’t matter so long as you are consistent)"
  },
  {
    "objectID": "slides/slides-14-randomization.html#randomization-test-code",
    "href": "slides/slides-14-randomization.html#randomization-test-code",
    "title": "Hypothesis Testing with Randomization",
    "section": "Randomization test (code)",
    "text": "Randomization test (code)\nLet’s perform one iteration of the simulation.\n\n# reproducibility\nset.seed(1)\nn_female <- sum(discrimination$sex == \"female\")\nn_male <- sum(discrimination$sex == \"male\")\n\n# create cards\ncards <- discrimination$decision\n\n# shuffle cards\nshuffled <- sample(cards)\n\n\n\n\n\n \n  \n    sex \n    not promote \n    promote \n    total \n  \n \n\n  \n    female \n    8 \n    16 \n    24 \n  \n  \n    male \n    5 \n    19 \n    24 \n  \n  \n    total \n    13 \n    35 \n    48 \n  \n\n\n\n\n\n\nUnder this simulation, 0.6666667 of females were promoted, and 0.7916667 of males were promoted. Simulated difference: -0.125"
  },
  {
    "objectID": "slides/slides-14-randomization.html#null-distribution",
    "href": "slides/slides-14-randomization.html#null-distribution",
    "title": "Hypothesis Testing with Randomization",
    "section": "Null distribution",
    "text": "Null distribution\n\n\n\nRepeat the previous step 1000 times:"
  },
  {
    "objectID": "slides/slides-14-randomization.html#obtain-p-value",
    "href": "slides/slides-14-randomization.html#obtain-p-value",
    "title": "Hypothesis Testing with Randomization",
    "section": "Obtain p-value",
    "text": "Obtain p-value\nRecall, the observed difference in our data was \\(\\hat{p}_{f} - \\hat{p}_{m} =\\) -0.2916667.\n\np-value is probability of observing data as or more extreme than our original data, given \\(H_{0}\\) true.\n\nWhere does “as or more extreme” correspond to on our plot?\n\n\n\n\n\n\n\n\n\n\n\n\n\nOut of 1000 simulations under \\(H_{0}\\), 29 resulted in a difference in promotion rates as or more extreme than our observed\nSo the p-value is 0.029"
  },
  {
    "objectID": "slides/slides-14-randomization.html#making-conclusion",
    "href": "slides/slides-14-randomization.html#making-conclusion",
    "title": "Hypothesis Testing with Randomization",
    "section": "Making conclusion",
    "text": "Making conclusion\nOur research question: Are individuals who identify their sex as female discriminated against in promotion decisions made by their managers who identify as male?\n\n\\(H_{0}\\): sex and decision are independent\n\\(H_{A}\\): sex and decision are not independent and equally-qualified female personnel are less likely to get promoted than male personnel by male supervisors\n\\(\\alpha = 0.05\\)\n\n\n\nInterpret our p-value in context.\nMake a decision in response to the research question."
  },
  {
    "objectID": "slides/slides-14-randomization.html#making-conclusion-answer",
    "href": "slides/slides-14-randomization.html#making-conclusion-answer",
    "title": "Hypothesis Testing with Randomization",
    "section": "Making conclusion (answer)",
    "text": "Making conclusion (answer)\n\nAssuming that sex and decision are independent, the probability of observing a difference in promotion rates as or more extreme as we did is 0.029.\nBecause the observed p-value of 0.029 is less than our significant level 0.05, we reject \\(H_{0}\\). The data provide strong evidence of sex discrimination against female candidates by the male supervisors.\n\nWhat kind of error could we have made?"
  },
  {
    "objectID": "slides/slides-14-randomization.html#simulate-under-null-1",
    "href": "slides/slides-14-randomization.html#simulate-under-null-1",
    "title": "Hypothesis Testing with Randomization",
    "section": "Simulate under null",
    "text": "Simulate under null\n\nWe will once again perform a randomization test to try and simulate the difference in proportions under \\(H_{0}\\)\n\nUnder \\(H_{0}\\), treatment group is no better than control group, so let’s simulate assuming that outcome and treatment are independent\n\nWrite down died on 65 cards, and survived on 25 cards. Then repeat several times:\n\nShuffle cards well\nDeal out 50 to be Control group, and remaining 40 to be Treatment group\nCalculate proportions of survival \\(\\hat{p}_{C, sim}\\) and \\(\\hat{p}_{T, sim}\\)\nObtain and record the simulated difference \\(\\hat{p}_{T, sim} - \\hat{p}_{C, sim}\\)"
  },
  {
    "objectID": "slides/slides-14-randomization.html#randomization-test-activity",
    "href": "slides/slides-14-randomization.html#randomization-test-activity",
    "title": "Hypothesis Testing with Randomization",
    "section": "Randomization test (activity)",
    "text": "Randomization test (activity)\nTry it!"
  },
  {
    "objectID": "live_code/intro_testing.html",
    "href": "live_code/intro_testing.html",
    "title": "Null distribution of sample proportion",
    "section": "",
    "text": "library(tidyverse)\nDefine \\(p\\) as the true proportion of students who get at least 7 hours of sleep on average. Our hypotheses were:\n\\(H_{0}\\): \\(p \\leq 0.5\\)\n\\(H_{A}\\): \\(p > 0.5\\)"
  },
  {
    "objectID": "live_code/intro_testing.html#simulating-null-distribution",
    "href": "live_code/intro_testing.html#simulating-null-distribution",
    "title": "Null distribution of sample proportion",
    "section": "Simulating null distribution",
    "text": "Simulating null distribution\nThe example below demonstrates how to obtain a null distribution for \\(\\hat{p}\\) under \\(H_{0}\\). We said:\n\npink slips represent students who get at least 7 hours, and\npurple slips represent students who do not get at least 7 hours\n\n\nset.seed(2)\nB <- 5000\nn <- 10\np_null_vec <- rep(NA, B)\nfor(b in 1:B){\n  null_samp <- sample(x = c(\"pink\", \"purple\"),\n                      size = n, \n                      replace = T, \n                      prob = c(0.5, 0.5)) \n  \n  p_null_vec[b] <- sum(null_samp == \"pink\")/n\n}\n\nThe code above repeatedly for 5000 iterations draws a new sample of size 10 from the world assuming \\(H_{0}\\) is true (in this case, \\(p = 0.5\\)). At every iteration, we record the proportion of pink slips in the sample of size 10 to represent the proportion of people who got at least 7 hours of sleep."
  },
  {
    "objectID": "live_code/intro_testing.html#visualizing-null-distribution",
    "href": "live_code/intro_testing.html#visualizing-null-distribution",
    "title": "Null distribution of sample proportion",
    "section": "Visualizing null distribution",
    "text": "Visualizing null distribution\nWe can visualize the null distribution by:\n\nCreating a data frame of our vector of simulated null statistics p_null_vec\nPiping into ggplot()\n\n\n\ndata.frame(<variable name> = <vector>)creates a data frame from vectors, and we can set the column/variable names.\nIn the code here, data.frame(p_sim = p_null_vec) creates a data frame with a variable called p_sim. The values that comprise that variable come from p_null_vec.\n\nnull_df <- data.frame(p_sim = p_null_vec)\nggplot(null_df, aes(x = p_sim))+\n  geom_histogram(binwidth = 0.1,col = \"white\")+\n  labs(x = \"Null dist. of proportion of students getting at least 7 hours\") +\n  theme_minimal()\n\n\n\n\nWe can add a vertical line to our plot to show where the observed \\(\\hat{p}\\) falls in the null distribution."
  },
  {
    "objectID": "practice_probs/practice-13-intro_testing.html",
    "href": "practice_probs/practice-13-intro_testing.html",
    "title": "Hypothesis testing",
    "section": "",
    "text": "For each of the research statements below, determine whether it represents a null hypothesis claim or an alternative hypothesis claim.\n\nThe number of hours that grade-school children spend doing homework predicts their future success on standardized tests.\nKing cheetahs on average run the same speed as standard spotted cheetahs.\nFor a particular student, the probability of correctly answer a 5-option multiple choice test is larger than 0.2 (i.e. better than guessing)\nThe probability of getting in a car accident is the same if using a cell phone then if not using a cell phone.\n\nWrite out the null and alternative hypotheses in words and also in statistical notation for each of the following situations. When writing in statistical notation, be sure to define quantities in context.\n\nNew York is known as “the city that never sleeps”. A random sample of 25 New Yorkers were asked how much they sleep they get per night. Does these data providing convincing evidence that New Yorkers on average sleep less than 8 hours per night?\nA study suggests that 25% of 25 year-olds have gotten married. You believe that this is incorrect and decide to collect your own data to conduct a hypothesis test.\n\nA Survey USA poll conducted in Seattle, WA in May 2021 reports that of the 650 respondents (adults living in this area), 159 support proposals to defund police departments.\n\nA journals writing a news story on the poll results wants to use the headline: “More than 1 in 5 adults living in Seattle support proposals to defund police departments”. You caution the journalist that they should first conduct a hypothesis test to see if the poll data provide convincing evidence for this claim. Write the hypotheses for this test using proper notation, defining any necessary quantities.\nDescribe in words a simulation scheme that would be appropriate for this situation. Also describe how the p-value can be calculated using the simulation results.\nThe histogram below shows the distribution of 1000 simulated proportions under \\(H_{0}\\). Estimate the p-value using the plot and use it to evaluate your hypotheses (i.e. make a conclusion). Assume a significance level of 0.05.\n\n\n\\((^*)\\) In a large university where 60% of the full-time students are employed at least 5 hours per week, the members of the Statistics Department faculty wonder if the same proportion of their students work at least 5 hours per week. They randomly sample 25 of their majors and find that 12 of the students work 5 or more hours per week.\nTwo sampling distributions were created to describe the variability in the proportion of statistics majors who work at least 5 hours per week: a null distribution and a bootstrap distribution. In both cases, \\(B=1000\\) simulations were generated.\n\n\nWhich distribution(s) was/were obtained by sampling with replacement, and which distribution(s) was/were obtained by sampling without replacement?\nEstimate the standard error of the simulated proportions based on each distribution. Are the two standard errors you estimated roughly equal?\nUsing the appropriate histogram, test the claim that 70% of statistics majors, like their peers, work at least 5 hours per week. State the hypotheses, find the p-value, and conclude in the context of the problem. Use a significance level of 0.10.\nUsing the appropriate histogram, find a 90% bootstrap confidence interval for the true proportions of statistics majors who work at least 5 hours per week. Interpret the confidence interval in the context of the problem.\nBriefly comment on how your conclusions in (c) and (d) compare.\n\nA study conducted in 2020 found that the U.S. adjusted divorce rate was 14 per 1000 married women. Joe is suspicious and disagrees with the stated divorce rate. Joe somehow collected data from 323 married or previously-married women, and asked them if they had a divorce in 2020. 55 of the women responded that they indeed had a divorce in 2020.\n\nWrite out the hypotheses corresponding to this scenario.\nDescribe in words a simulation scheme that would be appropriate for this situation. Also describe how the p-value can be calculated using the simulation results.\nThe histogram below shows the distribution of 1000 simulated proportions under \\(H_{0}\\). Estimate the p-value using the plot and use it to evaluate Joe’s hypotheses (i.e. make a conclusion). Assume a significance level of 0.05.\n\nJoe is some free time and also created a 90% bootstrap confidence interval for the divorce rate.\n\n\n\nHe obtained the following interval: (0.136, 0.204). Interpret this interval in context.\nBased on this interval, would it be appropriate for Joe to conclude that the study’s reported rate was wrong? Explain your reasoning.\nHow do your conclusions from (c) and (e) compare?"
  },
  {
    "objectID": "slides/slides-14-randomization.html#making-decision-and-conclusion",
    "href": "slides/slides-14-randomization.html#making-decision-and-conclusion",
    "title": "Hypothesis Testing with Randomization",
    "section": "Making decision and conclusion",
    "text": "Making decision and conclusion\nOur research question: Are individuals who identify their sex as female discriminated against in promotion decisions made by their managers who identify as male?\n\n\\(H_{0}\\): sex and decision are independent\n\\(H_{A}\\): sex and decision are not independent and equally-qualified female personnel are less likely to get promoted than male personnel by male supervisors\n\\(\\alpha = 0.05\\)\n\n\n\nInterpret our p-value in context.\nMake a decision and conclusion in response to the research question."
  },
  {
    "objectID": "slides/slides-14-randomization.html#making-decision-and-conclusion-answer",
    "href": "slides/slides-14-randomization.html#making-decision-and-conclusion-answer",
    "title": "Hypothesis Testing with Randomization",
    "section": "Making decision and conclusion (answer)",
    "text": "Making decision and conclusion (answer)\n\nAssuming that sex and decision are independent, the probability of observing a difference in promotion rates as or more extreme as we did is 0.029.\nBecause the observed p-value of 0.029 is less than our significant level 0.05, we reject \\(H_{0}\\). The data provide strong evidence of sex discrimination against female candidates by the male supervisors.\n\nWhat kind of error could we have made?"
  },
  {
    "objectID": "slides/slides-14-randomization.html#comprehension-questions",
    "href": "slides/slides-14-randomization.html#comprehension-questions",
    "title": "Hypothesis Testing with Randomization",
    "section": "Comprehension questions",
    "text": "Comprehension questions\n\nWhat were the similarities and differences between:\n\nhypothesis test for independence\nhypothesis test for two proportions\n\nHow do the randomization tests today differ from the test for one proportion that we learned last class?"
  },
  {
    "objectID": "live_code/randomization_two_props.html",
    "href": "live_code/randomization_two_props.html",
    "title": "Randomization for difference in proportions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(openintro)\nWe are using the cpr data from the openintro library. Define \\(p_{T}\\) as the proportion of treatment group who survived, \\(p_{C}\\) as the proportion of control group who survived. Our hypotheses were:\n\\(H_{0}\\): \\(p_{T} - p_{C} \\leq 0\\) versus \\(H_{A}\\): \\(p_{T} - p_{C} > 0\\)"
  },
  {
    "objectID": "live_code/randomization_two_props.html#simulating-null-distribution",
    "href": "live_code/randomization_two_props.html#simulating-null-distribution",
    "title": "Randomization for difference in proportions",
    "section": "Simulating null distribution",
    "text": "Simulating null distribution\nWe will store some values as objects in R for reproducibility.\n\n# get the observed outcomes as a vector\noutcomes <- cpr$outcome\n\n# store number of treatment and control observations from original sample\nn_t <- sum(cpr$group == \"treatment\")\nn_t\n\n[1] 40\n\nn_c <- sum(cpr$group == \"control\")\nn_c\n\n[1] 50\n\n# get observed difference in proportions\n\n# option 1\nobs_diff <- cpr |>\n  group_by(group) |>\n  summarise(p = mean(outcome == \"survived\")) |>\n  pull(p) |>\n  diff() # defaults to taking second value minus first\n\n# option 2\ncond_probs <- cpr |>\n  group_by(group) |>\n  summarise(p = mean(outcome == \"survived\")) |>\n  pull(p)\nobs_dff <- cond_probs[2] - cond_probs[1]\nobs_diff\n\n[1] 0.13\n\n\nNow we will perform the randomization test!\n\n\nIf you only pass in a vector as the argument to sample() without specifying the size of the sample, you will get a shuffled version of the vector back as output. That is, the function will resample without replacement from the vector, with the size being the same size as the vector.\n\nset.seed(310)\nB <- 1000\ndiff_sims <- rep(NA , B)\nfor(b in 1:B){\n  # shuffle the outcomes. See comment for more details.\n  shuffled <- sample(outcomes)\n  \n  # deal out first n_t cards to treatment group\n  treat_sim <- shuffled[1:n_t]\n  \n  # deal out remaining n_c cards to control group\n  control_sim <- shuffled[-c(1:n_t)]\n  \n  # calculate proportion of survival in each group\n  p_t_sim <- sum(treat_sim == \"survived\")/n_t\n  p_c_sim <- sum(control_sim == \"survived\")/n_c\n  \n  # calculate difference and store. \n  # I will do treatment - control because it mirrors how obs_diff is calculated\n  diff_sims[b] <- p_t_sim - p_c_sim\n}"
  },
  {
    "objectID": "slides/slides-14-randomization.html",
    "href": "slides/slides-14-randomization.html",
    "title": "Hypothesis Testing with Randomization",
    "section": "",
    "text": "Office hours change this week\nMid-semester feedback survey results"
  },
  {
    "objectID": "slides/slides-15-single-mean.html#housekeeping",
    "href": "slides/slides-15-single-mean.html#housekeeping",
    "title": "Hypothesis testing for a mean",
    "section": "Housekeeping",
    "text": "Housekeeping\n\nOffice hours tomorrow: 10:30am-12:00pm"
  },
  {
    "objectID": "slides/slides-15-single-mean.html#recap",
    "href": "slides/slides-15-single-mean.html#recap",
    "title": "Hypothesis testing for a mean",
    "section": "Recap",
    "text": "Recap\n\nWe have seen how to perform hypothesis tests for questions involving the following:\n\nA single proportion (STAT 201 sleep)\nIndependence of two categorical variables (banker sex discrimination)\n\nThink of as one population\n\nDifference in two proportions (blood thinner)\n\nThink of as two populations\n\n\nWe are now going to see another hypothesis test, this time for numerical data"
  },
  {
    "objectID": "slides/slides-15-single-mean.html#running-example-form-hypotheses",
    "href": "slides/slides-15-single-mean.html#running-example-form-hypotheses",
    "title": "Hypothesis testing for a mean",
    "section": "Running example + form hypotheses",
    "text": "Running example + form hypotheses\nWe will use the duke_forest dataset provided in openintro. It provides data on some houses that were sold in the Duke Forest neighborhood of Durham, NC in November 2020.\n\nBefore we look at the data, we should form our hypotheses. Suppose I am interested in learning if the average price of houses in Duke Forest is $500,000 or not.\n\nWhat might our hypotheses be?\n\n\n\\(H_{0}\\): \\(\\mu = 50\\) versus \\(H_{A}\\): \\(\\mu \\neq 50\\), where \\(\\mu\\) is the average house of prices in Duke Forest in $10,000\nTerminology: I will refer to \\(\\mu_{0} = 50\\) as my “null hypothesized value”. (i.e. the specific value of \\(\\mu\\) in \\(H_{0}\\))"
  },
  {
    "objectID": "slides/slides-15-single-mean.html#collect-data",
    "href": "slides/slides-15-single-mean.html#collect-data",
    "title": "Hypothesis testing for a mean",
    "section": "Collect data",
    "text": "Collect data\n\n\n\n\nThe observed/sample mean housing price is $55.99k from a sample of 98 houses.\n\nNow we must determine if we have “convincing evidence”! Choose \\(\\alpha = 0.05\\)"
  },
  {
    "objectID": "slides/slides-15-single-mean.html#simulating-null-distribution",
    "href": "slides/slides-15-single-mean.html#simulating-null-distribution",
    "title": "Hypothesis testing for a mean",
    "section": "Simulating null distribution",
    "text": "Simulating null distribution\nTo simulate from the null distribution, we need to operate in a world where \\(H_{0}\\) is true\n\nSo, I need to repeatedly simulate data sets of size 98 where the true mean is \\(50\\), without change anything else about the data sets.\nIf I don’t want to make any assumptions about how the data behave, how might I do that?"
  },
  {
    "objectID": "slides/slides-15-single-mean.html#bootstrap-to-the-rescue",
    "href": "slides/slides-15-single-mean.html#bootstrap-to-the-rescue",
    "title": "Hypothesis testing for a mean",
    "section": "Bootstrap to the rescue",
    "text": "Bootstrap to the rescue\n\nRecall the bootstrap: we repeatedly took resamples with replacement from our original data, of same size as original data\n\nAssuming the original data was representative, each one of these bootstrapped data sets gives us a plausible “new” sample of data, from which we can calculate statistics of interest\n\n\n\n\n\n\n\n\n\n\n\n\n\nReminder ourselves: Where is the bootstrap distribution centered?"
  },
  {
    "objectID": "slides/slides-15-single-mean.html#bootstrap-to-null-distribution",
    "href": "slides/slides-15-single-mean.html#bootstrap-to-null-distribution",
    "title": "Hypothesis testing for a mean",
    "section": "Bootstrap to null distribution",
    "text": "Bootstrap to null distribution\n\n\n\n\n\n\n\n\n\nThis is not the null distribution! The null distribution should be centered at \\(\\mu_{0} = 50\\).\nHowever, the null distribution should have the same variability in \\(\\bar{x}\\) as the bootstrap distribution.\n\n\n\n\nSo to get the null distribution, why not just shift the bootstrap distribution to be centered where we want it to be?"
  },
  {
    "objectID": "slides/slides-15-single-mean.html#shifting-to-the-bootstrap-distribution",
    "href": "slides/slides-15-single-mean.html#shifting-to-the-bootstrap-distribution",
    "title": "Hypothesis testing for a mean",
    "section": "Shifting to the bootstrap distribution",
    "text": "Shifting to the bootstrap distribution\n\nIn this example, bootstrap distribution is centered at \\(\\bar{x} = 55.99\\)\nIn order to center this distribution at \\(\\mu_{0} = 50\\), just subtract \\(55.99 - 50 = 5.99\\) from every single bootstrapped mean\n\nThis will give us a simulated distribution for \\(\\bar{x}\\) centered at \\(\\mu_{0} = 50\\), which is exactly the null distribution!\n\nWe call this “shifting the bootstrap distribution”, because we simply shift where the bootstrap distribution is centered\n\n\n\n\n\nmu0 <- 50\n# how much to shift by, where xbar is sample mean housing prices\nshift <- xbar - mu0\n# shift my vector of bootstrapped sample means\nnull_dist <- boot_means - shift"
  },
  {
    "objectID": "slides/slides-15-single-mean.html#null-distribution",
    "href": "slides/slides-15-single-mean.html#null-distribution",
    "title": "Hypothesis testing for a mean",
    "section": "Null distribution",
    "text": "Null distribution\n\n\n\nNotice where the distributions are centered"
  },
  {
    "objectID": "slides/slides-15-single-mean.html#obtain-the-p-value",
    "href": "slides/slides-15-single-mean.html#obtain-the-p-value",
    "title": "Hypothesis testing for a mean",
    "section": "Obtain the p-value",
    "text": "Obtain the p-value\n\\(H_{0}\\): \\(\\mu = 50\\) versus \\(H_{A}\\): \\(\\mu \\neq 50\\)\nOur observed sample mean housing price is 55.99.\n\n\nWhat does it mean to be “as or more extreme” now?"
  },
  {
    "objectID": "slides/slides-15-single-mean.html#two-sided-alternative-hypothesis",
    "href": "slides/slides-15-single-mean.html#two-sided-alternative-hypothesis",
    "title": "Hypothesis testing for a mean",
    "section": "Two-sided alternative hypothesis",
    "text": "Two-sided alternative hypothesis\n\nThis is the first time we’ve seen a two-sided hypothesis\nSince the alternative is “double sided”, we can be extreme in both the positive and negative direction!"
  },
  {
    "objectID": "slides/slides-15-single-mean.html#obtain-the-p-value-cont.",
    "href": "slides/slides-15-single-mean.html#obtain-the-p-value-cont.",
    "title": "Hypothesis testing for a mean",
    "section": "Obtain the p-value (cont.)",
    "text": "Obtain the p-value (cont.)\nLet \\(shift\\) represent the amount we shifted the distribution by:\n\\[shift = 55.99 - 50 = 5.99\\]\nSimulated means as or more extreme than \\(\\mu_{0} + shift\\) or \\(\\mu_{0} - shift\\) will contribute:\n\n\n\n\n\n\n\n\n\nsum( (null_dist >= mu0 + shift) | (null_dist <= mu0 - shift))/B\n\n[1] 0.0098"
  },
  {
    "objectID": "slides/slides-15-single-mean.html#make-decision-and-conclusion",
    "href": "slides/slides-15-single-mean.html#make-decision-and-conclusion",
    "title": "Hypothesis testing for a mean",
    "section": "Make decision and conclusion",
    "text": "Make decision and conclusion\n\nMake a decision and conclusion in the context of the research question.\n\n\nSince our p-value of 0.0098 is less than the significance level of 0.05, we reject \\(H_{0}\\). We have convincing evidence to suggest that the true average housing price of homes in Duke Forest in 2020 was not $500k."
  },
  {
    "objectID": "slides/slides-15-single-mean.html",
    "href": "slides/slides-15-single-mean.html",
    "title": "Hypothesis testing for a mean",
    "section": "",
    "text": "Office hours tomorrow: 10:30am-12:00pm"
  },
  {
    "objectID": "slides/slides-15-single-mean.html#comprehension-questions",
    "href": "slides/slides-15-single-mean.html#comprehension-questions",
    "title": "Hypothesis testing for a mean",
    "section": "Comprehension questions",
    "text": "Comprehension questions\n\nWhy did we shift the bootstrap distribution?\nHow do we estimate the p-value in the case of a two-sided alternative hypothesis?"
  },
  {
    "objectID": "live_code/randomization_two_props.html#obtain-p-value",
    "href": "live_code/randomization_two_props.html#obtain-p-value",
    "title": "Randomization for difference in proportions",
    "section": "Obtain p-value",
    "text": "Obtain p-value\nWe need to find how many simulated null differences in proportions were greater than or equal to our observed difference of obs_diff = 0.13.\n\np_val <- sum(diff_sims >= obs_diff) / B\np_val\n\n[1] 0.148"
  },
  {
    "objectID": "homework/hw5_r.html",
    "href": "homework/hw5_r.html",
    "title": "STAT 201: Problem Set 5 (R)",
    "section": "",
    "text": "In every code chunk where you perform random sampling, set a seed at the top of the chunk. I don’t care what seed you choose so long as you set a seed!\nMake your code as reproducible as possible. You should avoid “hard-coding” values. Instead, store values as variables for future use.\nNote: you will practice typing in mathematical notation in the narrative using Latex. To write the greek letter mu, type in \\(\\mu\\) in the narrative. You can add subscropts like this: \\(\\mu_{H}\\). To write a “does not equal” sign, type \\(\\neq\\) into the narrative.\nThe dataset is adapted from Little et al. (2007), and contains voice measurements from individuals both with and without Parkinson’s Disease (PD), a progressive neurological disorder that affects the motor system. The aim of Little et al.’s study was to examine whether they could diagnose PD by examining the spectral (sound-wave) properties of patients’ voices.\n147 measurements were taken from patients with PD, and 48 measurements were taken from healthy controls. For the purposes of this lab, you may assume that measurements are representative of the underlying populations (PD vs. healthy).\nThe variables in the dataset are as follows:\nThe data are stored in a variable called parksinsons. Run the following code chunk and take a look at the data before getting started."
  },
  {
    "objectID": "homework/hw5_r.html#part-1",
    "href": "homework/hw5_r.html#part-1",
    "title": "STAT 201: Problem Set 5 (R)",
    "section": "Part 1",
    "text": "Part 1\nResearchers suspect that patients with PD are less able to control their vocal muscles, and thus may have a greater voice jitter compared to healthy volunteers. Thus, they are interested in whether the mean jitter in voice recordings among patients with PD is greater than the mean jitter in voice recordings among healthy patients. The researchers select the 0.01 significance level.\n\nWrite out the null hypothesis and alternative hypotheses for the question in symbols, defining quantities when necessary.\n\nAnswer:\n\\(H_{0}\\):\n\\(H_{1}\\):\n\nDescribe in words how you would obtain 5000 simulations from the null distribution using a randomization test. (Hint: this should be very similar to the sex discrimination or the CPR example.) Then, describe how you would estimate the p-value using this null distribution. Be as specific as possible!\n\nAnswer:\n\nIt will be helpful to define some variables here. To make your life easier, create objects/variables to represent the following quantities:\n\nThe number of healthy patients\nThe number of PD patients\nThe vector of voice jitters\n\n\n\n\n\n\nSimulate the null distribution according to your description above. Store the results into a vector called null_dist_pt1.\n\n\n# set a seed using the set.seed() function\n\n\nWhat is your p-value, decision, and conclusion in context of the research question? Use in-line code for reproducibility.\n\n\n\n\nAnswer:\n\nWhat is the probability you’ve made a Type 1 error? If you cannot tell for sure, explain why. Similarly, what is the probability you’ve made a Type 2 error? If you cannot tell for sure, explain why.\n\nAnswer:"
  },
  {
    "objectID": "homework/hw5_r.html#part-2",
    "href": "homework/hw5_r.html#part-2",
    "title": "STAT 201: Problem Set 5 (R)",
    "section": "Part 2",
    "text": "Part 2\nWe will now answer the following question: is there enough evidence to suggest that the mean HNR in the voice recordings of the PD patients is significantly different from 21.5 at the \\(\\alpha=\\) 0.10 significance level?\n\nWrite out the null and alternative hypotheses for this question using symbols, defining any quantities as necessary.\n\nAnswer:\n\nDescribe in words how you would obtain 5000 simulations from the null distribution. Then, describe how you would estimate the p-value using this null distribution. Be as specific as possible!\n\nAnswer:\n\nTo make your life easier, create the following objects/variables.\n\n\nhnr_pd: a vector of HNR for the PD patients only (you may want to use some combination of filter() and pull() to obtain this)\nn_pd: the number of PD patients\nxbar_pd: the observed/sample mean HNR of PD patients\nmu_h0: the null hypothesized value for the population parameter\n\n\n\n\n\nSimulate the null distribution according to your description above. Store the results into a vector called null_dist_pt2.\n\n\n\n\n\nVisualize your null distribution. Make sure your visualization has informative axis labels and/or title.\n\n\n\n\n\nEstimate the p-value. Then answer the following: what is your p-value, decision, and conclusion in the context of the research question? Use in-line code for reproducibility!\n\n\n\n\nAnswer:"
  },
  {
    "objectID": "live_code/randomization_two_props.html#visualize-null-distribution",
    "href": "live_code/randomization_two_props.html#visualize-null-distribution",
    "title": "Randomization for difference in proportions",
    "section": "Visualize null distribution",
    "text": "Visualize null distribution"
  },
  {
    "objectID": "practice_probs/practice-15-ht-basics.html",
    "href": "practice_probs/practice-15-ht-basics.html",
    "title": "Misc. hypothesis testing practice",
    "section": "",
    "text": "For each of the statements (a) - (d), indicate if they are true or false interpretation of the following confidence interval. If false, provide or a reason or correction to the misinterpretation.\n“You collect a large sample and calculate a 95% confidence interval for the average number of cans of soda consumed annually per adult to be (440, 520), i.e. on average, adults in the US consume just under two cans of soda per day”.\n\n95% of adults in the US consume between 440 and 520 cans of soda per year.\nThere is a 95% chance that the true population average per adult yearly soda consumption is between 440 and 520 cans.\nThe true population average per adult soda consumption is between 440 and 520 cans, with 95% confidence.\nThe average soda consumption of the people who were is sampled is between 440 and 520 cans of soda per year, with 95% confidence.\n\nA food safety inspector is called upon to investigate a restaurant with a few customer reports of poor sanitation practices. The food safety inspector uses a hypothesis testing framework to evaluate whether regulations are not being met. If the inspector determines the restaurant is in gross violation, its license to serve food will be revoked.\n\nWrite the hypotheses in words (no population parameters necessary).\nWhat is a Type I error in this context?\nWhat is a Type II error in this context?\nWhich error is more problematic for the restaurant owner? For the diners? Why?\nDo you think the diners would prefer a higher or lower significance level \\(\\alpha\\) compared to what the restaurant owner prefers? Explain.\n\nConsider the following simple random sample \\(x = (47, 4, 92, 47, 12, 8)\\).\nWhich of the following sets of values could be a possible bootstrap sample from the observe data above? If a set of values could not be a bootstrap sample, determine why not.\n\n\\((47, 47, 47, 47, 47, 47)\\)\n\\((92, 4, 13,8, 47, 4)\\)\n\\((4, 8, 12, 12, 47)\\)\n\\((12, 4, 8, 8, 92, 12)\\)\n\\((8, 47, 12, 12, 8, 4, 92)\\)\n\nFor each of the following statements (a)-(e), indicate if they are a true or false interpretation of the p-value. If false, provide a reason or correction to the misinterpretation.\n“You are wondering if the average amount of cereal in a 10 oz. cereal box is greater than 10 oz. You collect 50 boxes of cereal marketed as 10 oz, conduct simulation-based hypothesis test, and obtain a p-value of 0.23.”\n\nThe probability that the average weight of all cereal boxes is 10 oz. is 0.23.\nThe probability that the average weight of all cereal boxes is something greater than 10 oz. is 0.23.\nBecause the p-value is 0.23, the average weight of all cereal boxes is 10 oz.\nBecause the p-value is small, the population average must be just barely about 10 oz.\nIf \\(H_{0}\\) is true, the probability of observing another sample with an average as or more extreme as the data is 0.23."
  },
  {
    "objectID": "slides/worksheet-16-normal.html",
    "href": "slides/worksheet-16-normal.html",
    "title": "Untitled",
    "section": "",
    "text": "library(tidyverse)\nknitr::opts_chunk$set(echo = F) \n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "practice_probs/practice-16-normal.html",
    "href": "practice_probs/practice-16-normal.html",
    "title": "Normal distribution",
    "section": "",
    "text": "True or false? Briefly explain why.\nAmong applicants to one law school, the average LSAT was about 169, the standard deviation about 9, and the highest score was 178. The distribution of the LSAT scores follows the normal curve.\nIn a law school class, the entering students averaged 160 on the LSAT. The variance was 64. The histogram of LSAT scores followed the normal curve reasonable well.\n\nAbout what percentage of the class scores below 152?\nOne student was 0.5 standard deviations above average on the LSAT. About what percentage of the students had lower scores than he did?\n\n\n\n\nWeights of 10-year-old girls are known to be Normally distributed with mean of 70 pounds and standard deviation of 13 pounds. Find the probability that a 10-year-old girl weighs between 60 and 85 pounds two ways:\n\nOptional, but helpful: draw a sketch of the curve and shade in the region of interest.\nWrite the probability of interest in \\(P()\\) form. Then write the R code necessary to find this probability, and actually execute the code to obtain the probability.\nConfirm your solution in (b) by transforming to z-scores first, then using code again to obtain the probability.\n\nConsider the same scenario as in 3. Without using any code than what is provided below, find the 60th percentile for the weight of 10-year-old girls.\n\nqnorm(0.6, mean = 0, sd = 1)\n\n[1] 0.2533471\n\n\nThe length of human pregnancies from contraception to birth varies according to a distribution that is approximately normal with mean 266 days and standard deviation 16 days. Without using code, obtain the following:\n\nBetween what values do the lengths of the middle 95% of all pregnancies fall?\nHow short are the shortest 2.5% of all pregnancies? How long do the longest 2.5% last?"
  },
  {
    "objectID": "slides/slides-17-clt.html#housekeeping",
    "href": "slides/slides-17-clt.html#housekeeping",
    "title": "Central Limit Theorem",
    "section": "Housekeeping",
    "text": "Housekeeping\n\nOffice hours tomorrow: 10:30am-12:00pm"
  },
  {
    "objectID": "slides/slides-17-clt.html#recap",
    "href": "slides/slides-17-clt.html#recap",
    "title": "Central Limit Theorem",
    "section": "Recap",
    "text": "Recap\n\n\n\n\nNormal distribution: symmetric, bell-shaped curve that is described by mean \\(\\mu\\) and standard deviation \\(\\sigma\\)\n\nCommon model used to describe behavior of continuous variables\n\nUse area under the Normal curve to obtain probabilities\n68-95-99.7 rule\nz-score standardizes observations to allow for easier comparison: \\(z = \\frac{x- \\mu}{\\sigma}\\)"
  },
  {
    "objectID": "slides/slides-17-clt.html#where-were-going",
    "href": "slides/slides-17-clt.html#where-were-going",
    "title": "Central Limit Theorem",
    "section": "Where we’re going",
    "text": "Where we’re going\n\nWe are going to learn one of the BIGGEST theorems in Statistics\nUses the Normal distribution, and will be immensely helpful for inference tasks of confidence intervals and hypothesis testing"
  },
  {
    "objectID": "slides/slides-17-clt.html#central-limit-theorem-clt",
    "href": "slides/slides-17-clt.html#central-limit-theorem-clt",
    "title": "Central Limit Theorem",
    "section": "Central Limit Theorem (CLT)",
    "text": "Central Limit Theorem (CLT)\n\nAssume that you have a sufficiently large sample of \\(n\\) independent values \\(x_{1},\\ldots, x_{n}\\) from a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nThen the distribution of sample means is approximately Normal:\n\\[\n\\bar{X} \\overset{\\cdot}{\\sim} N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\n\\]\n\nThat is, the sampling distribution of the sample mean is approximately normal with mean \\(\\mu\\) and standard error \\(\\sigma/\\sqrt{n}\\)"
  },
  {
    "objectID": "slides/slides-17-clt.html#activity",
    "href": "slides/slides-17-clt.html#activity",
    "title": "Central Limit Theorem",
    "section": "Activity",
    "text": "Activity"
  },
  {
    "objectID": "slides/slides-17-clt.html#examples",
    "href": "slides/slides-17-clt.html#examples",
    "title": "Central Limit Theorem",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "slides/slides-17-clt.html#proportion",
    "href": "slides/slides-17-clt.html#proportion",
    "title": "Central Limit Theorem",
    "section": "Proportion",
    "text": "Proportion\n\nRemember that a proportion is a mean:\n\\[\n\\hat{p} = \\frac{1}{n}\\sum_{i=1}^{n} x_{i}, \\qquad x_{i} =\\{0, 1\\}\n\\]\n\nTypically, \\(x_{i} = 1\\) is read as “success” and \\(x_{i} = 0\\) as “failure”, so \\(p\\) is probability or proportion of success\n\nSo the CLT applies to sampling distribution of sample proportions as well!\nCLT for sample proportions: if we have \\(n\\) independent binary observations with \\(np \\geq 10\\) and \\(n(1-p) \\geq 10\\), then:\n\n\\[\n\\hat{p} \\overset{\\cdot}{\\sim} N\\left(p, \\sqrt{\\frac{p(1-p)}{n}}\\right)\n\\]\n\n\nWhat do the conditions \\(np \\geq 10\\) and \\(n(1-p)\\geq 10\\) mean?\n\n```"
  },
  {
    "objectID": "slides/slides-17-clt.html#example",
    "href": "slides/slides-17-clt.html#example",
    "title": "Central Limit Theorem",
    "section": "Example",
    "text": "Example\nA poll of 100 randomly sampled registered voters in a town was conducted, asking voters if they support legalized marijuana. It was found that 60% of respondents were in support.\n\nWhat is the population parameter? What is the point estimate/statistic?\n\n\nFind a (symmetric) 90% confidence interval for the true proportion of town residents in favor of legalized marijuana.\n\n\n\nConditions for CLT met?\n\n\nIndependence: random sample\nSuccess-failure condition: \\(n\\hat{p} =100(0.6) = 60 \\geq 10\\) and \\(n(1-\\hat{p}) = 100(0.4) = 40 \\geq 10\\)"
  },
  {
    "objectID": "slides/slides-17-clt.html#height-example",
    "href": "slides/slides-17-clt.html#height-example",
    "title": "Central Limit Theorem",
    "section": "Height example",
    "text": "Height example\n\n\n\nThe average height of all NBA players in the 2008-9 season is 79.21 inches, with a standard deviation of 3.57 inches. We randomly sample \\(20\\) of these players and record their heights.\n\n\n\nWhat is the sampling distribution of the sample mean heights?"
  },
  {
    "objectID": "slides/slides-17-clt.html#height-example-solution",
    "href": "slides/slides-17-clt.html#height-example-solution",
    "title": "Central Limit Theorem",
    "section": "Height example: solution",
    "text": "Height example: solution\n\nWe have independent samples, but not a large sample size. However, the histogram of the data looks approximately Normal (no clear outliers).\nCLT applies! By CLT: \\(\\bar{X} \\overset{\\cdot}{\\sim} N\\left(79.21, \\frac{3.57}{\\sqrt{20}}\\right)\\)\nIf the data instead looked like the following, I would say normality condition is violated:"
  },
  {
    "objectID": "slides/slides-17-clt.html#bank-example",
    "href": "slides/slides-17-clt.html#bank-example",
    "title": "Central Limit Theorem",
    "section": "Bank example",
    "text": "Bank example\nCustomers are standing in line at a bank. The service time for each customer \\(i\\) is represented by \\(X_{i}\\). Suppose that the average service time for all customers is 5 minutes, with a standard deviation of 6 minutes.\n\nAssume that a bank currently has 36 customers in it, and all customers are independent of each other. What is the probability that the average service time of all these customers is less than 4 minutes?"
  },
  {
    "objectID": "slides/slides-17-clt.html#bank-example-solution",
    "href": "slides/slides-17-clt.html#bank-example-solution",
    "title": "Central Limit Theorem",
    "section": "Bank example: solution",
    "text": "Bank example: solution\n\nWe want \\(\\text{Pr}(\\bar{X} < 4)\\)\nConditions for CLT met: independence (random sample) and sufficiently large sample size \\((n=36)\\).\n\nSo by CLT, \\(\\bar{X} \\overset{\\cdot}{\\sim}N(5, \\frac{6}{\\sqrt{36}}) = N(5, 1)\\)\n\nUsing 68-95-99.7 rule, probability that the average service time of all these customers is less than 4 minutes is about \\(1 - (0.34 + 0.5) = 0.16\\)\n\npnorm(4, 5, 1) = 0.159"
  },
  {
    "objectID": "slides/slides-17-clt.html#mms-example",
    "href": "slides/slides-17-clt.html#mms-example",
    "title": "Central Limit Theorem",
    "section": "M&M’s example",
    "text": "M&M’s example\nMars, Inc. is the company that makes M&M’s. In 2008, Mars changed their color distribution to have 13% red candies.\n\n\nLet \\(p\\) be the proportion of red M&M’s in a random sample of \\(n\\) M&M’s. What is the distribution of \\(\\hat{p}\\) if we take random sample of size:\n\n\\(n = 100\\)\n\\(n = 10\\)"
  },
  {
    "objectID": "slides/slides-17-clt.html#why-is-clt-so-important",
    "href": "slides/slides-17-clt.html#why-is-clt-so-important",
    "title": "Central Limit Theorem",
    "section": "Why is CLT so important?",
    "text": "Why is CLT so important?\n\nAllows statisticians safely assume that the mean’s sampling distribution is approximately Normal. The Normal distribution has nice properties and is easy to work with.\nCan be applied to both continuous and discrete numeric data!\nDoes not depend on the underlying distribution of the data.\n\n\nFor many of these reasons, we can use the CLT for inference!"
  },
  {
    "objectID": "slides/slides-17-clt.html#clt-assumptions",
    "href": "slides/slides-17-clt.html#clt-assumptions",
    "title": "Central Limit Theorem",
    "section": "CLT assumptions",
    "text": "CLT assumptions\n\nRemark #1: does not require any assumption about how the data \\(x_{1},\\ldots, x_{n}\\) behave so long as the following assumptions hold:\n\nIndependent samples: usually achieved by random sampling\nSufficiently large sample size \\(n\\), where large is in relation to total size of population\n\nRemark #2: if the data \\(x_{1},\\ldots, x_{n}\\) are known to be Normal and independent, then the distribution of sample means is exactly Normal, even for small \\(n\\)\n\nFor this reason, if \\(n\\) is small we require the data to be Normal\nHow to know? We replace (2) above with the normality condition:\n\nIf \\(n\\) is small \\((n < 30)\\): we assume data are approximately normal if there are no clear outliers\nIf \\(n\\) is larger \\((30 \\leq n < ?)\\): we assume data are approximately normal if there are no particularly extreme outliers"
  },
  {
    "objectID": "slides/slides-17-clt.html#clt-for-proportions",
    "href": "slides/slides-17-clt.html#clt-for-proportions",
    "title": "Central Limit Theorem",
    "section": "CLT for proportions",
    "text": "CLT for proportions\nCLT for sample proportions: if we have \\(n\\) independent binary observations with \\(np \\geq 10\\) and \\(n(1-p) \\geq 10\\), then:\n\n\\[\n\\hat{p} \\overset{\\cdot}{\\sim} N\\left(p, \\sqrt{\\frac{p(1-p)}{n}}\\right)\n\\]\n\n\n\nWhat do the conditions \\(np \\geq 10\\) and \\(n(1-p)\\geq 10\\) mean?\n\nFor this reason, this is called the “success-failure” condition for CLT for proportions"
  },
  {
    "objectID": "slides/slides-17-clt.html#mms-example-solution",
    "href": "slides/slides-17-clt.html#mms-example-solution",
    "title": "Central Limit Theorem",
    "section": "M&M’s example: solution",
    "text": "M&M’s example: solution\nWe have independence due to the random sample. Need to check success-failure condition:\n\n\n\nIf \\(n= 100\\):\n\n\\(np = 100(0.13) = 13 \\geq 10\\)\n\\(n(1-p) = 100(0.87) = 87 \\geq 10\\)\n\nSo CLT applies:\n\n\n\\[\n\\begin{align*}\n\\hat{p} &\\overset{\\cdot}{\\sim} N\\left(0.13, \\sqrt{\\frac{0.13(1-0.13)}{100}}\\right) \\\\\n&= N(0.13, 0.034 )\n\\end{align*}\n\\]\n\n\n\nIf \\(n = 10\\):\n\n\\(np = 10(0.13) = 1.3 < 10\\)\n\nSuccess-failure condition not met. Cannot use CLT."
  },
  {
    "objectID": "slides/slides-17-clt.html#mms-example-cont.",
    "href": "slides/slides-17-clt.html#mms-example-cont.",
    "title": "Central Limit Theorem",
    "section": "M&M’s example (cont.)",
    "text": "M&M’s example (cont.)\nThe following histograms display sampling distributions for \\(\\hat{p}\\) = proportion of red candies in random samples of size \\(n = \\{10, 50, 100, 200\\}\\):"
  },
  {
    "objectID": "slides/slides-17-clt.html#mathematical-cis",
    "href": "slides/slides-17-clt.html#mathematical-cis",
    "title": "Central Limit Theorem",
    "section": "Mathematical CIs",
    "text": "Mathematical CIs\n\nRather than using simulation techniques (i.e. bootstrap) to obtain the sampling distribution, the CLT gives us the sampling distribution of a mean “for free”\n\n(assuming conditions are met)\n\nFormula for a (symmetric) \\(\\gamma \\times 100\\%\\) confidence interval:\n\\[\n\\text{point estimate} \\pm \\underbrace{\\text{critical value} \\times \\text{SE}}_{\\text{Margin of Error}}\n\\]\n\npoint estimate: the “best guess” statistic from our observed data (e.g. \\(\\hat{p}\\) and \\(\\bar{x}\\))\nSE: standard error of the statistic\ncritical value: percentile that guarantees the \\(\\gamma\\times 100\\). This will vary depending on your data/assumptions"
  },
  {
    "objectID": "slides/slides-17-clt.html#ci-for-a-single-proportion",
    "href": "slides/slides-17-clt.html#ci-for-a-single-proportion",
    "title": "Central Limit Theorem",
    "section": "CI for a single proportion",
    "text": "CI for a single proportion\nSuppose that I have a sample of \\(n\\) binary values, and I would like to obtain a \\(\\gamma \\times 100\\%\\) confidence interval for the probability of success \\(p\\).\n\nIf assumptions of CLT for sample proportions hold:\n\\[\n\\hat{p} \\overset{\\cdot}{\\sim} N\\left(p, \\sqrt{\\frac{p(1-p)}{n}}\\right)\n\\]\n\n\nHow do we know if success-failure condition holds without knowing \\(p\\)? Let’s use our best guess: \\(\\hat{p}\\)\n\nSo need \\(n\\hat{p}\\) and \\(n(1-\\hat{p})\\) both \\(\\geq 10\\)\n\n\n\nPoint estimate: observed \\(\\hat{p}\\) from our sample\nStandard error: \\(\\sqrt{p(1-p)/n}\\)\n\nBut we still don’t have \\(p\\)! Instead use the following approximation for CI: \\(\\text{SE}(\\hat{p}) \\approx \\sqrt{\\hat{p}(1-\\hat{p})/n}\\)"
  },
  {
    "objectID": "slides/slides-17-clt.html#critical-value",
    "href": "slides/slides-17-clt.html#critical-value",
    "title": "Central Limit Theorem",
    "section": "Critical value",
    "text": "Critical value\n\nAt this point: \\(\\hat{p} \\overset{\\cdot}{\\sim} N\\left(p, \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\right)\\)\nIf we want a \\(\\gamma \\times 100\\%\\) CI for \\(p\\), we just need the bounds of the middle \\(\\gamma \\times 100\\%\\) of the Normal distribution above!\n\nThese are the \\((1-\\gamma)/2\\) and \\((1+\\gamma)/2\\) percentiles\nBut…we still don’t know \\(p\\)\n\n\n\nCritical value: instead, we use the percentiles of the standard normal \\(N(0,1)\\) distribution: \\(z_{(1-\\gamma)/2}^{*}\\) and \\(z_{(1+\\gamma)/2}^{*}\\)\n\nSince the normal distribution is symmetric, \\(z_{(1+\\gamma)/2}^{*} = - z_{(1-\\gamma)/2}^{*}\\)"
  },
  {
    "objectID": "slides/slides-17-clt.html#example-cont.",
    "href": "slides/slides-17-clt.html#example-cont.",
    "title": "Central Limit Theorem",
    "section": "Example (cont.)",
    "text": "Example (cont.)\n\nFind a (symmetric) 90% confidence interval for the true proportion of town residents in favor of legalized marijuana.\n\n\nGathering components for CI:\n\n\nPoint estimate: \\(\\hat{p}\\) = 0.6\nStandard error: \\(\\text{SE}(\\hat{p}) \\approx \\sqrt{\\frac{0.6(0.4)}{100}} \\approx 0.049\\)\n\nCritical value: what percentiles do we want?\n\n\n\\(z_{0.95}^{*} =\\) qnorm(0.95, mean = 0, sd = 1) \\(\\approx 1.645\\)\n\n\n\nSo our 90% confidence interval for \\(p\\) is:\n\\[\n0.6 \\pm 1.645(0.049) = (0.519, 0.681)\n\\]\n\n\n\nInterpret the confidence interval in context!"
  },
  {
    "objectID": "slides/slides-17-clt.html#towards-a-ci-for-a-single-proportion",
    "href": "slides/slides-17-clt.html#towards-a-ci-for-a-single-proportion",
    "title": "Central Limit Theorem",
    "section": "Towards a CI for a single proportion",
    "text": "Towards a CI for a single proportion\nSuppose that I have a sample of \\(n\\) binary (0/1) values. I want a \\(\\gamma \\times 100\\%\\) confidence interval for the probability of success \\(p\\) using the sample.\n\nIf assumptions of CLT for sample proportions hold, then we know\n\\[\n\\hat{p} \\overset{\\cdot}{\\sim} N\\left(p, \\sqrt{\\frac{p(1-p)}{n}}\\right)\n\\]\n\n\nWe can use/manipulate this result to obtain a confidence interval for the unknown \\(p\\)!\nHow do we know if success-failure condition holds without knowing \\(p\\)?\n\nLet’s use our best guess: \\(\\hat{p}\\)\nSuccess-failure condition for inference: \\(n\\hat{p}\\) and \\(n(1-\\hat{p})\\) both \\(\\geq 10\\)"
  },
  {
    "objectID": "slides/slides-17-clt.html#ci-for-single-proportion",
    "href": "slides/slides-17-clt.html#ci-for-single-proportion",
    "title": "Central Limit Theorem",
    "section": "CI for single proportion",
    "text": "CI for single proportion\nSo the formula for a (symmetric) \\(\\gamma\\times 100\\%\\) CI for \\(p\\) is:\n\n\\[\n\\hat{p} \\pm z_{(1+\\gamma)/2}^{*}\\times \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]where the critical value is obtained from \\(N(0,1)\\) distribution\n\n\nCome take STAT 311 to see why this is our CI!"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#housekeeping",
    "href": "slides/slides-18-ci-mean.html#housekeeping",
    "title": "Confidence Intervals for a Mean",
    "section": "Housekeeping",
    "text": "Housekeeping"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#recap",
    "href": "slides/slides-18-ci-mean.html#recap",
    "title": "Confidence Intervals for Means",
    "section": "Recap",
    "text": "Recap\n\n\n\n\nCentral Limit Theorem: if we have a sufficiently large sample of \\(n\\) independent observations from a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then \\(\\bar{X} \\overset{\\cdot}{\\sim} N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\\)\nWhen considering the special case of sample proportions, if success-failure condition is met, we have \\(\\hat{p} \\overset{\\cdot}{\\sim} N\\left(p, \\sqrt{\\frac{p(1-p)}{n}}\\right)\\)\nTo obtain a \\(\\gamma\\times 100\\%\\) CI for a mean, we use\n\n\\[\n\\text{point estimate} \\pm \\text{critical value} \\times \\text{SE}\n\\]\n\n\nWe needed to replace the standard error with an estimate"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#checking-normality",
    "href": "slides/slides-18-ci-mean.html#checking-normality",
    "title": "Confidence Intervals for Means",
    "section": "Checking normality",
    "text": "Checking normality\n\nRemember, CLT requires a sufficiently large sample size \\(n\\) or assumption of Normality of the underlying data.\nNo perfect way to check Normality, but rule of thumb:\n\nIf \\(n < 30\\) small: check that there are no clear outliers\nIf \\(n \\geq 30\\) large: check that there are no particularly extreme outliers"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#ci-for-a-single-mean-known-variance",
    "href": "slides/slides-18-ci-mean.html#ci-for-a-single-mean-known-variance",
    "title": "Confidence Intervals for Means",
    "section": "CI for a single mean (known variance)",
    "text": "CI for a single mean (known variance)\nSuppose we want a \\(\\gamma\\times 100\\%\\) CI for population mean \\(\\mu\\).\n\nWhat would your “best guess” point estimate for \\(\\mu\\) be?\n\n\nIf CLT holds, then we know\n\\[\n\\bar{X} \\overset{\\cdot}{\\sim} N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\n\\]\nSo our \\(\\gamma \\times 100\\%\\) CI for \\(\\mu\\) is:\n\\[\n\\text{point estimate} \\pm \\underbrace{\\text{critical value} \\times \\text{SE}}_{\\text{Margin of Error}} = \\bar{x} \\pm z_{(1+\\gamma)/2}^* \\times \\frac{\\sigma}{\\sqrt{n}}\n\\]"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#example-age-at-marriage",
    "href": "slides/slides-18-ci-mean.html#example-age-at-marriage",
    "title": "Confidence Intervals for Means",
    "section": "Example: age at marriage",
    "text": "Example: age at marriage\n\n\n\nIn 2006-2010, the CDC conducted a thorough survey asking US women their age at first marriage. The standard deviation of the responses is 4.72 years. Suppose we randomly sample 25 US women and ask them their age at first marriage (plotted below). Their average age at marriage was 23.32.\n\n\n\n\n\n\n\n\nWhat is/are the population parameter(s)? What is the statistic?\n\n\n\nWe will obtain an 80% confidence interval for the mean age of US women at first marriage.\n\n\n\nAre conditions of CLT met?\nIf so, what does CLT tell us?"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#example-age-at-marriage-cont.",
    "href": "slides/slides-18-ci-mean.html#example-age-at-marriage-cont.",
    "title": "Confidence Intervals for Means",
    "section": "Example: age at marriage (cont.)",
    "text": "Example: age at marriage (cont.)\n\nObtain an 80% confidence interval for the mean age of US women at first marriage.\n\n\n\n\nBy CLT: \\[\\bar{X} \\overset{\\cdot}{\\sim}N\\left(\\mu, \\frac{4.72}{\\sqrt{25}}\\right) = N(\\mu, 0.944)\\]\n\nCollect necessary components:\n\n\nPoint estimate: \\(\\bar{x} = 23.32\\)\nStandard error: \\(0.944\\)\nCritical value: \\(z_{0.9}^{*} =\\) qnorm(0.9, 0, 1) \\(= 1.28\\)\n\n\nSo our 80% confidence interval is \\(23.32 \\pm 1.28 \\times 0.944 = (22.11, 24.53)\\)\n\n\n\nInterpret this interval!"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#utility-of-this-model",
    "href": "slides/slides-18-ci-mean.html#utility-of-this-model",
    "title": "Confidence Intervals for Means",
    "section": "Utility of this model",
    "text": "Utility of this model\n\nThe previous formula for the confidence interval for \\(\\mu\\) relies on knowing \\(\\sigma\\)\nBut wait…\n\nWant to construct a CI for \\(\\mu\\) because we don’t know its value\nIf we don’t know \\(\\mu\\), it seems highly unlikely that we would know \\(\\sigma\\)!\n\nSo in practice, we will have to estimate standard error for \\(\\bar{X}\\):\n\n\n\\[\n    \\text{SE}\\approx \\frac{s}{\\sqrt{n}}\n\\]\nwhere \\(s\\) is the observed sample standard deviation\n\n\nRecall we did something similar for CI for \\(p\\)"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#variance-issue",
    "href": "slides/slides-18-ci-mean.html#variance-issue",
    "title": "Confidence Intervals for Means",
    "section": "Variance issue",
    "text": "Variance issue\n\nReplacing \\(s\\) for \\(\\sigma\\) works well enough when \\(n\\) is extremely large so we can estimate \\(\\sigma\\) accurately\nHowever, estimating variance is extremely difficult when \\(n\\) is small, and still not great for large \\(n\\)\nSo if \\(\\sigma\\) is unknown, we cannot use the Normal approximation to model \\(\\bar{X}\\) for inferential tasks\nInstead, we will use a new distribution for inference calculations, called the \\(t\\)-distribution"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#quick-remarks",
    "href": "slides/slides-18-ci-mean.html#quick-remarks",
    "title": "Confidence Intervals for a Mean",
    "section": "Quick remarks",
    "text": "Quick remarks\n\nIf \\(n\\) small, \\(\\sigma\\) unknown, but underlying distribution is Normal, then CLT says \\(\\bar{X} \\sim N(\\mu, \\sigma/\\sqrt{n})\\) exactly\n\nBUT in order to use this result for CIs, we need to know \\(\\sigma\\)"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#t-distribution",
    "href": "slides/slides-18-ci-mean.html#t-distribution",
    "title": "Confidence Intervals for Means",
    "section": "\\(t\\)-distribution",
    "text": "\\(t\\)-distribution\n\nThe \\(t\\)-distribution is symmetric and bell-curved (like the Normal distribution)\nHas “thicker tails” than the Normal distribution (the tails decay more slowly)\n\n\n\n\n\n\n\n\n\n\n\n\n\\(t\\)-distribution is always centered at 0\nOne parameter: degrees of freedom (df) defines exact shape of the \\(t\\)\n\nDenoted \\(t_{df}\\) (e.g. \\(t_{1}\\) or \\(t_{20}\\))\n\n\n\n\n\nAs \\(df\\) increase, \\(t\\) resembles the \\(N(0,1)\\). When \\(df \\geq 30\\), the \\(t_{df}\\) is nearly identical to \\(N(0,1)\\)"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#working-with-t-distribution",
    "href": "slides/slides-18-ci-mean.html#working-with-t-distribution",
    "title": "Confidence Intervals for Means",
    "section": "Working with \\(t\\) distribution",
    "text": "Working with \\(t\\) distribution\n\n\n\nLet’s draw pictures for the following:\n\nWhat proportion of the \\(t_{2}\\)-distribution falls below -1.5?\nWhat value of the \\(t_{2}\\)-distribution has \\(70\\%\\) area lying below it?"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#t-distribution-in-r",
    "href": "slides/slides-18-ci-mean.html#t-distribution-in-r",
    "title": "Confidence Intervals for Means",
    "section": "\\(t\\) distribution in R",
    "text": "\\(t\\) distribution in R\n\npnorm(x, mean, sd) and qnorm(%, mean, sd) used to find probabilities and percentiles for the Normal distribution\nAnalogous functions for \\(t\\)-distribution: pt(x, df) and qt(%, df)\n\n\n\n\n\n\n\n\n\n\n\npt(-1.5,df =2) = 0.1361966\n\n\n\n\n\n\n\n\n\n\nqt(0.7, df =2) = 0.6172134"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#ci-for-a-single-mean-unknown-variance",
    "href": "slides/slides-18-ci-mean.html#ci-for-a-single-mean-unknown-variance",
    "title": "Confidence Intervals for Means",
    "section": "CI for a single mean (unknown variance)",
    "text": "CI for a single mean (unknown variance)\n\nStill require independent observations and the Normality condition for CLT\nGeneral formula for \\(\\gamma \\times 100\\%\\) CI is the same, but we simply change what goes into the margin of error.\n\n\n\\[\n\\begin{align*}\n\\text{point estimate} &\\pm t^*_{df, (1+\\gamma)/2} \\times \\widehat{\\text{SE}} \\\\\n\\bar{x} &\\pm t_{df, (1+\\gamma)/2}^* \\times \\frac{s}{\\sqrt{n}}\n\\end{align*}\n\\]\n\n\n\\(df = n-1\\)\ncritical value \\(t^*_{df, (1+\\gamma)/2}\\) = \\((1+\\gamma)/2\\) percentile of the \\(t_{df}\\) distribution"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#example-age-at-marriage-cont.-1",
    "href": "slides/slides-18-ci-mean.html#example-age-at-marriage-cont.-1",
    "title": "Confidence Intervals for Means",
    "section": "Example: age at marriage (cont.)",
    "text": "Example: age at marriage (cont.)\nLet’s return to the age at marriage example. Once again let’s obtain an 80% confidence interval for the average age of first marriage for US women, but now suppose we don’t know \\(\\sigma\\).\n\nIn our sample of \\(n = 25\\) women, we observed a sample mean of \\(23.32\\) years and a sample standard deviation of \\(s = 4.03\\) years.\n\n\n\n\n\nPoint estimate: \\(\\bar{x} = 23.32\\)\nStandard error: \\(\\widehat{\\text{SE}} = \\frac{s}{\\sqrt{n}}= \\frac{4.03}{\\sqrt{25}} = 0.806\\)\nCritical value:\n\n\\(df = n-1 = 24\\)\n\\(t_{24}^*\\) = qt(0.9, df =24) = 1.32\n\n\n\nSo our 80% confidence interval for \\(\\mu\\) is:\n\\[\n23.32 \\pm 1.32 \\times 0.806 = (22.26, 24.38)\n\\]"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#comparing-cis",
    "href": "slides/slides-18-ci-mean.html#comparing-cis",
    "title": "Confidence Intervals for Means",
    "section": "Comparing CIs",
    "text": "Comparing CIs\n\n\nKnown variance:\n80% CI: (22.11, 24.53)\n\nUnknown variance:\n80% CI: (22.26, 24.38)\n\n\n\n\nHow do the two intervals compare?\n\nInterpretation of CI does not change even if we use a different model!"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#examples",
    "href": "slides/slides-18-ci-mean.html#examples",
    "title": "Confidence Intervals for Means",
    "section": "Examples",
    "text": "Examples\nAssume that all conditions necessary for inference are satisfied.\n\n\n\nqnorm(0.90) = 1.28\nqnorm(0.95) = 1.64\nqnorm(0.975) = 1.96\n\n\n\nqt(0.90, df = 35) = 1.31\nqt(0.95, df = 35) = 1.69\nqt(0.975, df = 35) = 2.03\n\n\n\nqt(0.90, df = 36) = 1.31\nqt(0.95, df = 36) = 1.69\nqt(0.975, df = 36) = 2.03\n\n\n\n\n\nA 90% confidence interval for a population mean \\(\\mu\\) is given as \\((18.985, 21.015)\\). The interval was obtained based on a SRS for 36 observations. Calculate the sample mean and sample standard deviation.\nThe standard deviation for students at particular Ivy League college is 250 points. Two students, Raina and Luke, want to estimate the average SAT score of students at this college. They want their margin of error to be no more than 25 points.\n\nRaina wants to use a 90% confidence level. How large a sample does Raina need to collect?\nLuke wants to use a 95% confidence level. Without calculations, determine whether Luke’s sample should be larger or smaller than Raina’s. Explain your reasoning.\nCalculate the minimum sample size for Luke."
  },
  {
    "objectID": "practice_probs/practice-17-clt.html",
    "href": "practice_probs/practice-17-clt.html",
    "title": "CLT and CIs for proportion",
    "section": "",
    "text": "A survey found that American families generate an average of 17.2 pounds of glass garbage each year. Assume that the standard deviation is 2.5 pounds.\nSuppose we randomly survey 40 families. Set up a calculation for (and if you have access to R, actually calculate) the probability that the mean of glass garbage of these 40 families is less than 18 pounds.\nDefine what a sampling distribution of the sample proportion is. Describe how the shape, center, and spread of the sampling distribution change as the sample size increases when \\(p=0.2\\).\nA survey of 1509 high school seniors who took the SAT and who completed an optional web survey shows that 55% of high school seniors are fairly certain that they will participate in a study abroad program in college.\n\nIs this sample a representative sample from the population of all high school seniors in the US? Explain.\nSuppose the conditions for inference are met, regardless of your answer in (a). Using a mathematical model, construct a 90% confidence interval for the proportion of high school seniors who are fairly certain they will participate in a study abroad program in college. Interpret this interval in context.\nBased on this interval, would it be appropriate to claim that the majority of high school seniors are fairly certain they will participate in a study abroad program in college?\n\n\\((^*)\\) The average teacher salary in Vermont is $62,483. Suppose that the distribution of teacher salaries is approximately normal with standard deviation $7000.\n\nWhat is the probability that a randomly selected Vermont teacher makes less than $60,000 per year?\nIf we randomly sample 25 Vermont teachers and obtain their salaries, what is the probability that the mean of their salaries is less than $60,000 per year?\nCompare the probabilities in (a) and (b), and explain mathematically why one is larger than the other.\nHow would your answers to (a) and (b) change if the distribution of teacher salaries was not normal?"
  },
  {
    "objectID": "slides/slides-17-clt.html",
    "href": "slides/slides-17-clt.html",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "Office hours tomorrow: 10:30am-12:00pm"
  },
  {
    "objectID": "slides/slides-17-clt.html#comprehension-questions",
    "href": "slides/slides-17-clt.html#comprehension-questions",
    "title": "Central Limit Theorem",
    "section": "Comprehension questions",
    "text": "Comprehension questions\n\nWhat is the main takeaway of the CLT?\nWhat are the assumptions of the CLT?\nHow do we construct a \\(\\gamma \\times 100\\%\\) confidence interval using a mathematical model?"
  },
  {
    "objectID": "slides/slides-17-clt.html#normality-condition",
    "href": "slides/slides-17-clt.html#normality-condition",
    "title": "Central Limit Theorem",
    "section": "Normality condition",
    "text": "Normality condition\n\nDo you believe the large sample size/normality condition is satisified in the following two samples?\n\n\n\n\n\n\nSample 1: small \\(n < 30\\). But histogram and boxplot reveals no clear outliers, so I would say normality condition is met.\nSample 2: larger \\(n \\geq 30\\). Even though \\(n\\) is larger, there is a particularly extreme outlier, so I would say normality condition is not met."
  },
  {
    "objectID": "slides/slides-17-clt.html#proportion-as-a-mean",
    "href": "slides/slides-17-clt.html#proportion-as-a-mean",
    "title": "Central Limit Theorem",
    "section": "Proportion as a mean",
    "text": "Proportion as a mean\nRemember \\(\\hat{p}\\) is a sample mean! So the CLT applies to proportions as well!\n\\[\n\\hat{p} = \\frac{1}{n}\\sum_{i=1}^{n} x_{i} \\qquad \\qquad x_{i} =\\{0, 1\\}\n\\]\n\nTypically, \\(x_{i} = 1\\) is read as “success” and \\(x_{i} = 0\\) as “failure”, so \\(p\\) is the population-level probability of success"
  },
  {
    "objectID": "slides/slides-17-clt.html#towards-a-ci-for-a-single-proportion-cont.",
    "href": "slides/slides-17-clt.html#towards-a-ci-for-a-single-proportion-cont.",
    "title": "Central Limit Theorem",
    "section": "Towards a CI for a single proportion (cont.)",
    "text": "Towards a CI for a single proportion (cont.)\n\nPoint estimate: observed \\(\\hat{p}\\) from our sample\nStandard error: \\(\\sqrt{p(1-p)/n}\\)\n\nBut we still don’t have \\(p\\)!\nInstead, use the following approximation for CI:\n\n\n\n\\[\\text{SE}(\\hat{p}) \\approx \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\n\n\nCritical value: to obtain the middle \\(\\gamma \\times 100\\%\\) part, we use the \\((1-\\gamma)/2\\) and \\((1+\\gamma)/2\\) percentiles of the \\(N(0,1)\\) distribution\n\n\\(z_{(1-\\gamma)/2}^{*}\\) (lower bound) and \\(z_{(1+\\gamma)/2}^{*}\\) (upper bound)\nNote: \\(z_{(1+\\gamma)/2}^{*} = - z_{(1-\\gamma)/2}^{*}\\)"
  },
  {
    "objectID": "slides/slides-17-clt.html#warm-up",
    "href": "slides/slides-17-clt.html#warm-up",
    "title": "Central Limit Theorem",
    "section": "Warm-up",
    "text": "Warm-up\n\nLet \\(Z \\sim N(0,1)\\). If the 10th percentile of \\(Z\\) is -1.28, what is the 90th percentile?\nLet \\(X \\sim N(0,2)\\). If the 10th percentile of \\(X\\) is -2.56, what is the 90th percentile? Or can you not say without code?\nLet \\(Y \\sim N(2,1)\\). If the 10th percentile of \\(Y\\) is 0.72, what is the 90th percentile? Or can you not say without code?"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#paired-data",
    "href": "slides/slides-18-ci-mean.html#paired-data",
    "title": "Confidence Intervals for Means",
    "section": "Paired data",
    "text": "Paired data\nSuppose we have two sets of observations/data \\(\\boldsymbol{x} = (x_{1}, x_{2}, \\ldots x_{n})\\) and \\(\\boldsymbol{y} = (y_{1}, y_{2}, \\ldots, y_{n})\\)\n\nThe data are considered paired data if each \\(x_{i}\\) corresponds to exactly one \\(y_{i}\\)\nExample: your score on the midterm and your score on the final\nWhen analyzing paired data, we are typically interested in the difference in outcomes of each pair of observations"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#paired-differences",
    "href": "slides/slides-18-ci-mean.html#paired-differences",
    "title": "Confidence Intervals for Means",
    "section": "Paired differences",
    "text": "Paired differences\n\nLet \\(d_{i} = y_{i} - x_{i}\\) for each \\(i = 1,\\ldots, n\\) be the observed differences\nThe \\(d_{i}\\) come from larger population with true mean difference \\(\\mu_{d}\\) and standard deviation of differences \\(\\sigma_{d}\\)\nThe sample mean difference and sample standard deviation of the differences are\n\n\n\\[\\bar{d} = \\frac{1}{n}\\sum_{i=1}^{n} d_{i} \\qquad \\qquad s_{d} = \\frac{1}{n-1}\\sum_{i=1}^{n} (d_{i} - \\bar{d})^2 \\]"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#clt-for-mean-difference-in-pairs",
    "href": "slides/slides-18-ci-mean.html#clt-for-mean-difference-in-pairs",
    "title": "Confidence Intervals for Means",
    "section": "CLT for mean difference in pairs",
    "text": "CLT for mean difference in pairs\n\nSuppose the \\(n\\) observational units are independent and the distribution of the differences is approximately normal. Then CLT says:\n\\[\n\\bar{d} \\overset{\\cdot}{\\sim} N\\left(\\mu_{d}, \\frac{\\sigma_{d}}{\\sqrt{n}} \\right)\n\\]\nWe are usually interested in performing inference for \\(\\mu_{d}\\) when both \\(\\mu_{d}\\) and \\(\\sigma_{d}\\) unknown\nOur formula for \\(\\gamma\\times 100\\%\\) CI for \\(\\mu_{d}\\) is analogous to the formula for one mean when \\(\\sigma\\) unknown:\n\n\n\\[\n\\begin{align*}\n\\text{point estimate} &\\pm t^*_{df, (1+\\gamma)/2} \\times \\widehat{\\text{SE}} \\\\\n\\bar{d} &\\pm t_{df, (1+\\gamma)/2}^* \\times \\frac{s_{d}}{\\sqrt{n}}\n\\end{align*}\n\\]\nwhere \\(df = n-1\\)"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#ci-for-mean-difference-in-pairs",
    "href": "slides/slides-18-ci-mean.html#ci-for-mean-difference-in-pairs",
    "title": "Confidence Intervals for Means",
    "section": "CI for mean difference in pairs",
    "text": "CI for mean difference in pairs\n\nWe are usually interested in performing inference for \\(\\mu_{d}\\) when both \\(\\mu_{d}\\) and \\(\\sigma_{d}\\) unknown\nOur formula for \\(\\gamma\\times 100\\%\\) CI for \\(\\mu_{d}\\) is analogous to the formula for one mean when \\(\\sigma\\) unknown:\n\n\n\\[\n\\begin{align*}\n\\text{point estimate} &\\pm t^*_{df, (1+\\gamma)/2} \\times \\widehat{\\text{SE}} \\\\\n\\bar{d} &\\pm t_{df, (1+\\gamma)/2}^* \\times \\frac{s_{d}}{\\sqrt{n}}\n\\end{align*}\n\\]\nwhere \\(df = n-1\\)"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#example-zinc",
    "href": "slides/slides-18-ci-mean.html#example-zinc",
    "title": "Confidence Intervals for Means",
    "section": "Example: zinc",
    "text": "Example: zinc\n\n\n\nData consist of measured zinc concentrations in bottom water and surface water at 10 randomly sampled wells:\n\nDo the data suggest that the true average concentration in the bottom water is different than that of surface water? Let’s answer this using a 95% confidence interval.\n\n\n\n\n\n\n\n\n\n\n\n\n  bottom surface\n1  0.430   0.415\n2  0.266   0.238\n3  0.567   0.390\n4  0.531   0.410\n5  0.707   0.605\n6  0.716   0.609\n\n\n\nAre the data paired? Does CLT apply?"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#example-zinc-cont.",
    "href": "slides/slides-18-ci-mean.html#example-zinc-cont.",
    "title": "Confidence Intervals for Means",
    "section": "Example: zinc (cont.)",
    "text": "Example: zinc (cont.)\n\n\n\nzinc <- zinc |>\n  mutate(d = bottom - surface)\nd_bar <- mean(zinc$d)\nd_bar\n\n[1] 0.0804\n\ns_d <- sd(zinc$d)\ns_d\n\n[1] 0.05227321\n\n\n\n\n\n\n\npoint estimate: \\(\\bar{d} = 0.0804\\)\nSE \\(\\approx\\) \\(\\frac{s_{d}}{\\sqrt{n}} = \\frac{0.052}{\\sqrt{10}} = 0.016\\)\n\ncritical value: what code would you write?\n\n\n\\(df = n-1 = 9\\)\n\\(t_{9, 0.975}^{*} =\\) qt(0.975,9) \\(= 2.26\\)\n\n\n\n\n\nSo our 95% confidence interval is:\n\\[0.0804 \\pm 2.26(0.016) = (0.044, 0.117)\\]\n\n\n\nDo the data suggest that the true average concentration in the bottom water is different than that of surface water? Explain."
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#example-zinc-cont.-1",
    "href": "slides/slides-18-ci-mean.html#example-zinc-cont.-1",
    "title": "Confidence Intervals for a Mean",
    "section": "Example: zinc (cont.)",
    "text": "Example: zinc (cont.)"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#difference-of-two-means",
    "href": "slides/slides-18-ci-mean.html#difference-of-two-means",
    "title": "Confidence Intervals for Means",
    "section": "Difference of two means",
    "text": "Difference of two means\nNow consider two populations under the condition that the data/populations are not paired.\nWe might be interested in learning about whether or not the means of each population are equal (think about the voice jitter homework problem)!\n\nLet \\(\\mu_{1}\\) and \\(\\mu_{2}\\) represent the population means for the two populations 1 and 2\nSamples of size \\(n_{1}\\) and \\(n_{2}\\) from each population, respectively\nWe might think it reasonable to use \\(\\bar{x}_{1} - \\bar{x}_{2}\\) as a point estimate for \\(\\mu_{1} - \\mu_{2}\\)"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#conditions-for-inference",
    "href": "slides/slides-18-ci-mean.html#conditions-for-inference",
    "title": "Confidence Intervals for Means",
    "section": "Conditions for inference",
    "text": "Conditions for inference\nNow that we have two populations, conditions for CLT and use of the \\(t\\)-distribution for inference will look slightly different:\n\nIndependence (extended): need data within and between the two groups\n\ne.g.the two data sets come from independent random samples or from a randomized experiment\n\nNormality: we need to check for approximate normality for both groups separately"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#formula-for-ci-for-difference-in-two-means",
    "href": "slides/slides-18-ci-mean.html#formula-for-ci-for-difference-in-two-means",
    "title": "Confidence Intervals for a Mean",
    "section": "Formula for CI for difference in two means",
    "text": "Formula for CI for difference in two means\nIf the conditions hold, then our usual formula for \\(\\gamma \\times 100\\%\\) CI still holds:\n\\[\n\\text{point estimate} \\pm \\text{critical value} \\times \\text{SE}\n\\]\n\npoint estimate: \\(\\bar{x}_{1} - \\bar{x}_{2}\\)\n\n\n\n\n\n\n\n\nIf \\(\\sigma_{1}\\) and \\(\\sigma_{2}\\) known:\n\n\n\n\\(\\text{SE} = \\sqrt{\\frac{\\sigma_{1}^2}{n_{1}} + \\frac{\\sigma_{2}^2}{n_{2}}}\\)\ncritical value: \\(z_{(1+\\gamma)/2}^*\\)\n\ni.e. \\((1+\\gamma)/2\\) percentile of \\(N(0,1)\\)\n\n\n\n\n\n\n\nIf \\(\\sigma_{1}\\) and \\(\\sigma_{2}\\) unknown:\n\n\n\n\\(\\widehat{\\text{SE}} = \\sqrt{\\frac{s_{1}^2}{n_{1}} + \\frac{s_{2}^2}{n_{2}}}\\)\ncritical value: \\(t_{df, (1+\\gamma)/2}^*\\)\n\ni.e. \\((1+\\gamma)/2\\) percentile of \\(t_{df}\\)\n\\(df = \\min\\{n_{1} -1, n_{2} - 1\\}\\)"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#example-birth-term",
    "href": "slides/slides-18-ci-mean.html#example-birth-term",
    "title": "Confidence Intervals for Means",
    "section": "Example: birth term",
    "text": "Example: birth term\nThe births dataset from openintro contains a random sample of births for babies in NC where the mother was or was not a smoker. We will consider the birth weights of babies who were carried to full term.\n\n\n\n\nConvince yourself that this data isn’t paired!\n\n\nPopulation 1: mothers who smoke (in NC)\nPopulation 2: mothers who don’t smoke (in NC)\n\n\nResearch question: are average birth weight of full term babies different between smoking and non-smoking mothers? Create a 95% confidence interval to answer this question.\n\n\nWe care about the difference in means \\(\\mu_{smoke} - \\mu_{non}\\)"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#example-birth-term-cont.",
    "href": "slides/slides-18-ci-mean.html#example-birth-term-cont.",
    "title": "Confidence Intervals for Means",
    "section": "Example: birth term (cont.)",
    "text": "Example: birth term (cont.)\n\nAre average voice shimmers different between people with and without Parkinson’s? Create a 95% confidence interval to answer this question.\n\n\n\n\n\n\n\n\nstatus\nn\nxbar\ns\n\n\n\n\nHealthy\n48\n0.163\n0.058\n\n\nPD\n147\n0.321\n0.208\n\n\n\n\n\n\n\n\nList of 136\n $ line                            :List of 6\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : num 0.5\n  ..$ linetype     : num 1\n  ..$ lineend      : chr \"butt\"\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ rect                            :List of 5\n  ..$ fill         : chr \"white\"\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : num 0.5\n  ..$ linetype     : num 1\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ text                            :List of 11\n  ..$ family       : chr \"\"\n  ..$ face         : chr \"plain\"\n  ..$ colour       : chr \"black\"\n  ..$ size         : num 28\n  ..$ hjust        : num 0.5\n  ..$ vjust        : num 0.5\n  ..$ angle        : num 0\n  ..$ lineheight   : num 0.9\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : logi FALSE\n  ..$ inherit.blank: logi FALSE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ title                           : NULL\n $ aspect.ratio                    : NULL\n $ axis.title                      : NULL\n $ axis.title.x                    :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 2.75points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.top                :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 2.75points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.bottom             : NULL\n $ axis.title.y                    :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : num 90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.75points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.y.left               : NULL\n $ axis.title.y.right              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : num -90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 2.75points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text                       :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : chr \"grey30\"\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x                     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 2.2points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.top                 :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 2.2points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.bottom              : NULL\n $ axis.text.y                     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 1\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.2points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.y.left                : NULL\n $ axis.text.y.right               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 2.2points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.theta                 : NULL\n $ axis.text.r                     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0.5\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.2points 0points 2.2points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.ticks                      : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.ticks.x                    : NULL\n $ axis.ticks.x.top                : NULL\n $ axis.ticks.x.bottom             : NULL\n $ axis.ticks.y                    : NULL\n $ axis.ticks.y.left               : NULL\n $ axis.ticks.y.right              : NULL\n $ axis.ticks.theta                : NULL\n $ axis.ticks.r                    : NULL\n $ axis.minor.ticks.x.top          : NULL\n $ axis.minor.ticks.x.bottom       : NULL\n $ axis.minor.ticks.y.left         : NULL\n $ axis.minor.ticks.y.right        : NULL\n $ axis.minor.ticks.theta          : NULL\n $ axis.minor.ticks.r              : NULL\n $ axis.ticks.length               : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ axis.ticks.length.x             : NULL\n $ axis.ticks.length.x.top         : NULL\n $ axis.ticks.length.x.bottom      : NULL\n $ axis.ticks.length.y             : NULL\n $ axis.ticks.length.y.left        : NULL\n $ axis.ticks.length.y.right       : NULL\n $ axis.ticks.length.theta         : NULL\n $ axis.ticks.length.r             : NULL\n $ axis.minor.ticks.length         : 'rel' num 0.75\n $ axis.minor.ticks.length.x       : NULL\n $ axis.minor.ticks.length.x.top   : NULL\n $ axis.minor.ticks.length.x.bottom: NULL\n $ axis.minor.ticks.length.y       : NULL\n $ axis.minor.ticks.length.y.left  : NULL\n $ axis.minor.ticks.length.y.right : NULL\n $ axis.minor.ticks.length.theta   : NULL\n $ axis.minor.ticks.length.r       : NULL\n $ axis.line                       : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.line.x                     : NULL\n $ axis.line.x.top                 : NULL\n $ axis.line.x.bottom              : NULL\n $ axis.line.y                     : NULL\n $ axis.line.y.left                : NULL\n $ axis.line.y.right               : NULL\n $ axis.line.theta                 : NULL\n $ axis.line.r                     : NULL\n $ legend.background               : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.margin                   : 'margin' num [1:4] 5.5points 5.5points 5.5points 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing                  : 'simpleUnit' num 11points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing.x                : NULL\n $ legend.spacing.y                : NULL\n $ legend.key                      : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.key.size                 : 'simpleUnit' num 1.2lines\n  ..- attr(*, \"unit\")= int 3\n $ legend.key.height               : NULL\n $ legend.key.width                : NULL\n $ legend.key.spacing              : 'simpleUnit' num 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ legend.key.spacing.x            : NULL\n $ legend.key.spacing.y            : NULL\n $ legend.frame                    : NULL\n $ legend.ticks                    : NULL\n $ legend.ticks.length             : 'rel' num 0.2\n $ legend.axis.line                : NULL\n $ legend.text                     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.text.position            : NULL\n $ legend.title                    :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.title.position           : NULL\n $ legend.position                 : chr \"right\"\n $ legend.position.inside          : NULL\n $ legend.direction                : NULL\n $ legend.byrow                    : NULL\n $ legend.justification            : chr \"center\"\n $ legend.justification.top        : NULL\n $ legend.justification.bottom     : NULL\n $ legend.justification.left       : NULL\n $ legend.justification.right      : NULL\n $ legend.justification.inside     : NULL\n $ legend.location                 : NULL\n $ legend.box                      : NULL\n $ legend.box.just                 : NULL\n $ legend.box.margin               : 'margin' num [1:4] 0cm 0cm 0cm 0cm\n  ..- attr(*, \"unit\")= int 1\n $ legend.box.background           : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.box.spacing              : 'simpleUnit' num 11points\n  ..- attr(*, \"unit\")= int 8\n  [list output truncated]\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi TRUE\n - attr(*, \"validate\")= logi TRUE\n\n\n\n\nDo assumptions for CLT hold?\n\n\nIndependence: random sample!\nNormality condition: \\(n \\geq 30\\) in both groups with no particularly extreme outliers\n\n\n\nSet-up/find the following:\n\n\nPoint estimate\nStandard error\nCode for critical value"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#example-birth-term-cont.-1",
    "href": "slides/slides-18-ci-mean.html#example-birth-term-cont.-1",
    "title": "Confidence Intervals for Means",
    "section": "Example: birth term (cont.)",
    "text": "Example: birth term (cont.)\n\n\n\n\nPoint estimate: \\(\\bar{x}_{\\text{PD}} - \\bar{x}_{\\text{H}} = 0.32 - 0.16 = 0.158\\)\nSE \\(\\approx \\sqrt{\\frac{s_{\\text{PD}}^2}{n_{\\text{PD}}} + \\frac{s_{\\text{H}}^2}{n_{\\text{H}}}} = \\sqrt{\\frac{0.21^2}{147} + \\frac{0.06^2}{48}} = 0.019\\)\nCritical value:\n\n\\(df = \\min\\{n_{\\text{PD}} -1, n_{\\text{H}} -1 \\} = \\min\\{147 - 1, 48- 2\\} = 47\\)\nWant \\(0.975\\)-th percentile of \\(t_{47}\\) distribution: qt(0.975, df =47) = 2.01\n\n\n\n\nPutting everything together, our 95% CI for \\(\\mu_{s} - \\mu_{n}\\) is: \\[\n0.158 \\pm 2.01 \\times 0.019 = (0.12, 0.196)\n\\]\nAre average voice shimmers different between people with and without Parkinson’s? Briefly explain why or why not."
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#temp",
    "href": "slides/slides-18-ci-mean.html#temp",
    "title": "Confidence Intervals for a Mean",
    "section": "Temp",
    "text": "Temp\nIf the conditions hold, then our usual formula for \\(\\gamma \\times 100\\%\\) CI still holds:\n\\[\n\\text{point estimate} \\pm \\text{critical value} \\times \\text{SE}\n\\]\n\nPoint estimate\n\n\n\n\n\n\n\nIf \\(\\sigma_{1}\\) and \\(\\sigma_{2}\\) known:\n\n\n\\(\\text{SE} = \\sqrt{\\frac{\\sigma_{1}^2}{n_{1}} + \\frac{\\sigma_{2}^2}{n_{2}}}\\)\ncritical value: \\(z_{(1+\\gamma)/2}^*\\)\n\ni.e. \\((1+\\gamma)/2\\) percentile of \\(N(0,1)\\)\n\n\n\n\nIf \\(\\sigma_{1}\\) and \\(\\sigma_{2}\\) unknown:\n\n\n\\(\\text{SE} \\approx \\sqrt{\\frac{s_{1}^2}{n_{1}} + \\frac{s_{2}^2}{n_{2}}}\\)\ncritical value: \\(t_{df, (1+\\gamma)/2}^*\\)\n\ni.e. \\((1+\\gamma)/2\\) percentile of \\(t_{df}\\)\n\\(df = \\min\\{n_{1} -1, n_{2} - 1\\}\\)"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#ci-for-differences-in-two-means",
    "href": "slides/slides-18-ci-mean.html#ci-for-differences-in-two-means",
    "title": "Confidence Intervals for a Mean",
    "section": "CI for differences in two means",
    "text": "CI for differences in two means\nIf the conditions hold, then our usual formula for \\(\\gamma \\times 100\\%\\) CI still holds:\n\\[\n\\text{point estimate} \\pm \\text{critical value} \\times \\text{SE}\n\\]\n\nPoint estimate\n\n\n\n\n\n\n\nIf \\(\\sigma_{1}\\) and \\(\\sigma_{2}\\) known:\n\n\n\\(\\text{SE} = \\sqrt{\\frac{\\sigma_{1}^2}{n_{1}} + \\frac{\\sigma_{2}^2}{n_{2}}}\\)\ncritical value: \\(z_{(1+\\gamma)/2}^*\\)\n\n\\((1+\\gamma)/2\\) percentile of \\(N(0,1)\\)\n\n\n\n\nIf \\(\\sigma_{1}\\) and \\(\\sigma_{2}\\) unknown:\n\n\n\\(\\text{SE} \\approx \\sqrt{\\frac{s_{1}^2}{n_{1}} + \\frac{s_{2}^2}{n_{2}}}\\)\ncritical value: \\(t_{df, (1+\\gamma)/2}^*\\)\n\n\\((1+\\gamma)/2\\) percentile of \\(t_{df}\\)\n\\(df = \\min\\{n_{1} -1, n_{2} - 1\\}\\)"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#ci-for-difference-in-two-means",
    "href": "slides/slides-18-ci-mean.html#ci-for-difference-in-two-means",
    "title": "Confidence Intervals for Means",
    "section": "CI for difference in two means",
    "text": "CI for difference in two means\nIf the conditions hold, then our usual formula for \\(\\gamma \\times 100\\%\\) CI still holds:\n\\[\n\\text{point estimate} \\pm \\text{critical value} \\times \\text{SE}\n\\]\n\nPoint estimate\n\n\n\n\n\n\n\nIf \\(\\sigma_{1}\\) and \\(\\sigma_{2}\\) known:\n\n\n\\(\\text{SE} = \\sqrt{\\frac{\\sigma_{1}^2}{n_{1}} + \\frac{\\sigma_{2}^2}{n_{2}}}\\)\ncritical value: \\(z_{(1+\\gamma)/2}^*\\)\n\n\\((1+\\gamma)/2\\) percentile of \\(N(0,1)\\)\n\n\n\n\nIf \\(\\sigma_{1}\\) and \\(\\sigma_{2}\\) unknown:\n\n\n\\(\\text{SE} \\approx \\sqrt{\\frac{s_{1}^2}{n_{1}} + \\frac{s_{2}^2}{n_{2}}}\\)\ncritical value: \\(t_{df, (1+\\gamma)/2}^*\\)\n\n\\((1+\\gamma)/2\\) percentile of \\(t_{df}\\)\n\\(df = \\min\\{n_{1} -1, n_{2} - 1\\}\\)"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#example-voice-jitter",
    "href": "slides/slides-18-ci-mean.html#example-voice-jitter",
    "title": "Confidence Intervals for Means",
    "section": "Example: voice jitter",
    "text": "Example: voice jitter\nLet’s consider the voice shimmer of PD vs non-PD patients.\n\n\n\n\nConvince yourself that this data isn’t paired!\n\n\nPopulation 1: people with Parkinson’s Disease\nPopulation 2: people without Parkinson’s Disease\n\n\nResearch question: are average voice shimmers different between people with and without Parkinson’s? Create a 95% confidence interval to answer this question.\n\n\nWe care about the difference in means \\(\\mu_{\\text{PD}} - \\mu_{\\text{H}}\\)"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#example-voice-shimmer",
    "href": "slides/slides-18-ci-mean.html#example-voice-shimmer",
    "title": "Confidence Intervals for Means",
    "section": "Example: voice shimmer",
    "text": "Example: voice shimmer\nLet’s consider the voice shimmer of PD vs non-PD patients from last week’s homework.\n\n\n\n\nConvince yourself that this data isn’t paired!\n\n\nPopulation 1: people with Parkinson’s Disease\nPopulation 2: people without Parkinson’s Disease\n\n\nResearch question: are average voice shimmers different between people with and without Parkinson’s? Create a 95% confidence interval to answer this question.\n\n\nWe care about the difference in means \\(\\mu_{\\text{PD}} - \\mu_{\\text{H}}\\)"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#example-voice-shimmer-cont.",
    "href": "slides/slides-18-ci-mean.html#example-voice-shimmer-cont.",
    "title": "Confidence Intervals for Means",
    "section": "Example: voice shimmer (cont.)",
    "text": "Example: voice shimmer (cont.)\n\nAre average voice shimmers different between people with and without Parkinson’s? Create a 95% confidence interval to answer this question.\n\n\n\n\n\n\n\n\nstatus\nn\nxbar\ns\n\n\n\n\nHealthy\n48\n0.163\n0.058\n\n\nPD\n147\n0.321\n0.208\n\n\n\n\n\n\n\n\n\n\nDo assumptions for CLT hold?\n\n\nIndependence: random sample!\nNormality condition: \\(n \\geq 30\\) in both groups with no particularly extreme outliers\n\n\n\nSet-up/find the following:\n\n\nPoint estimate\nStandard error\nCode for critical value"
  },
  {
    "objectID": "slides/slides-18-ci-mean.html#example-voice-shimmer-cont.-1",
    "href": "slides/slides-18-ci-mean.html#example-voice-shimmer-cont.-1",
    "title": "Confidence Intervals for Means",
    "section": "Example: voice shimmer (cont.)",
    "text": "Example: voice shimmer (cont.)\n\n\n\n\nPoint estimate: \\(\\bar{x}_{\\text{PD}} - \\bar{x}_{\\text{H}} = 0.32 - 0.16 = 0.158\\)\nSE \\(\\approx \\sqrt{\\frac{s_{\\text{PD}}^2}{n_{\\text{PD}}} + \\frac{s_{\\text{H}}^2}{n_{\\text{H}}}} = \\sqrt{\\frac{0.21^2}{147} + \\frac{0.06^2}{48}} = 0.019\\)\nCritical value:\n\n\\(df = \\min\\{n_{\\text{PD}} -1, n_{\\text{H}} -1 \\} = \\min\\{147 - 1, 48- 1\\} = 47\\)\nWant \\(0.975\\)-th percentile of \\(t_{47}\\) distribution: qt(0.975, df =47) = 2.01\n\n\n\n\nPutting everything together, our 95% CI for \\(\\mu_{\\text{PD}} - \\mu_{\\text{H}}\\) is: \\[\n0.158 \\pm 2.01 \\times 0.019 = (0.12, 0.196)\n\\]\n\nInterpret this CI in context. Note: direction of difference matters!\nAre average voice shimmers different between people with and without Parkinson’s? Briefly explain why or why not."
  },
  {
    "objectID": "live_code/ht_diff_means.html",
    "href": "live_code/ht_diff_means.html",
    "title": "HT: Difference in means",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readr)\nurl_file <- \"https://raw.githubusercontent.com/midd-stat201-fall2024/midd-stat201-fall2024.github.io/refs/heads/main/data/parkinsons.csv\"\nparkinsons <- read_csv(url_file)\n\n\nGet summary statistics\n\nx_pd <- parkinsons |>\n  filter(status == \"PD\") |>\n  pull(shimmer)\nx_healthy <- parkinsons |>\n  filter(status == \"Healthy\") |>\n  pull(shimmer)\nn1 <- length(x_pd)\nn2 <- length(x_healthy)\nxbar1 <- mean(x_pd)\nxbar2 <- mean(x_healthy)\ns1 <- sd(x_pd)\ns2 <- sd(x_healthy)\n\n\n\nObtain quantities for CI\n\npoint_est <- xbar1 - xbar2\nSE <- sqrt(s1^2/n1 + s2^2/n2)\ndf <- min(n1-1, n2-1)\ncv <- qt(0.975, df = df)\n\nlower <- point_est - cv * SE\nupper <- point_est + cv * SE\n\nOur 95% CI for the difference in voice shimmers (PD - non PD) is (0.12 , 0.197)."
  },
  {
    "objectID": "live_code/ht_diff_means.html#get-summary-statistics",
    "href": "live_code/ht_diff_means.html#get-summary-statistics",
    "title": "HT: Difference in means",
    "section": "Get summary statistics",
    "text": "Get summary statistics\n\nx_pd <- parkinsons |>\n  filter(status == \"PD\") |>\n  pull(shimmer)\nx_healthy <- parkinsons |>\n  filter(status == \"Healthy\") |>\n  pull(shimmer)\nn1 <- length(x_pd)\nn2 <- length(x_healthy)\nxbar1 <- mean(x_pd)\nxbar2 <- mean(x_healthy)\ns1 <- sd(x_pd)\ns2 <- sd(x_healthy)"
  },
  {
    "objectID": "live_code/ht_diff_means.html#obtain-quantities-for-ci",
    "href": "live_code/ht_diff_means.html#obtain-quantities-for-ci",
    "title": "HT: Difference in means",
    "section": "Obtain quantities for CI",
    "text": "Obtain quantities for CI\n\npoint_est <- xbar1 - xbar2\npoint_est\n\n[1] 0.1582457\n\nSE <- sqrt( s1^2/n1 + s2^2/n2)\nSE\n\n[1] 0.01906297\n\ndf <- min(n1-1, n2-1)\ndf\n\n[1] 47\n\ncv <- qt(0.975, df = df)\ncv\n\n[1] 2.011741\n\nlower <- point_est - cv * SE\nupper <- point_est + cv * SE\n\nOur 95% CI for the different in voice shimmers (PD - non PD) is (0.12 , 0.197)."
  },
  {
    "objectID": "project/project_description.html",
    "href": "project/project_description.html",
    "title": "Final project",
    "section": "",
    "text": "Deliverable\nDate\nTime\n\n\n\n\nGroup creation\nWednesday 10/30\nBeginning of class\n\n\nProposal for data collection\nMonday 11/04\n11:59pm\n\n\nRevised proposal for data collection (if applicable)\nSaturday 11/09\n11:59pm\n\n\nCollect your data\nAnytime between proposal approval and Thanksgiving break\n\n\n\nExcel/GoogleSheets of your data\nSunday 12/01\n11:59pm\n\n\nRough draft of report\nSunday 12/08\n11:59pm\n\n\nFinal Presentation\n(Section AZ)\nWednesday 12/11\n7-10pm (slides due at 6pm)\n\n\nFinal Presentation (Section BY)\nThursday 12/12\n9am-12pm (slides due at 8am)\n\n\nFinal report (knitted version and .Rmd) and data with data dictionary\nSunday 12/15 (tentative)\n11:59pm\n\n\nReflection*\nSunday 12/15 (tentative)\n11:59pm\n\n\n\n*Reflection should be submitted individually. All other deliverables are one-per-group."
  },
  {
    "objectID": "project/project_description.html#description",
    "href": "project/project_description.html#description",
    "title": "Final project",
    "section": "Description",
    "text": "Description\nAs a final project, you will work in groups to collect and analyze data from fellow Middlebury students! The final project will give you a chance to demonstrate and celebrate your mastery over the the statistics and data science concepts you have learned this entire semester! Your group will come up with a list of research questions that you will subsequently answer using appropriate statistical methods.\nIn particular, your group will:\n\ncome up with a list of inferential research questions you are interested in answering\ndetermine your target population, create a list of variables you’d like to observe, and devise an appropriate sampling strategy\nhave a finalized data set that anyone can access and use, accompanied by a “data dictionary”\npresent your project to the class (and perhaps other professors/TAs)\nsubmit a well-written, professional report\nsubmit a reflection on the project, which will include a rubric for how much each team member contributed\n\nYou should have fun and be creative with this project!! Your research questions can be serious or funny (so long as they are PG-13 and below!)\n\nWeek 12 of the semester will be mostly devoted to project work time."
  },
  {
    "objectID": "project/project_description.html#requirements",
    "href": "project/project_description.html#requirements",
    "title": "Final project",
    "section": "Requirements",
    "text": "Requirements\nThe following looks like a lot, but is meant to provide you structure for strong final project that you are proud of!\n\n\nGroup creation\n\n\nEach group will ideally contain three people.\nBy the due date listed in the table above, you should either:\n\nform a group of three on your own and then e-mail Prof. Tang your decision, cc-ing everyone in the group\nform a group of two and then e-mail Prof. Tang your decision, cc-ing the other person in the group.\n\nYour group of two may be randomly paired with another person or pair\n\ne-mail Prof. Tang if you don’t have a preference and/or want to meet new people and would like to be randomly placed in a group\n\n\n\nProposal for data collection\n\n\nBy the due date listed in the table above, your group should submit to Canvas a document detailing the following:\n\nYour target population(s) of interest\nA list of research questions your group is interested in exploring/answering. You should have at least one question per group member, but more is better!\n\nNote: we have not learned about regression yet. But we will soon learn linear regression, which answers questions about how changes in variable \\(x\\) might lead to changes in continuous variable \\(y\\)\n\nA list of variables you will observe/collect from each observational unit that you may be useful and/or required for answering your reseach question(s).\n\nYou should plan to collect at least four numerical variables (at least two of which are continuous), and at least two categorical variables\n\nA detailed description of how you plan to collect your data (i.e. survey students)\n\nHow many people do you plan to include in your data? (You should aim for least 30 people, if possible!)\nHow will you have people submit their data? (e.g. fill out a paper form, GoogleForm)\nIf you want to conduct an observational study: will you implement SRS, stratified, clustering, etc? How exactly will you implement this sampling method?\nIf you want to conduct an experiment: will you randomly recruit people to your study? How will you randomly assign treatments? (Make sure your experiment is ethical!!)\n\nA rough description of the statistical methods you might use to answer each question you listed in (2).\n\nThis might be along the lines of “hypothesis test for a single mean” or “confidence interval for a proportion” or “linear regression” along with a one-two sentence justification for the method is appropriate.\nIt is possible that we haven’t yet learned a method to answer some of your questions. If so, please indicate this on your proposal along with a brief justification for why you think what we have currently learned doesn’t apply.\n\n\n\nOnce you have submitted your proposal, Prof. Tang will give you feedback. Based on the feedback, your group will either have to submit a revised proposal, or you will be given the okay to go forth and collect your data.\n\n\n\nData set\n\n\nOnce you have collected your data, you should compile it all into a single Excel or GoogleSheets. Please give each variable an informative one-word name. This should be submitted to Canvas by the end of Thanksgiving break."
  },
  {
    "objectID": "homework/hw6_r.html",
    "href": "homework/hw6_r.html",
    "title": "STAT 201: Problem Set 6 (R)",
    "section": "",
    "text": "library(tidyverse)\nlibrary(openintro)\nknitr::opts_chunk$set(fig.width=6, fig.height=3)\nWe will use the possum data from the openintro library. Take a look at the Help file and the data before proceeding. For this homework, assume the observations were collected via a random sample.\nAlso, your code will continue to be graded for reproducibility and “clean code”. If you make figures, make sure to have informative axis labels."
  },
  {
    "objectID": "homework/hw6_r.html#part-1",
    "href": "homework/hw6_r.html#part-1",
    "title": "STAT 201: Problem Set 6 (R)",
    "section": "Part 1",
    "text": "Part 1\nWe will obtain a 95% confidence interval for the average total length of possums from Victoria.\n\nAre the conditions for inference satisfied? Explain. You may (and should!) write code in the following R chunk to help you answer this question.\n\n\n\n\nAnswer:\n\nRegardless of how you answered in (1), obtain a 95% confidence interval for the average total length of possums from Victoria. Interpret your confidence interval in context.\n\nBe as reproducible as possible, and report your answer using in-line code.\n\n# start by creating vector(s) representing the data of interest for this problem\n\n# now obtain necessary quantities \n\nAnswer:"
  },
  {
    "objectID": "homework/hw6_r.html#part-2",
    "href": "homework/hw6_r.html#part-2",
    "title": "STAT 201: Problem Set 6 (R)",
    "section": "Part 2",
    "text": "Part 2\nIs the average tail length of possums 5 years old or younger in Victoria different from the average tail length of possums 5 years old or younger in New South Wales or Queensland? We will obtain a confidence interval to answer this question.\n\nAre the conditions for inference satisfied? Explain. You may (and should!) write code in the following R chunk to help you answer this question. (Hint: does making a faceted plot sound interesting to you?)\n\n\n\n\nAnswer:\n\nRegardless of how you answered in (1), obtain a 90% confidence interval appropriate for the research question. Be as reproducible as possible, and report your answer using in-line code.\n\n\n# start by creating vector(s) representing the data of interest for this problem\n\n# now obtain necessary quantities \n\nAnswer:\n\nInterpret the confidence interval in context and answer the research question using your interval as support.\n\nAnswer:"
  },
  {
    "objectID": "project/project_description.html#deadlines",
    "href": "project/project_description.html#deadlines",
    "title": "Final project",
    "section": "Deadlines",
    "text": "Deadlines\n\n\n\nDeliverable\nDate\nTime\n\n\n\n\nGroup creation\nWednesday 10/30\nBeginning of class\n\n\nProposal for data collection\nMonday 11/04\n11:59pm\n\n\nRevised proposal for data collection (if applicable)\nSaturday 11/09\n11:59pm\n\n\nCollect your data\nAnytime between proposal approval and Thanksgiving break\n\n\n\nExcel/GoogleSheets of your data\nSunday 12/01\n11:59pm\n\n\nRough draft of report\nSunday 12/08\n11:59pm\n\n\nFinal presentations (Section AZ)\nWednesday 12/11\n7-10pm (slides due at 6pm)\n\n\nFinal presentations (Section BY)\nThursday 12/12\n9am-12pm (slides due at 8am)\n\n\nFinal report (knitted version and .Rmd) and data with data dictionary\nSunday 12/15 (tentative)\n11:59pm\n\n\nReflection*\nSudday 12/15 (tentative)\n11:59pm\n\n\n\n*Reflection should be submitted individually. All other deliverables are one-per-group."
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#housekeeping",
    "href": "slides/slides-19-ht-prop.html#housekeeping",
    "title": "Hypothesis testing with CLT",
    "section": "Housekeeping",
    "text": "Housekeeping\n\n\n\n\nHomework 6 due tonight\nModified office hours"
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#recap",
    "href": "slides/slides-19-ht-prop.html#recap",
    "title": "Hypothesis testing with CLT",
    "section": "Recap",
    "text": "Recap\n\nCLT -> sampling distribution for sample means -> confidence intervals for populations means\nNow we’re returning to hypothesis testing!\n\nTwo sets of hypotheses (competing claims)\nCollect data, calculate a statistic from the observed data, set significance level\nObtain p-value from the null distribution: sampling distribution assuming if \\(H_{0}\\) were true\n\np-value: probability of observing data as or more extreme as our own, assuming \\(H_{0}\\) true\n\nMake a decision"
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#hypothesis-testing-using-mathematical-model",
    "href": "slides/slides-19-ht-prop.html#hypothesis-testing-using-mathematical-model",
    "title": "Hypothesis testing with CLT",
    "section": "Hypothesis testing using mathematical model",
    "text": "Hypothesis testing using mathematical model\n\nWe learned how to conduct hypothesis tests (HTs) using simulation to obtain null distribution\nBut we can also use CLT to obtain null distribution!\nSo the only step that will “look different” is #3: how we obtain our null distribution and p-value\n\nLooks different depending on type of data\n\nMake a conclusion in terms of \\(H_{A}\\)"
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#define-hypotheses",
    "href": "slides/slides-19-ht-prop.html#define-hypotheses",
    "title": "Hypothesis testing with CLT",
    "section": "1. Define hypotheses",
    "text": "1. Define hypotheses\nWant to conduct a hypothesis test about a population proportion.\n\n\n\n\\(H_{0}: p = p_{0}\\)\n\\(H_{A}: p \\neq p_{0}\\) or \\(H_{A}: p > p_{0}\\) or \\(H_{A}: p < p_{0}\\)\n\n\n\n\\(H_{0}: p \\geq p_{0}\\)\n\\(H_{A}: p < p_{0}\\)\n\n\n\n\\(H_{0}: p \\leq p_{0}\\)\n\\(H_{A}: p > p_{0}\\)\n\n\n\n\nRemember, \\(p_{0}\\) is our “null hypothesized value”: the population proportion if \\(H_{0}\\) were true"
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#collect-data-set-significance",
    "href": "slides/slides-19-ht-prop.html#collect-data-set-significance",
    "title": "Hypothesis testing with CLT",
    "section": "2. Collect data, set significance",
    "text": "2. Collect data, set significance\n\nObtain observed sample proportion \\(\\hat{p}_{obs}\\)\nSet \\(\\alpha\\) significance level"
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#null-distribution-and-p-value",
    "href": "slides/slides-19-ht-prop.html#null-distribution-and-p-value",
    "title": "Hypothesis testing with CLT",
    "section": "3. Null distribution and p-value",
    "text": "3. Null distribution and p-value\nRecall CLT for sample proportion: if we have \\(n\\) independent binary observations that satisfy the success-failure condition, then\n\\[\n\\hat{p} \\overset{\\cdot}{\\sim} N\\left(p, \\sqrt{\\frac{p(1-p)}{n}} \\right)\n\\]\n\nThis is the sampling distribution of \\(\\hat{p}\\)\nBut we want the null distribution of \\(\\hat{p}\\): the sampling distribution under \\(H_{0}\\)\nWe should operate in a world where \\(H_{0}\\) is true, which means we operate assuming \\(p =p_{0}\\)\nSo to use CLT, we must satisfy:\n\nIndependence\nSuccess-failure condition under \\(H_{0}\\): \\(np_{0} \\geq 10\\) and \\(n(1-p_{0}) \\geq 10\\)"
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#null-distribution-and-p-value-cont.",
    "href": "slides/slides-19-ht-prop.html#null-distribution-and-p-value-cont.",
    "title": "Hypothesis testing with CLT",
    "section": "3. Null distribution and p-value (cont.)",
    "text": "3. Null distribution and p-value (cont.)\nIf CLT holds and \\(H_{0}\\) is true, then our null distribution is:\n\\[\n\\hat{p} \\overset{\\cdot}{\\sim} N\\left(p_{0}, \\sqrt{\\frac{p_{0} (1-p_{0})}{n}} \\right)\n\\]\n\nWe can standardize the null distribution by taking z-score:\n\n\n\\[\nZ = \\frac{\\hat{p} - p_{0}}{\\sqrt{\\frac{p_{0} (1-p_{0})}{n}}} \\sim N(0,1)\n\\]"
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#null-distribution-and-p-value-cont.-1",
    "href": "slides/slides-19-ht-prop.html#null-distribution-and-p-value-cont.-1",
    "title": "Hypothesis testing with CLT",
    "section": "3. Null distribution and p-value (cont.)",
    "text": "3. Null distribution and p-value (cont.)\n\np-value requires us to compare our observed data to the null distribution\nCalculate a test statistic: a quantity that assesses how consistent your sample data are with \\(H_{0}\\)\n\nOur test statistic here is:\n\n\n\\[z =\\frac{\\hat{p}_{\\text{obs}} - p_{0}}{\\sqrt{\\frac{p_{0} (1-p_{0})}{n}}}\\]\n\n\nIf \\(|z|\\) large, then that usually means \\(\\hat{p}\\) is extremely unusual for \\(H_{0}\\), which is convincing evidence against \\(H_{0}\\)\n\np-value is then \\(\\text{Pr}(Z \\geq z)\\) or \\(\\text{Pr}(Z \\leq z)\\) (or both), depending on \\(H_{A}\\)\n\nEasily obtained using pnorm()"
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#example-taste-test",
    "href": "slides/slides-19-ht-prop.html#example-taste-test",
    "title": "Hypothesis testing with CLT",
    "section": "Example: taste test",
    "text": "Example: taste test\nSome people claim that they can tell the difference between a diet soda and a regular soda in the first sip. A researcher wanted to test this claim using a hypothesis test at the 0.05 significance level.\n\nHe randomly sampled 80 such people.\nHe then filled 80 plain white cups with soda, half diet and half regular through random assignment, and asked each person to take one sip from their cup and identify the soda as diet or regular.\n53 participants correctly identified the soda.\n\n\nLet \\(p\\) be the probability/rate of correctly identifying soda type among people who think they can tell the difference."
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#example-taste-test-cont.",
    "href": "slides/slides-19-ht-prop.html#example-taste-test-cont.",
    "title": "Hypothesis testing with CLT",
    "section": "Example: taste test (cont.)",
    "text": "Example: taste test (cont.)\n\nDefine hypotheses\n\n\\(H_{0}\\): \\(p = 0.5\\) (random guessing)\n\\(H_{A}\\): \\(p > 0.5\\) (better than random guessing)\nNote: \\(p_{0} = 0.5\\) is our null hypothesized value!\n\nCollect data\n\n\\(\\hat{p}_{\\text{obs}} = \\frac{53}{80} = 0.6625\\)\n\n\n\nNote: significance level already determined to be 0.05"
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#make-decision-and-conclude",
    "href": "slides/slides-19-ht-prop.html#make-decision-and-conclude",
    "title": "Hypothesis testing with CLT",
    "section": "4. Make decision and conclude",
    "text": "4. Make decision and conclude\nSince our p-value of 0.0019 is less than our significance level of 0.05, we reject \\(H_{0}\\). The data provide strong evidence that the rate of correctly identifying a soda for these people is better than random guessing."
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#example-taste-test-cont.-1",
    "href": "slides/slides-19-ht-prop.html#example-taste-test-cont.-1",
    "title": "Hypothesis testing with CLT",
    "section": "Example: taste test (cont.)",
    "text": "Example: taste test (cont.)\n\nObtain null distribution and p-value\n\n\nCheck conditions for inference satisfied\n\n\nIndependence: random sample\nsuccess-failure: \\(np_{0} = 80(0.5) = 40 \\geq 10\\) and \\(n(1-p_{0}) = 40 \\geq 10\\)\n\n\nNull distribution\n\n\n\\[\\hat{p} \\overset{\\cdot}{\\sim} N\\left(0.5, \\sqrt{\\frac{0.5(1-0.5)}{80}} = 0.056 \\right)\\]\n\n\nTest statistic:\n\n\n\\[z = \\frac{\\hat{p}_{obs} - p_{0}}{\\text{SE}_{0}} = \\frac{0.6625 - 0.5}{0.056} = 2.90\\]\n\ni.e. if \\(H_{0}\\) true, our observed \\(\\hat{p}_{obs}\\) is 2.90 SDs above the mean"
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#p-value-derivation-extra",
    "href": "slides/slides-19-ht-prop.html#p-value-derivation-extra",
    "title": "Hypothesis testing with CLT",
    "section": "p-value derivation (extra)",
    "text": "p-value derivation (extra)"
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#example-mms",
    "href": "slides/slides-19-ht-prop.html#example-mms",
    "title": "Hypothesis testing with CLT",
    "section": "Example: M&M’s",
    "text": "Example: M&M’s\nM&M’s reported that 14% of its candies are yellow. We are interested in testing this claim. In a random sample of 100 M&M’s, 9 were found to be yellow. Conduct a hypothesis test at the \\(0.10\\) level.\n\\(p =\\) true proportion of yellow M&M’s\n\n\n\n\nWrite out null and alternative hypotheses\nCollect data (i.e. obtain our observed statistics)\ni) Verify conditions for CLT are met\n\n\n\n\n\\(H_{0}: p = 0.14\\) versus \\(H_{A}: p \\neq 0.14\\)\n\\(\\hat{p}_{obs} = \\frac{9}{100} = 0.09\\)\ni) Independence: random sample\nSuccess-failure: \\(np_{0} = 100(0.14) = 14 \\geq 10\\) and \\(n(1-p_{0}) = 86 \\geq 10\\)"
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#example-mms-cont.",
    "href": "slides/slides-19-ht-prop.html#example-mms-cont.",
    "title": "Hypothesis testing with CLT",
    "section": "Example: M&M’s (cont.)",
    "text": "Example: M&M’s (cont.)\n\n\n\n\nii) Obtain null distribution\niii) Obtain test statistic \\(z\\)\n\n\n\n\nii) By CLT, our null distribution is \\(\\hat{p} \\overset{\\cdot}{\\sim} N\\left(0.14, \\sqrt{\\frac{0.14(1-0.14)}{100}} \\right)\\)\n\\(\\quad =N(0.14, 0.035)\\)\n\niii) \\(z = \\frac{\\hat{p}_{obs} - p_{0}}{\\text{SE}_{0}} = \\frac{0.09 - 0.14}{0.035} = -1.43\\)\n\n\n\n\n\n\n\n\niv) Obtain p-value. Write out in \\(\\text{Pr}()\\) notation or in code what we want to find. Drawing a picture may help!\n\n\n\n\niv) Since \\(H_{A}\\) is two-sided, we want \\[\\begin{align*}\n   \\text{p-value} &= \\text{Pr}(Z \\leq -1.43 \\cup Z \\geq 1.43) \\\\\n&= \\text{Pr}(Z \\leq -1.43) + \\text{Pr}(Z \\geq 1.43)  \\\\\n&= 2\\times \\text{Pr}(Z \\geq 1.43) \\\\\n&= \\texttt{2 * (1 - pnorm(1.43))} \\\\\n&= 0.153\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#example-mms-cont.-1",
    "href": "slides/slides-19-ht-prop.html#example-mms-cont.-1",
    "title": "Hypothesis testing with CLT",
    "section": "Example: M&M’s (cont.)",
    "text": "Example: M&M’s (cont.)\n\n\nMake a decision and conclusion in context.\n\n\nSince our p-value of 0.153 is greater than our significance level of 0.10, we fail to reject \\(H_{0}\\). The data are not strong enough to suggest that the true proportion of yellow M&Ms is different from 14%."
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#example-taste-test-cont.-2",
    "href": "slides/slides-19-ht-prop.html#example-taste-test-cont.-2",
    "title": "Hypothesis testing with CLT",
    "section": "4. Example: taste test (cont.)",
    "text": "4. Example: taste test (cont.)\n\n\nCalculate p-value\n\n\nRemember \\(H_{A}: p > 0.5\\)\n\n\n\n\\[\n\\text{p-value} = \\text{Pr}(Z \\geq z) = \\text{Pr}(Z \\geq 2.90) = \\texttt{1 - pnorm(2.90, 0, 1) = 0.0019}\n\\]\n\n\nDecision and conclusion\n\nSince our p-value of 0.0019 is less than our significance level of 0.05, we reject \\(H_{0}\\). The data provide strong evidence that the rate of correctly identifying a soda for these people is better than random guessing."
  },
  {
    "objectID": "practice_probs/practice-19-ht-prop.html",
    "href": "practice_probs/practice-19-ht-prop.html",
    "title": "HT for single proportion",
    "section": "",
    "text": "A recent poll found that 11% of US adults say they have smoked cigarettes in the past week, a historical low. In a random sample of 730 randomly selected students at four-year colleges, it was found that 66 students have smoked cigarettes in the past week. Test that claim that the smoking rate of students at four-year colleges is the same the national US adult average at the 0.05 significance level.\nAn apple farmer has historically lost an average of 4% of his trees each year. He believes that he has been losing more trees lately.\n\nIn a sample of 300 trees, 20 have died. Test the farmer’s claim at the 0.01 level.\nHow would the situation change if the farmer’s sample size had been 200 instead of 300?\n\n\n\n\n\n\n\\((^*)\\) A survey was conducted on 850 randomly sampled student loan borrowers, asking them if they owed more than $30,000 in student debt. A 95% confidence interval for the true proportion of student loan borrowers with at $30,000 of debt was found to be (0.135, 0.185).\n\nBased on this information, out of the 850 respondents, how many answered “yes” to the question? Justify your answer.\nUse hypothesis testing to examine the claim that fewer than 18.2% of all student loan borrowers owe at least $30,000 at the 0.05 significance level."
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#difference-of-two-proportions",
    "href": "slides/slides-19-ht-prop.html#difference-of-two-proportions",
    "title": "Hypothesis testing with CLT",
    "section": "Difference of two proportions",
    "text": "Difference of two proportions\nSuppose we have two populations 1 and 2, and want to conduct a hypothesis test for the difference in population proportions: \\(p_{1} - p_{2}\\)\n\nWe have samples of size \\(n_{1}\\) and \\(n_{2}\\)\nReasonable point estimate: \\(\\hat{p}_{1, obs} - \\hat{p}_{2,obs}\\)\nApply same process as in previous test for a single proportion, this time working with the sampling distribution of the difference of two sample proportions"
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#sampling-dist.-of-difference-of-two-proportions",
    "href": "slides/slides-19-ht-prop.html#sampling-dist.-of-difference-of-two-proportions",
    "title": "Hypothesis testing with CLT",
    "section": "Sampling dist. of difference of two proportions",
    "text": "Sampling dist. of difference of two proportions\n\nIn order to use CLT approximation, we have to ensure conditions are met:\n\nIndependence (extended): data are independent within and between groups\nSuccess-failure (extended): success-failure conditions holds for both groups (must perform four total checks)\n\nIf above hold, then:\n\n\n\\[\n\\hat{p}_{1} - \\hat{p}_{2} \\overset{\\cdot}{\\sim} N\\left(p_{1} - p_{2}, \\sqrt{\\frac{p_{1} (1-p_{1})}{n_{1}} + \\frac{p_{2} (1-p_{2})}{n_{2}}} \\right)\n\\]\nwhere \\(p_{1}\\) and \\(p_{2}\\) are the population proportions"
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#hypothesis-test-for-difference-in-proportions",
    "href": "slides/slides-19-ht-prop.html#hypothesis-test-for-difference-in-proportions",
    "title": "Hypothesis testing with CLT",
    "section": "Hypothesis test for difference in proportions",
    "text": "Hypothesis test for difference in proportions\n\nDefine hypotheses. Hypothesis tests for difference in proportions in this class will take the form:\n\n\n\\[\n\\begin{align*}\nH_{0}: \\ &p_{1} = p_{2} \\qquad \\Rightarrow \\qquad  H_{0}: p_{1} - p_{2} = 0 \\\\\nH_{A}: \\ &p_{1} \\neq p_{2} \\qquad \\Rightarrow \\qquad  H_{A}: p_{1} - p_{2} \\neq 0 \\\\\n\\text{ or }\\ &p_{1} < p_{2}  \\qquad \\Rightarrow \\qquad   \\qquad \\  p_{1} - p_{2} < 0 \\\\\n\\text{ or }\\ &p_{1} > p_{2} \\qquad \\Rightarrow \\qquad   \\qquad \\  p_{1} - p_{2} > 0\n\\end{align*}\n\\]\n\n\nCollect data/summarise (i.e. obtain \\(\\hat{p}_{1,obs}\\) and \\(\\hat{p}_{2,obs}\\))"
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#pooled-proportion",
    "href": "slides/slides-19-ht-prop.html#pooled-proportion",
    "title": "Hypothesis testing with CLT",
    "section": "Pooled proportion",
    "text": "Pooled proportion\n\nTo obtain null distribution, we would need to know \\(p_{1}\\) and \\(p_{2}\\) to verify success-failure conditions\nWe obviously don’t have these values, so maybe use \\(\\hat{p}_{1,obs}\\) and \\(\\hat{p}_{2,obs}\\)?\nBut wait! If \\(H_{0}: p_{1} = p_{2}\\), then \\(\\hat{p}_{1,obs}\\) and \\(\\hat{p}_{2,obs}\\) in theory come from the same population\n\nSo under this null hypothesis, we use a special proportion called the pooled proportion to check the success-failure conditions:\n\n\n\n\\[\n\\hat{p}_{pooled} = \\frac{\\text{total # of successes from both samples}}{\\text{combined sample size}} = \\frac{n_{1} \\hat{p}_{1,obs} + n_{2} \\hat{p}_{2,obs}}{n_{1} + n_{2}}\n\\]\n\n\nThis is the best estimate of both \\(p_{1}\\) and \\(p_{2}\\) if null hypothesis of \\(p_{1} = p_{2}\\) is true!"
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#hypothesis-test-cont.",
    "href": "slides/slides-19-ht-prop.html#hypothesis-test-cont.",
    "title": "Hypothesis testing with CLT",
    "section": "Hypothesis test (cont.)",
    "text": "Hypothesis test (cont.)\n\nObtain null distribution (first verify conditions for inference using \\(\\hat{p}_{pooled}\\))\n\nIf conditions satisfied, then we have the “general” sampling distribution of \\(\\hat{p}_{1} - \\hat{p}_{2}\\) from previous slide.\nBut to obtain the null distribution, we assume \\(H_{0}: p_{1} - p_{2} = 0\\) is true, and will estimate \\(p_{1}\\) and \\(p_{2}\\) using \\(\\hat{p}_{pooled}\\) to approximate standard error:\n\n\n\n\\[\n\\begin{align*}\n\\hat{p}_{1} - \\hat{p}_{2} &\\overset{\\cdot}{\\sim} N\\left(p_{1} - p_{2}, \\sqrt{\\frac{p_{1} (1-p_{1})}{n_{1}} + \\frac{p_{2} (1-p_{2})}{n_{2}}}  \\right) \\qquad \\text{(CLT)} \\\\ &\\overset{\\cdot}{\\sim} N\\big(0, \\underbrace{\\sqrt{\\frac{\\hat{p}_{pooled}(1 - \\hat{p}_{pooled})}{n_{1}} + \\frac{\\hat{p}_{pooled}(1 - \\hat{p}_{pooled})}{n_{2}}}}_{\\widehat{\\text{SE}}_{0}} \\big) \\qquad (H_{0})\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#hypothesis-test-cont.-1",
    "href": "slides/slides-19-ht-prop.html#hypothesis-test-cont.-1",
    "title": "Hypothesis testing with CLT",
    "section": "Hypothesis test (cont.)",
    "text": "Hypothesis test (cont.)\nObtain test-statistic:\n\\[\nz = \\frac{\\text{point estimate} - \\text{null value}}{\\text{SE}} \\approx \\frac{(\\hat{p}_{1,obs} - \\hat{p}_{2,obs}) - 0}{\\widehat{\\text{SE}}_{0}}\n\\]\n\nTo obtain p-value, we want \\(\\text{Pr}(Z \\geq z)\\) and/or \\(\\text{Pr}(Z \\leq z)\\) where \\(Z \\sim N(0,1)\\)\n\nObtain using pnorm(z, 0, 1)"
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#test-statistic-and-p-value",
    "href": "slides/slides-19-ht-prop.html#test-statistic-and-p-value",
    "title": "Hypothesis testing with CLT",
    "section": "3. Test statistic and p-value",
    "text": "3. Test statistic and p-value\n\np-value requires us to compare our observed data to the null distribution\nCalculate a test statistic: a quantity that assesses how consistent your sample data are with \\(H_{0}\\)\n\nOur test statistic is of the form:\n\n\n\\[z =\\frac{\\text{point estimate} - \\text{null value}}{\\text{SE}}\\]\n\n\nFor this specific test:\n\n\n\\[z =\\frac{\\hat{p}_{\\text{obs}} - p_{0}}{\\sqrt{\\frac{p_{0} (1-p_{0})}{n}}}\\]\n\n\nIf \\(|z|\\) large, then that usually means \\(\\hat{p}\\) is extremely unusual for \\(H_{0}\\), which is convincing evidence against \\(H_{0}\\)\n\np-value is then \\(\\text{Pr}(Z \\geq z)\\) or \\(\\text{Pr}(Z \\leq z)\\) (or both), depending on \\(H_{A}\\)\n\nEasily obtained using pnorm()"
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#test-statistic",
    "href": "slides/slides-19-ht-prop.html#test-statistic",
    "title": "Hypothesis testing with CLT",
    "section": "3. Test statistic",
    "text": "3. Test statistic\n\np-value requires us to compare our observed data to the null distribution\nCalculate a test statistic: a quantity that assesses how consistent your sample data are with \\(H_{0}\\)\n\nOur test statistic is of the form:\n\n\n\\[\\frac{\\text{point estimate} - \\text{null value}}{\\text{SE}}\\]\n\n\nFor this specific test, our test statistic is:\n\n\n\\[z =\\frac{\\hat{p}_{\\text{obs}} - p_{0}}{\\sqrt{\\frac{p_{0} (1-p_{0})}{n}}}\\]\nwhich is distributed \\(N(0,1)\\)"
  },
  {
    "objectID": "slides/slides-19-ht-prop.html#obtain-p-value",
    "href": "slides/slides-19-ht-prop.html#obtain-p-value",
    "title": "Hypothesis testing with CLT",
    "section": "Obtain p-value",
    "text": "Obtain p-value\n\nIf \\(|z|\\) large, then that usually means observed value is extremely unusual for \\(H_{0}\\), which is convincing evidence against \\(H_{0}\\)\np-value is then \\(\\text{Pr}(Z \\geq z)\\) or \\(\\text{Pr}(Z \\leq z)\\) (or both), depending on \\(H_{A}\\)\n\nEasily obtained using pnorm()"
  },
  {
    "objectID": "slides/slides-20-ht-diff-prop.html#housekeeping",
    "href": "slides/slides-20-ht-diff-prop.html#housekeeping",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Housekeeping",
    "text": "Housekeeping\n\n\n\n\nNo office hours Friday"
  },
  {
    "objectID": "slides/slides-20-ht-diff-prop.html#recap",
    "href": "slides/slides-20-ht-diff-prop.html#recap",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Recap",
    "text": "Recap\n\nHypothesis test for single proportion: \\(H_{0}: p = p_{0}\\) vs \\(H_{A}: p \\neq p_{0}\\) (or \\(>, <\\))\n\nNull distribution (assuming CLT holds under \\(H_{0}\\)):\n\\[\n\\hat{p} \\overset{\\cdot}{\\sim}N\\left(p_{0}, \\sqrt{\\frac{p_{0} (1-p_{0})}{n}} \\right)\n\\]\nObtain test statistic and calculate p-value\n\\[\nz = \\frac{\\hat{p}_{obs} - p_{0}}{\\text{SE}_{0}} \\sim N(0,1)\n\\]"
  },
  {
    "objectID": "slides/slides-20-ht-diff-prop.html#difference-of-two-proportions",
    "href": "slides/slides-20-ht-diff-prop.html#difference-of-two-proportions",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Difference of two proportions",
    "text": "Difference of two proportions\nSuppose we have two populations 1 and 2, and want to conduct a hypothesis test for the difference in population proportions: \\(p_{1} - p_{2}\\)\n\nWe have samples of size \\(n_{1}\\) and \\(n_{2}\\)\nReasonable point estimate: \\(\\hat{p}_{1, obs} - \\hat{p}_{2,obs}\\)\nWe will now work with the sampling distribution of the difference of two sample proportions"
  },
  {
    "objectID": "slides/slides-20-ht-diff-prop.html#sampling-dist.-of-difference-of-two-proportions",
    "href": "slides/slides-20-ht-diff-prop.html#sampling-dist.-of-difference-of-two-proportions",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Sampling dist. of difference of two proportions",
    "text": "Sampling dist. of difference of two proportions\n\nIn order to use CLT approximation, we have to ensure conditions are met:\n\nIndependence (extended): data are independent within and between groups\nSuccess-failure (extended): success-failure conditions holds for both groups (must perform four total checks)\n\nIf above hold, then:\n\n\n\\[\n\\hat{p}_{1} - \\hat{p}_{2} \\overset{\\cdot}{\\sim} N\\left(p_{1} - p_{2}, \\sqrt{\\frac{p_{1} (1-p_{1})}{n_{1}} + \\frac{p_{2} (1-p_{2})}{n_{2}}} \\right)\n\\]\nwhere \\(p_{1}\\) and \\(p_{2}\\) are the population proportions"
  },
  {
    "objectID": "slides/slides-20-ht-diff-prop.html#hypothesis-test-for-difference-in-proportions",
    "href": "slides/slides-20-ht-diff-prop.html#hypothesis-test-for-difference-in-proportions",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Hypothesis test for difference in proportions",
    "text": "Hypothesis test for difference in proportions\n\nDefine hypotheses. Hypothesis tests for difference in proportions in this class will take the form:\n\n\n\\[\n\\begin{align*}\nH_{0}: \\ &p_{1} = p_{2}  \\\\\nH_{A}: \\ &p_{1} \\neq p_{2}  \\\\\n\\text{ or }\\ &p_{1} < p_{2}   \\\\\n\\text{ or }\\ &p_{1} > p_{2}\n\\end{align*}\n\\]\n\n\nSet \\(\\alpha\\) and collect data/summarise (i.e. obtain \\(\\hat{p}_{1,obs}\\) and \\(\\hat{p}_{2,obs}\\))"
  },
  {
    "objectID": "slides/slides-20-ht-diff-prop.html#pooled-proportion",
    "href": "slides/slides-20-ht-diff-prop.html#pooled-proportion",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Pooled proportion",
    "text": "Pooled proportion\n\nTo verify success-failure conditions, need to know \\(p_{1}\\) and \\(p_{2}\\)\n\nWe don’t have these values, so maybe use \\(\\hat{p}_{1,obs}\\) and \\(\\hat{p}_{2,obs}\\)?\n\nBut wait! If \\(H_{0}: p_{1} = p_{2}\\), then \\(\\hat{p}_{1,obs}\\) and \\(\\hat{p}_{2,obs}\\)come from the same population\nSo under this null, we use a special proportion called the pooled proportion:\n\n\n\\[\n\\hat{p}_{pooled} = \\frac{\\text{total # of successes from both samples}}{\\text{combined sample size}}\n\\]\n\n\nThis is the best estimate of both \\(p_{1}\\) and \\(p_{2}\\) if \\(H_{0}: p_{1} = p_{2}\\) is true!\n\n\n\n\nFor this reason, use \\(\\hat{p}_{pooled}\\) to verify success-failure conditions"
  },
  {
    "objectID": "slides/slides-20-ht-diff-prop.html#hypothesis-test-cont.",
    "href": "slides/slides-20-ht-diff-prop.html#hypothesis-test-cont.",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Hypothesis test (cont.)",
    "text": "Hypothesis test (cont.)\n\nObtain null distribution\n\nIf conditions satisfied, then we know the sampling distribution of \\(\\hat{p}_{1} - \\hat{p}_{2}\\)\nTo obtain the null distribution we assume \\(H_{0}: p_{1} - p_{2} = 0\\) is true and we \\(\\hat{p}_{pooled}\\) to estimate \\(p_{1}\\) and \\(p_{2}\\) to approximate standard error:\n\n\n\n\\[\n\\begin{align*}\n\\hat{p}_{1} - \\hat{p}_{2} &\\overset{\\cdot}{\\sim} N\\left(p_{1} - p_{2}, \\sqrt{\\frac{p_{1} (1-p_{1})}{n_{1}} + \\frac{p_{2} (1-p_{2})}{n_{2}}}  \\right) \\qquad \\text{(CLT)} \\\\ &\\overset{\\cdot}{\\sim} N\\big(0, \\underbrace{\\sqrt{\\frac{\\hat{p}_{pooled}(1 - \\hat{p}_{pooled})}{n_{1}} + \\frac{\\hat{p}_{pooled}(1 - \\hat{p}_{pooled})}{n_{2}}}}_{\\widehat{\\text{SE}}_{0}} \\big) \\qquad (H_{0})\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/slides-20-ht-diff-prop.html#hypothesis-test-cont.-1",
    "href": "slides/slides-20-ht-diff-prop.html#hypothesis-test-cont.-1",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Hypothesis test (cont.)",
    "text": "Hypothesis test (cont.)\nObtain test-statistic:\n\\[\nz = \\frac{\\text{point estimate} - \\text{null value}}{\\text{SE}} \\approx \\frac{(\\hat{p}_{1,obs} - \\hat{p}_{2,obs}) - 0}{\\widehat{\\text{SE}}_{0}}\n\\]\n\nTo obtain p-value, we want \\(\\text{Pr}(Z \\geq z)\\) and/or \\(\\text{Pr}(Z \\leq z)\\) where \\(Z \\sim N(0,1)\\)\n\nObtain using pnorm(z, 0, 1)"
  },
  {
    "objectID": "slides/slides-20-ht-diff-prop.html#example-offshore-drilling",
    "href": "slides/slides-20-ht-diff-prop.html#example-offshore-drilling",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Example: offshore drilling",
    "text": "Example: offshore drilling\nA survey asked 827 randomly sampled registered voters in California: Do you support or oppose about drilling for oil and natural gas of the Coast of California? Or do you now know enough to say? We have the following distribution of responses separated by whether the respondent graduated from college:\n\n\n\n\n \n  \n    position \n    no \n    yes \n    total \n  \n \n\n  \n    do_not_know \n    131 \n    104 \n    235 \n  \n  \n    oppose \n    126 \n    180 \n    306 \n  \n  \n    support \n    132 \n    154 \n    286 \n  \n  \n    total \n    389 \n    438 \n    827 \n  \n\n\n\n\n\n\n\nDo the data provide strong evidence at the 0.05 level that the proportion of college graduates who support off-shore drilling in California is different than that of non-college graduates?"
  },
  {
    "objectID": "slides/slides-20-ht-diff-prop.html#example-offshore-drilling-cont.",
    "href": "slides/slides-20-ht-diff-prop.html#example-offshore-drilling-cont.",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Example: offshore drilling (cont.)",
    "text": "Example: offshore drilling (cont.)\n\n\n\n\nDo the data provide strong evidence at the 0.05 level that the proportion of college graduates who support off-shore drilling in California is different than that of non-college graduates?\n\n\n\nDefine parameters and hypotheses\n\n\nLet \\(p_{c}\\) be the proportion of registered voters from California who are college-graduates who support off-shore drilling\nLet \\(p_{nc}\\) be the proportion be of registered voters from California who are not college-graduates who support off-shore drilling\n\\(H_{0}: p_{nc} - p_{c} = 0\\) and \\(H_{A}: p_{nc} - p_{c} \\neq 0\\)"
  },
  {
    "objectID": "slides/slides-20-ht-diff-prop.html#example-offshore-drilling-cont.-1",
    "href": "slides/slides-20-ht-diff-prop.html#example-offshore-drilling-cont.-1",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Example: offshore drilling (cont.)",
    "text": "Example: offshore drilling (cont.)\n\nObtain observed proportions and pooled proportion.\n\n\n\n\n\n \n  \n    position \n    no \n    yes \n    total \n  \n \n\n  \n    do_not_know \n    131 \n    104 \n    235 \n  \n  \n    oppose \n    126 \n    180 \n    306 \n  \n  \n    support \n    132 \n    154 \n    286 \n  \n  \n    total \n    389 \n    438 \n    827 \n  \n\n\n\n\n\n\n\\(\\hat{p}_{nc, obs}= \\frac{132}{389} = 0.339\\)\n\\(\\hat{p}_{c, obs}= \\frac{154}{438} = 0.352\\)\n\\(\\hat{p}_{pooled} =\\frac{132 + 154}{389 + 438} = \\frac{286}{827} = 0.346\\)\n\n\n\nCheck conditions for inference are satisfied."
  },
  {
    "objectID": "slides/slides-20-ht-diff-prop.html#example-offshore-drilling-cont.-2",
    "href": "slides/slides-20-ht-diff-prop.html#example-offshore-drilling-cont.-2",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Example: offshore drilling (cont.)",
    "text": "Example: offshore drilling (cont.)\n\nConditions for inference:\n\nIndependence: random sample\nSuccess-failure:\n\n\\(n_{nc} \\hat{p}_{pooled} = 389 \\times 0.346 = 134.59 > 10\\)\n\\(n_{nc} (1 - \\hat{p}_{pooled}) = 389 \\times (1 - 0.346) = 254.41 > 10\\)\n\\(n_{c} \\hat{p}_{pooled} = 438 \\times 0.346 = 151.55 > 10\\)\n\\(n_{c} (1 - \\hat{p}_{pooled}) = 438 \\times (1 - 0.346) = 286.45 > 10\\)\n\n\nSince conditions are met, we can proceed"
  },
  {
    "objectID": "slides/slides-20-ht-diff-prop.html#hypotheses-and-null-distribution",
    "href": "slides/slides-20-ht-diff-prop.html#hypotheses-and-null-distribution",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Hypotheses and null distribution",
    "text": "Hypotheses and null distribution\nWant to conduct a hypothesis test for the mean \\(\\mu\\) of a population.\n\nHypotheses: \\(H_0: \\mu= \\mu_{0}\\) versus \\(H_{A}: \\mu \\neq \\mu_{0} \\ (\\text{or } \\mu > \\mu_{0} \\text{ or } \\mu < \\mu_{0})\\)\nVerify conditions for CLT\n\nIndependence\nApproximate normality or large sample size\n\nIf conditions satisfied, the CLT under \\(H_{0}\\) gives us null distribution for \\(\\bar{X}\\):\n\\[\n\\bar{X} \\overset{\\cdot}{\\sim}  N\\left(\\mu_{0}, \\frac{\\sigma}{\\sqrt{n}}\\right)\n\\]"
  },
  {
    "objectID": "slides/slides-20-ht-diff-prop.html#z-test-and-t-test",
    "href": "slides/slides-20-ht-diff-prop.html#z-test-and-t-test",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "z-test and t-test",
    "text": "z-test and t-test\n\nIf \\(\\sigma\\) known, we perform a z-test where our test-statistic is:\n\n\n\\[z = \\frac{\\bar{x} - \\mu_{0}}{\\frac{\\sigma}{\\sqrt{n}}} \\sim N(0,1)\\]\nand we obtain our p-value using pnorm()\n\n\nIf \\(\\sigma\\) unknown, we perform a t-test by estimating \\(\\sigma\\) with \\(s\\). Our test statistic is:\n\n\n\\[\nt = \\frac{\\bar{x} - \\mu_{0}}{\\frac{s}{\\sqrt{n}}} \\sim t_{df} \\qquad df = n-1\n\\]\nand we obtain our p-value using pt()"
  },
  {
    "objectID": "slides/slides-20-ht-diff-prop.html#example-height",
    "href": "slides/slides-20-ht-diff-prop.html#example-height",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Example: height",
    "text": "Example: height\n\nIn the US, the average height for women is 5’3.5” or 63.5 inches\nLet’s conduct a hypothesis test to see if the average height of female-identifying students in STAT 201 is equal to national average.\n\nDefine parameters and hypotheses\n\nI took a random sample of 10 female-identifying students across both sections. Set \\(\\alpha = 0.10\\)"
  },
  {
    "objectID": "slides/slides-20-ht-diff-prop.html#example-height-cont.",
    "href": "slides/slides-20-ht-diff-prop.html#example-height-cont.",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Example: height (cont.)",
    "text": "Example: height (cont.)\n\n\n\n\n\n\n\nn\nmean\nsd\n\n\n\n\n10\n64\n2.748737\n\n\n\n\n\n\n\n\n\n\nAre conditions for inference met?\nIf so, what test (z-test or t-test) should we perform? What is our test-statistic?\n\n\n\n\nConditions:\n\nIndependence: random sample\nApproximate normality: \\(n = 10 < 30\\), but no clear outliers\n\nSince we don’t know \\(\\sigma\\), we perform a \\(t\\)-test:\n\n\\(t = \\frac{\\bar{x} - \\mu_{0}}{s / \\sqrt{n}} = \\frac{64 - 63.5}{2.749 / \\sqrt{10}} = 0.575\\)"
  },
  {
    "objectID": "slides/slides-20-ht-diff-prop.html#example-height-cont.-1",
    "href": "slides/slides-20-ht-diff-prop.html#example-height-cont.-1",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Example: height (cont.)",
    "text": "Example: height (cont.)\n\n\nDraw a picture and write code to find our p-value\n\n\n\\(df = n-1 = 9\\)\np-value is \\(\\text{Pr}(T \\geq 0.575) + \\text{Pr}(T \\leq -0.575) = 2\\times \\text{Pr}(T \\geq 0.575)\\) where \\(T \\sim t_{9}\\)\n\\(2 \\times \\texttt{(1-pt(0.575, 9))}\\) = 0.5793797\n\n\nMake a decision and conclusion in context\n\n\nSince our p-value is greater than 0.01, we fail to reject \\(H_{0}\\). The data do not provide sufficient evidence to suggest that the average height of female-identifying students in STAT 201 is different from national average."
  },
  {
    "objectID": "slides/slides-20-ht-diff-prop.html#example-offshore-drilling-cont.-3",
    "href": "slides/slides-20-ht-diff-prop.html#example-offshore-drilling-cont.-3",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Example: offshore drilling (cont.)",
    "text": "Example: offshore drilling (cont.)\n\n\nFind the null distribution for \\(\\hat{p}_{nc} - \\hat{p}_{c}\\)\n\n\n\n\\[\n\\hat{p}_{nc} - \\hat{p}_{c} \\overset{\\cdot}{\\sim}N\\left(0, \\sqrt{\\frac{0.346(1 - 0.346)}{389} + \\frac{0.346(1 - 0.346)}{438}} = 0.033 \\right)\n\\]\n\n\n\nSet up calculation for test statistic\n\n\n\n\\[\n    z =\\frac{( \\hat{p}_{nc, obs}- \\hat{p}_{c, obs}) - 0}{\\text{SE}_{0}} = \\frac{(0.339 - 0.352) - 0}{0.033} = -0.394\n\\]\n\n\n\nDraw picture and write code for p-value"
  },
  {
    "objectID": "slides/slides-20-ht-diff-prop.html#example-offshore-drilling-cont.-4",
    "href": "slides/slides-20-ht-diff-prop.html#example-offshore-drilling-cont.-4",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Example: offshore drilling (cont.)",
    "text": "Example: offshore drilling (cont.)\np-value calculation:\n\n\\(\\text{Pr}(Z \\leq z) + \\text{Pr}(Z \\geq -z) = 2 \\times \\text{Pr}(Z \\geq 0.394)\\)\n2 * (1 - pnorm(0.394)) = 0.694\n\n\n\nMake a decision and conclusion in context.\n\n\n\nSince our p-value is greater the 0.05, we fail to reject \\(H_{0}\\). The data do not provide strong evidence of a difference between the proportions of college graduates and non-college graduates who support off-shore drilling among California voters."
  },
  {
    "objectID": "slides/slides-20-ht-cont.html#housekeeping",
    "href": "slides/slides-20-ht-cont.html#housekeeping",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Housekeeping",
    "text": "Housekeeping\n\n\n\n\nNo office hours Friday"
  },
  {
    "objectID": "slides/slides-20-ht-cont.html#recap",
    "href": "slides/slides-20-ht-cont.html#recap",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Recap",
    "text": "Recap\n\nHypothesis test for single proportion: \\(H_{0}: p = p_{0}\\) vs \\(H_{A}: p \\neq p_{0}\\) (or \\(>, <\\))\n\nNull distribution (assuming CLT holds under \\(H_{0}\\)):\n\\[\n\\hat{p} \\overset{\\cdot}{\\sim}N\\left(p_{0}, \\sqrt{\\frac{p_{0} (1-p_{0})}{n}} \\right)\n\\]\nObtain test statistic and calculate p-value\n\\[\nz = \\frac{\\hat{p}_{obs} - p_{0}}{\\text{SE}_{0}} \\sim N(0,1)\n\\]"
  },
  {
    "objectID": "slides/slides-20-ht-cont.html#difference-of-two-proportions",
    "href": "slides/slides-20-ht-cont.html#difference-of-two-proportions",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Difference of two proportions",
    "text": "Difference of two proportions\nSuppose we have two populations 1 and 2, and want to conduct a hypothesis test for the difference in population proportions: \\(p_{1} - p_{2}\\)\n\nWe have samples of size \\(n_{1}\\) and \\(n_{2}\\)\nReasonable point estimate: \\(\\hat{p}_{1, obs} - \\hat{p}_{2,obs}\\)\nWe will now work with the sampling distribution of the difference of two sample proportions"
  },
  {
    "objectID": "slides/slides-20-ht-cont.html#sampling-dist.-of-difference-of-two-proportions",
    "href": "slides/slides-20-ht-cont.html#sampling-dist.-of-difference-of-two-proportions",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Sampling dist. of difference of two proportions",
    "text": "Sampling dist. of difference of two proportions\n\nIn order to use CLT approximation, we have to ensure conditions are met:\n\nIndependence (extended): data are independent within and between groups\nSuccess-failure (extended): success-failure conditions holds for both groups (must perform four total checks)\n\nIf above hold, then:\n\n\n\\[\n\\hat{p}_{1} - \\hat{p}_{2} \\overset{\\cdot}{\\sim} N\\left(p_{1} - p_{2}, \\sqrt{\\frac{p_{1} (1-p_{1})}{n_{1}} + \\frac{p_{2} (1-p_{2})}{n_{2}}} \\right)\n\\]\nwhere \\(p_{1}\\) and \\(p_{2}\\) are the population proportions"
  },
  {
    "objectID": "slides/slides-20-ht-cont.html#hypothesis-test-for-difference-in-proportions",
    "href": "slides/slides-20-ht-cont.html#hypothesis-test-for-difference-in-proportions",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Hypothesis test for difference in proportions",
    "text": "Hypothesis test for difference in proportions\n\nDefine hypotheses. Hypothesis tests for difference in proportions in this class will take the form:\n\n\n\\[\n\\begin{align*}\nH_{0}: \\ &p_{1} = p_{2}  \\\\\nH_{A}: \\ &p_{1} \\neq p_{2}  \\\\\n\\text{ or }\\ &p_{1} < p_{2}   \\\\\n\\text{ or }\\ &p_{1} > p_{2}\n\\end{align*}\n\\]\n\n\nSet \\(\\alpha\\) and collect data/summarise (i.e. obtain \\(\\hat{p}_{1,obs}\\) and \\(\\hat{p}_{2,obs}\\))"
  },
  {
    "objectID": "slides/slides-20-ht-cont.html#pooled-proportion",
    "href": "slides/slides-20-ht-cont.html#pooled-proportion",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Pooled proportion",
    "text": "Pooled proportion\n\nTo verify success-failure conditions, need to know \\(p_{1}\\) and \\(p_{2}\\)\n\nWe don’t have these values, so maybe use \\(\\hat{p}_{1,obs}\\) and \\(\\hat{p}_{2,obs}\\)?\n\nBut wait! If \\(H_{0}: p_{1} = p_{2}\\), then maybe \\(\\hat{p}_{1,obs}\\) and \\(\\hat{p}_{2,obs}\\) come from the same population\nSo under this null, we use a special proportion called the pooled proportion:\n\n\n\\[\n\\hat{p}_{pooled} = \\frac{\\text{total # of successes from both samples}}{\\text{combined sample size}}\n\\]\n\n\nThis is the best estimate of both \\(p_{1}\\) and \\(p_{2}\\) if \\(H_{0}: p_{1} = p_{2}\\) is true!\n\n\n\n\nFor this reason, use \\(\\hat{p}_{pooled}\\) to verify success-failure conditions"
  },
  {
    "objectID": "slides/slides-20-ht-cont.html#hypothesis-test-cont.",
    "href": "slides/slides-20-ht-cont.html#hypothesis-test-cont.",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Hypothesis test (cont.)",
    "text": "Hypothesis test (cont.)\n\nObtain null distribution\n\nIf conditions satisfied, then we know the sampling distribution of \\(\\hat{p}_{1} - \\hat{p}_{2}\\)\nTo obtain the null distribution we assume \\(H_{0}: p_{1} - p_{2} = 0\\) is true and we \\(\\hat{p}_{pooled}\\) to estimate \\(p_{1}\\) and \\(p_{2}\\) to approximate standard error:\n\n\n\n\\[\n\\begin{align*}\n\\hat{p}_{1} - \\hat{p}_{2} &\\overset{\\cdot}{\\sim} N\\left(p_{1} - p_{2}, \\sqrt{\\frac{p_{1} (1-p_{1})}{n_{1}} + \\frac{p_{2} (1-p_{2})}{n_{2}}}  \\right) \\qquad \\text{(CLT)} \\\\ &\\overset{\\cdot}{\\sim} N\\big(0, \\underbrace{\\sqrt{\\frac{\\hat{p}_{pooled}(1 - \\hat{p}_{pooled})}{n_{1}} + \\frac{\\hat{p}_{pooled}(1 - \\hat{p}_{pooled})}{n_{2}}}}_{\\widehat{\\text{SE}}_{0}} \\big) \\qquad (H_{0})\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/slides-20-ht-cont.html#hypothesis-test-cont.-1",
    "href": "slides/slides-20-ht-cont.html#hypothesis-test-cont.-1",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Hypothesis test (cont.)",
    "text": "Hypothesis test (cont.)\nObtain test-statistic:\n\\[\nz = \\frac{\\text{point estimate} - \\text{null value}}{\\text{SE}} \\approx \\frac{(\\hat{p}_{1,obs} - \\hat{p}_{2,obs}) - 0}{\\widehat{\\text{SE}}_{0}}\n\\]\n\nTo obtain p-value, we want \\(\\text{Pr}(Z \\geq z)\\) and/or \\(\\text{Pr}(Z \\leq z)\\) where \\(Z \\sim N(0,1)\\)\n\nObtain using pnorm(z, 0, 1)"
  },
  {
    "objectID": "slides/slides-20-ht-cont.html#example-offshore-drilling",
    "href": "slides/slides-20-ht-cont.html#example-offshore-drilling",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Example: offshore drilling",
    "text": "Example: offshore drilling\nA survey asked 827 randomly sampled registered voters in California: Do you support or oppose drilling for oil and natural gas off the Coast of California? Or do you not know enough to say? We have the following distribution of responses separated by whether the respondent graduated from college:\n\n\n\n\n \n  \n    position \n    no \n    yes \n    total \n  \n \n\n  \n    do_not_know \n    131 \n    104 \n    235 \n  \n  \n    oppose \n    126 \n    180 \n    306 \n  \n  \n    support \n    132 \n    154 \n    286 \n  \n  \n    total \n    389 \n    438 \n    827 \n  \n\n\n\n\n\n\n\nDo the data provide strong evidence at the 0.05 level that the proportion of college graduates who support off-shore drilling in California is different than that of non-college graduates?"
  },
  {
    "objectID": "slides/slides-20-ht-cont.html#example-offshore-drilling-cont.",
    "href": "slides/slides-20-ht-cont.html#example-offshore-drilling-cont.",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Example: offshore drilling (cont.)",
    "text": "Example: offshore drilling (cont.)\n\n\n\n\nDo the data provide strong evidence at the 0.05 level that the proportion of college graduates who support off-shore drilling in California is different than that of non-college graduates?\n\n\n\nDefine parameters and hypotheses\n\n\nLet \\(p_{c}\\) be the proportion of registered voters from California who are college-graduates who support off-shore drilling\nLet \\(p_{nc}\\) be the proportion be of registered voters from California who are not college-graduates who support off-shore drilling\n\\(H_{0}: p_{nc} - p_{c} = 0\\) and \\(H_{A}: p_{nc} - p_{c} \\neq 0\\)"
  },
  {
    "objectID": "slides/slides-20-ht-cont.html#example-offshore-drilling-cont.-1",
    "href": "slides/slides-20-ht-cont.html#example-offshore-drilling-cont.-1",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Example: offshore drilling (cont.)",
    "text": "Example: offshore drilling (cont.)\n\nObtain observed proportions and pooled proportion.\n\n\n\n\n\n \n  \n    position \n    no \n    yes \n    total \n  \n \n\n  \n    do_not_know \n    131 \n    104 \n    235 \n  \n  \n    oppose \n    126 \n    180 \n    306 \n  \n  \n    support \n    132 \n    154 \n    286 \n  \n  \n    total \n    389 \n    438 \n    827 \n  \n\n\n\n\n\n\n\\(\\hat{p}_{nc, obs}= \\frac{132}{389} = 0.339\\)\n\\(\\hat{p}_{c, obs}= \\frac{154}{438} = 0.352\\)\n\\(\\hat{p}_{pooled} =\\frac{132 + 154}{389 + 438} = \\frac{286}{827} = 0.346\\)\n\n\n\nCheck conditions for inference are satisfied."
  },
  {
    "objectID": "slides/slides-20-ht-cont.html#example-offshore-drilling-cont.-2",
    "href": "slides/slides-20-ht-cont.html#example-offshore-drilling-cont.-2",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Example: offshore drilling (cont.)",
    "text": "Example: offshore drilling (cont.)\n\nConditions for inference:\n\nIndependence: random sample\nSuccess-failure:\n\n\\(n_{nc} \\hat{p}_{pooled} = 389 \\times 0.346 = 134.59 \\geq 10\\)\n\\(n_{nc} (1 - \\hat{p}_{pooled}) = 389 \\times (1 - 0.346) = 254.41 \\geq 10\\)\n\\(n_{c} \\hat{p}_{pooled} = 438 \\times 0.346 = 151.55 \\geq 10\\)\n\\(n_{c} (1 - \\hat{p}_{pooled}) = 438 \\times (1 - 0.346) = 286.45 \\geq 10\\)\n\n\nSince conditions are met, we can proceed"
  },
  {
    "objectID": "slides/slides-20-ht-cont.html#example-offshore-drilling-cont.-3",
    "href": "slides/slides-20-ht-cont.html#example-offshore-drilling-cont.-3",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Example: offshore drilling (cont.)",
    "text": "Example: offshore drilling (cont.)\n\n\nFind the null distribution for \\(\\hat{p}_{nc} - \\hat{p}_{c}\\)\n\n\n\n\\[\n\\hat{p}_{nc} - \\hat{p}_{c} \\overset{\\cdot}{\\sim}N\\left(0, \\sqrt{\\frac{0.346(1 - 0.346)}{389} + \\frac{0.346(1 - 0.346)}{438}} = 0.033 \\right)\n\\]\n\n\n\nSet up calculation for test statistic\n\n\n\n\\[\n    z =\\frac{( \\hat{p}_{nc, obs}- \\hat{p}_{c, obs}) - 0}{\\text{SE}_{0}} = \\frac{(0.339 - 0.352) - 0}{0.033} = -0.394\n\\]\n\n\n\nDraw picture and write code for p-value"
  },
  {
    "objectID": "slides/slides-20-ht-cont.html#example-offshore-drilling-cont.-4",
    "href": "slides/slides-20-ht-cont.html#example-offshore-drilling-cont.-4",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Example: offshore drilling (cont.)",
    "text": "Example: offshore drilling (cont.)\np-value calculation:\n\n\\(\\text{Pr}(Z \\leq z) + \\text{Pr}(Z \\geq -z) = 2 \\times \\text{Pr}(Z \\geq 0.394)\\)\n2 * (1 - pnorm(0.394)) = 0.694\n\n\n\nMake a decision and conclusion in context.\n\n\n\nSince our p-value is greater the 0.05, we fail to reject \\(H_{0}\\). The data do not provide strong evidence of a difference between the proportions of college graduates and non-college graduates who support off-shore drilling among California voters."
  },
  {
    "objectID": "slides/slides-20-ht-cont.html#hypotheses-and-null-distribution",
    "href": "slides/slides-20-ht-cont.html#hypotheses-and-null-distribution",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Hypotheses and null distribution",
    "text": "Hypotheses and null distribution\nWant to conduct a hypothesis test for the mean \\(\\mu\\) of a population.\n\nHypotheses: \\(H_0: \\mu= \\mu_{0}\\) versus \\(H_{A}: \\mu \\neq \\mu_{0} \\ (\\text{or } \\mu > \\mu_{0} \\text{ or } \\mu < \\mu_{0})\\)\nVerify conditions for CLT\n\nIndependence\nApproximate normality or large sample size\n\nIf conditions satisfied, the CLT under \\(H_{0}\\) gives us null distribution for \\(\\bar{X}\\):\n\\[\n\\bar{X} \\overset{\\cdot}{\\sim}  N\\left(\\mu_{0}, \\frac{\\sigma}{\\sqrt{n}}\\right)\n\\]"
  },
  {
    "objectID": "slides/slides-20-ht-cont.html#z-test-and-t-test",
    "href": "slides/slides-20-ht-cont.html#z-test-and-t-test",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "z-test and t-test",
    "text": "z-test and t-test\n\nIf \\(\\sigma\\) known, we perform a z-test where our test-statistic is:\n\n\n\\[z = \\frac{\\bar{x} - \\mu_{0}}{\\frac{\\sigma}{\\sqrt{n}}} \\sim N(0,1)\\]\nand we obtain our p-value using pnorm()\n\n\nIf \\(\\sigma\\) unknown, we perform a t-test by estimating \\(\\sigma\\) with \\(s\\). Our test statistic is:\n\n\n\\[\nt = \\frac{\\bar{x} - \\mu_{0}}{\\frac{s}{\\sqrt{n}}} \\sim t_{df} \\qquad df = n-1\n\\]\nand we obtain our p-value using pt()"
  },
  {
    "objectID": "slides/slides-20-ht-cont.html#example-height",
    "href": "slides/slides-20-ht-cont.html#example-height",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Example: height",
    "text": "Example: height\n\nIn the US, the average height for women is 5’3.5” or 63.5 inches\nLet’s conduct a hypothesis test to see if the average height of female-identifying students in STAT 201 is equal to national average.\n\nDefine parameters and hypotheses\n\nI took a random sample of 10 female-identifying students across both sections. Set \\(\\alpha = 0.10\\)"
  },
  {
    "objectID": "slides/slides-20-ht-cont.html#example-height-cont.",
    "href": "slides/slides-20-ht-cont.html#example-height-cont.",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Example: height (cont.)",
    "text": "Example: height (cont.)\n\n\n\n\n\n\n\nn\nmean\nsd\n\n\n\n\n10\n64\n2.748737\n\n\n\n\n\n\n\n\n\n\nAre conditions for inference met?\nIf so, what test (z-test or t-test) should we perform? What is our test-statistic?\n\n\n\n\nConditions:\n\nIndependence: random sample\nApproximate normality: \\(n = 10 < 30\\), but no clear outliers\n\nSince we don’t know \\(\\sigma\\), we perform a \\(t\\)-test:\n\n\\(t = \\frac{\\bar{x} - \\mu_{0}}{s / \\sqrt{n}} = \\frac{64 - 63.5}{2.749 / \\sqrt{10}} = 0.575\\)"
  },
  {
    "objectID": "slides/slides-20-ht-cont.html#example-height-cont.-1",
    "href": "slides/slides-20-ht-cont.html#example-height-cont.-1",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Example: height (cont.)",
    "text": "Example: height (cont.)\n\n\nDraw a picture and write code to find our p-value\n\n\n\\(df = n-1 = 9\\)\np-value is \\(\\text{Pr}(T \\geq 0.575) + \\text{Pr}(T \\leq -0.575) = 2\\times \\text{Pr}(T \\geq 0.575)\\) where \\(T \\sim t_{9}\\)\n\\(2 \\times \\texttt{(1-pt(0.575, 9))}\\) = 0.5793797\n\n\nMake a decision and conclusion in context\n\n\nSince our p-value is greater than 0.10, we fail to reject \\(H_{0}\\). The data do not provide sufficient evidence to suggest that the average height of female-identifying students in STAT 201 is different from national average."
  },
  {
    "objectID": "practice_probs/practice-20-ht-cont.html",
    "href": "practice_probs/practice-20-ht-cont.html",
    "title": "More with HT",
    "section": "",
    "text": "Let’s return to the offshore drilling data from class.\n\n\n\n\n\n \n  \n    position \n    no \n    yes \n    total \n  \n \n\n  \n    do_not_know \n    131 \n    104 \n    235 \n  \n  \n    oppose \n    126 \n    180 \n    306 \n  \n  \n    support \n    132 \n    154 \n    286 \n  \n  \n    total \n    389 \n    438 \n    827 \n  \n\n\n\n\n\nConduct a hypothesis test to determine if the data provide strong evidence that the proportion of college graduates who do not have an opinion on this issue is different than that of non-college graduates.\n\nNew York is known as “the city that never sleeps”. A random sample of 25 New Yorkers were asked how much sleep they get per night. Statistical summaries includes a sample mean hours of 7.73 and standard deviation of 0.77. The point estimate suggests New Yorkers sleep less than 8 hours a night on average. Is the result statistically significant at the 0.05 level? Make a conclusion based on the your decision.\n\\((^*)\\) According to a report on sleep deprivation by the Centers for Disease Control and Prevention, the proportion of California residents who reported insufficient rest or sleep during each of the preceding 30 days is 8.0%, while this proportion is 8.8% for Oregon residents. These data are based on simple random samples of 11,545 California and 4,691 Oregon residents.\n\nConduct a hypothesis test to determine if these data provide strong evidence the rate of sleep deprivation is different for the two states. (Reminder: Check conditions)\nIt is possible the conclusion of the test in part (a) is incorrect. If this is the case, what type of error was made?\n\nThe population of all verbal GRE scores are known to have a standard deviation of 8.5. A certain graduate department hopes to receive applicants with a verbal GRE scores over 210. This year, the mean verbal GRE scores for the 42 applicants was 212.79. Using a significance level of 0.05, is this new mean significantly greater than the desired mean of 210?"
  },
  {
    "objectID": "slides/slides-21-ht-diff-means.html",
    "href": "slides/slides-21-ht-diff-means.html",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "",
    "text": "No office hours tomorrow\nDaylight savings this weekend\nData collection proposal due Monday 11/4 midnight!"
  },
  {
    "objectID": "slides/slides-21-ht-diff-means.html#recap",
    "href": "slides/slides-21-ht-diff-means.html#recap",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Recap",
    "text": "Recap\n\nTest for difference in two proportions\n\nLearned about \\(\\hat{p}_{pooled}\\)\n\nTest for a single mean\n\n\\(z\\)-test: we know \\(\\sigma\\), use standard Normal distribution\n\\(t\\)-test: we don’t know \\(\\sigma\\), use \\(t\\) distribution"
  },
  {
    "objectID": "slides/slides-21-ht-diff-means.html#paired-data-recap",
    "href": "slides/slides-21-ht-diff-means.html#paired-data-recap",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Paired data (recap)",
    "text": "Paired data (recap)\n\nRecall paired data: we have two set of data \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{y}\\) where each \\(x_{i}\\) has a corresponding to one \\(y_{i}\\)\n\nCan obtain differences \\(d_{i} = y_{i} - x_{i}\\)\nWe are interested in the true mean difference \\(\\mu_{d}\\)\n\nRecall: if observational units are independent and the differences are approximately Normal, then CLT gives us:\n\n\n\\[\n\\bar{d} \\overset{\\cdot}{\\sim} N\\left(\\mu_{d}, \\frac{\\sigma_{d}}{\\sqrt{n}}\\right)\n\\]\n\n\nWe don’t typically know \\(\\sigma_{d}\\), so replace with sample \\(s_{d}\\) (and then use \\(t\\) distribution)"
  },
  {
    "objectID": "slides/slides-21-ht-diff-means.html#hypothesis-test",
    "href": "slides/slides-21-ht-diff-means.html#hypothesis-test",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Hypothesis test",
    "text": "Hypothesis test\n\nHypotheses: \\(H_0: \\mu_{d} = \\mu_{0}\\) versus \\(H_{A}: \\mu_{d} \\neq \\mu_0\\) (or \\(>\\) or \\(<\\) )\nObtain summary statistics \\(\\bar{d}_{obs}\\) and \\(s_{d}\\)\n\nCheck if CLT holds. If so, what is our null distribution?\n\n\n\n\\[\n\\bar{d} \\overset{\\cdot}{\\sim} N\\left(\\mu_0, \\frac{\\sigma_{d}}{\\sqrt{n}} \\right)\n\\]\n\n\n\nBecause we don’t know \\(\\sigma_{d}\\), our test statistic here is:\n\n\n\n\\[\nt = \\frac{\\bar{d}_{obs} - \\mu_0}{\\frac{s_{d}}{\\sqrt{n}}} \\sim t_{df}\n\\]\nwhere \\(df = n-1\\)"
  },
  {
    "objectID": "slides/slides-21-ht-diff-means.html#example-zinc-revisited",
    "href": "slides/slides-21-ht-diff-means.html#example-zinc-revisited",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Example: zinc (revisited)",
    "text": "Example: zinc (revisited)\n\n\n\nData consist of measured zinc concentrations in bottom water and surface water at 10 randomly sampled wells:\n\nDo the data suggest that the true average concentration in the bottom water is greater than that of surface water? Let’s now answer this using a hypothesis test at the 0.05 level.\n\n\n\nDefine parameters and hypotheses\n\n\nLet \\(\\mu_{d}\\) be the true mean difference between zinc concentrations (bottom-surface)\n\\(H_{0}: \\mu_{d} = 0\\) versus \\(H_{A}: \\mu_{d} > 0\\)\n\nLast week, we saw conditions for CLT were satisfied"
  },
  {
    "objectID": "slides/slides-21-ht-diff-means.html#example-zinc-cont.",
    "href": "slides/slides-21-ht-diff-means.html#example-zinc-cont.",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Example: zinc (cont.)",
    "text": "Example: zinc (cont.)\n\nzinc <- zinc |>\n  mutate(d = bottom - surface)\nd_bar <- mean(zinc$d)\nd_bar\n\n[1] 0.0804\n\ns_d <- sd(zinc$d)\ns_d\n\n[1] 0.05227321\n\n\n\n\n\n\nFind the test-statistic"
  },
  {
    "objectID": "slides/slides-21-ht-diff-means.html#housekeeping",
    "href": "slides/slides-21-ht-diff-means.html#housekeeping",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Housekeeping",
    "text": "Housekeeping\n\n\n\n\nNo office hours tomorrow\nDaylight savings this weekend\nData collection proposal due Monday 11/4 midnight!"
  },
  {
    "objectID": "slides/slides-21-ht-diff-means.html#sampling-distribution-for-difference-in-means",
    "href": "slides/slides-21-ht-diff-means.html#sampling-distribution-for-difference-in-means",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Sampling distribution for difference in means",
    "text": "Sampling distribution for difference in means\n\nTwo populations, interest in \\(\\mu_{1} - \\mu_{2}\\) (or other order)\nSamples of size \\(n_{1}\\) and \\(n_{2}\\)\nIf CLT holds, we learned sampling distribution of difference in sample means is:\n\n\n\\[\n\\bar{X}_{1} - \\bar{X}_{2} \\overset{\\cdot}{\\sim} N\\left(\\mu_{1} - \\mu_{2}, \\sqrt{\\frac{\\sigma_{1}^2}{n_{1}} + \\frac{\\sigma_{2}^2}{n_{2}}} \\right)\n\\]\n\n\nWhen we don’t know the population standard deviations, we replace the \\(\\sigma\\) with \\(s\\) and use a \\(t\\) distribution\nSame thing will happen for hypothesis test!\n\n\nSame conditions for inference: independence (extended) and approximate normality/large sample size (extended)"
  },
  {
    "objectID": "slides/slides-21-ht-diff-means.html#hypothesis-test-1",
    "href": "slides/slides-21-ht-diff-means.html#hypothesis-test-1",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Hypothesis test",
    "text": "Hypothesis test\nHypotheses \\(H_{0}: \\mu_{1} = \\mu_{2}\\) versus \\(H_{A}: \\mu_{1} \\neq \\mu_{2}\\) (or \\(>\\) or \\(<\\))\n\nIf CLT holds, our null distribution for the difference in sample means is:\n\n\n\\[\n\\bar{X}_{1} - \\bar{X}_{2} \\overset{\\cdot}{\\sim} N\\left(0, \\sqrt{\\frac{\\sigma_{1}^2}{n_{1}} + \\frac{\\sigma_{2}^2}{n_{2}} }\\right)\n\\]\n\n\n\nIn practice, use \\(s_{1}\\) and \\(s_{2}\\). So our test-statistic is…\n\n\n\n\\[\nt= \\frac{\\text{point est} - \\text{null value} }{\\widehat{\\text{SE}}_{0}} = \\frac{(\\bar{x}_{1} - \\bar{x}_{2}) - 0}{\\sqrt{\\frac{s_{1}^2}{n_{1}} + \\frac{s_{2}^2}{n_{2}}}} \\sim t_{df}\n\\]\nwhere \\(df = \\min(n_{1}-1, n_{2}-1)\\)"
  },
  {
    "objectID": "slides/slides-21-ht-diff-means.html#activity",
    "href": "slides/slides-21-ht-diff-means.html#activity",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Activity",
    "text": "Activity\nMunchkins!"
  },
  {
    "objectID": "slides/slides-21-ht-diff-means.html#example-zinc-cont.-1",
    "href": "slides/slides-21-ht-diff-means.html#example-zinc-cont.-1",
    "title": "Hypothesis testing with CLT (cont.)",
    "section": "Example: zinc (cont.)",
    "text": "Example: zinc (cont.)\n\n\\[t = \\frac{\\bar{d}_{obs} - \\mu_{0}}{s_{d}/\\sqrt{n}} = \\frac{0.0804 - 0}{0.052/\\sqrt{10}} = 4.889  \\sim t_{9}\\]\n\n\nSo our p-value is \\(\\text{Pr}(T \\geq t) = \\text{Pr}(T \\geq 4.889) = 1 - \\texttt{pt(4.889, 9)} = 0\\)\nWe reject \\(H_{0}\\)! The data provide convincing evidence that zinc concentrations of bottom well water is greater than those of surface water."
  },
  {
    "objectID": "live_code/ht_diff_means_b.html",
    "href": "live_code/ht_diff_means_b.html",
    "title": "HT for difference in means",
    "section": "",
    "text": "chocolate <- c(18, 16, 19, 19, 18, 17, 16, 19, 19, 18, 16, 19, 16, 18, 15, 16, 18, 16) \nglazed <- c(16, 15, 16, 17, 15, 14, 15, 17, 18, 16, 15)\n\n\nsd_c <- sd(chocolate)\nsd_c\n\n[1] 1.377931\n\nsd_g <- sd(glazed)\nsd_g\n\n[1] 1.167748\n\nxbar_c <- mean(chocolate)\nxbar_c\n\n[1] 17.38889\n\nxbar_g <- mean(glazed)\nxbar_g\n\n[1] 15.81818\n\n# check conditions\nhist(chocolate)\n\n\n\nhist(glazed)\n\n\n\nn_c <- length(chocolate)\nn_c\n\n[1] 18\n\nn_g <- length(glazed)\nn_g\n\n[1] 11\n\nse <- sqrt(sd_c^2 / n_c + sd_g^2/n_g)\ntest_stat <- ((xbar_c - xbar_g) - 0)/se\ntest_stat\n\n[1] 3.279075\n\ndf <- min(n_c - 1, n_g - 1)\n2*(1-pt(test_stat, df))\n\n[1] 0.008301925"
  },
  {
    "objectID": "live_code/ci_diff_means.html",
    "href": "live_code/ci_diff_means.html",
    "title": "CI: Difference in means",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readr)\nurl_file <- \"https://raw.githubusercontent.com/midd-stat201-fall2024/midd-stat201-fall2024.github.io/refs/heads/main/data/parkinsons.csv\"\nparkinsons <- read_csv(url_file)\n\n\nGet summary statistics\n\nx_pd <- parkinsons |>\n  filter(status == \"PD\") |>\n  pull(shimmer)\nx_healthy <- parkinsons |>\n  filter(status == \"Healthy\") |>\n  pull(shimmer)\nn1 <- length(x_pd)\nn2 <- length(x_healthy)\nxbar1 <- mean(x_pd)\nxbar2 <- mean(x_healthy)\ns1 <- sd(x_pd)\ns2 <- sd(x_healthy)\n\n\n\nObtain quantities for CI\n\npoint_est <- xbar1 - xbar2\nSE <- sqrt(s1^2/n1 + s2^2/n2)\ndf <- min(n1-1, n2-1)\ncv <- qt(0.975, df = df)\n\nlower <- point_est - cv * SE\nupper <- point_est + cv * SE\n\nOur 95% CI for the difference in voice shimmers (PD - non PD) is (0.12 , 0.197)."
  },
  {
    "objectID": "live_code/ht_diff_means_a.html",
    "href": "live_code/ht_diff_means_a.html",
    "title": "HT for difference in means",
    "section": "",
    "text": "library(tidyverse)\nchocolate <- c(20, 18, 17, 17, 17, 17, 16, 17, 18, 19, 17, 19, 17, 18 ) \nglazed <- c(15, 15, 15, 16, 17, 15, 15, 15, 15, 15, 15, 15, 16, 14, 15,15 )\n\n# check conditions\nhist(chocolate)\n\n\n\nhist(glazed)\n\n\n\nxbar_c <- mean(chocolate)\nxbar_c\n\n[1] 17.64286\n\nxbar_g <- mean(glazed)\nxbar_g\n\n[1] 15.1875\n\nsd_c <- sd(chocolate)\nsd_c\n\n[1] 1.081818\n\nsd_g <- sd(glazed)\nsd_g\n\n[1] 0.6551081\n\nn_c <- length(chocolate)\nn_c\n\n[1] 14\n\nn_g <- length(glazed)\nn_g\n\n[1] 16\n\ndf <- min(n_c - 1, n_g - 1)\ndf\n\n[1] 13\n\npoint_est <- xbar_c - xbar_g\nse <- sqrt(sd_c^2/n_c + sd_g^2/n_g)\ntest_stat <- (point_est - 0)/se\n2*(1-pt(test_stat, df))\n\n[1] 5.276221e-06"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#housekeeping",
    "href": "slides/slides-23-slr-intro.html#housekeeping",
    "title": "Introduction to Simple Linear Regression",
    "section": "Housekeeping",
    "text": "Housekeeping\n\n\n\n\nHomework 7 due tonight!\nLast problem set is assigned today! Atypical due date: Wednesday 11/13"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#fitting-a-line-to-data",
    "href": "slides/slides-23-slr-intro.html#fitting-a-line-to-data",
    "title": "Introduction to Simple Linear Regression",
    "section": "Fitting a line to data",
    "text": "Fitting a line to data\n\nHopefully we are all familiar with the equation of a line: \\(y = mx + b\\)\n\nIntercept \\(b\\) and slope \\(m\\) determine specific line\nThis function is deterministic: as long as we know \\(x\\), we know value of \\(y\\) exactly\n\nLinear regression: statistical method where the relationship between variable \\(x\\) and variable \\(y\\) is modeled as a line + error:\n\n\n\\[\ny = \\underbrace{\\beta_{0} + \\beta_{1} x}_{\\text{line}} + \\underbrace{\\epsilon}_{\\text{error}}\n\\]"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#linear-regression-model",
    "href": "slides/slides-23-slr-intro.html#linear-regression-model",
    "title": "Introduction to Simple Linear Regression",
    "section": "Linear regression model",
    "text": "Linear regression model\n\\[\ny = \\beta_{0} + \\beta_{1} x + \\epsilon\n\\]\n\nWe have two variables:\n\n\\(y\\) is response variable. Must be continuous numerical.\n\\(x\\) is explanatory variable, also called the predictor variable\n\nCan be numerical or categorical\n\n\n\\(\\beta_{0}\\) and \\(\\beta_{1}\\) are the model parameters (intercept and slope)\n\nEstimated using the data, with point estimates \\(b_{0}\\) and \\(b_{1}\\)\n\n\\(\\epsilon\\) (epsilon) represents the error\n\nAccounts for variability: we do not expect all data to fall perfectly on the line!\nSometimes we drop the \\(\\epsilon\\) term for convenience"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#linear-relationship",
    "href": "slides/slides-23-slr-intro.html#linear-relationship",
    "title": "Introduction to Simple Linear Regression",
    "section": "Linear relationship",
    "text": "Linear relationship\nSuppose we have the following data:\n\n\n\n\n\n\nObservations won’t fall exactly on a line, but do fall around a straight line, so maybe a linear relationship makes sense!"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#fitted-values",
    "href": "slides/slides-23-slr-intro.html#fitted-values",
    "title": "Introduction to Simple Linear Regression",
    "section": "Fitted values",
    "text": "Fitted values\nSuppose we have some specific estimates \\(b_0\\) and \\(b_{1}\\). We could fit the linear relationship using these values as:\n\\[\n\\hat{y} = b_{0} + b_{1} x\n\\]\n\nThe hat on \\(y\\) signifies that this is an estimate: the estimated/fitted value of \\(y\\) given these specific values of \\(x\\), \\(b_{0}\\) and \\(b_{1}\\)\n\nWe observe \\(y\\), but can obtain a corresponding estimate \\(\\hat{y}\\)\n\nNote that the fitted value is obtained without the error"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#fitted-values-cont.",
    "href": "slides/slides-23-slr-intro.html#fitted-values-cont.",
    "title": "Introduction to Simple Linear Regression",
    "section": "Fitted values (cont.)",
    "text": "Fitted values (cont.)\n\n\n\n\n\n\nSuppose our estimated line is the yellow one\nEvery observed value \\(y_{i}\\) has a corresponding fitted value \\(\\hat{y}_{i}\\); the above plot just shows three specific examples"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#residual",
    "href": "slides/slides-23-slr-intro.html#residual",
    "title": "Introduction to Simple Linear Regression",
    "section": "Residual",
    "text": "Residual\nResiduals are the remaining variation in the data after fitting a model.\n\\[\n\\text{data} = \\text{fit} + \\text{residual}\n\\]\n\nFor each observation \\(i\\), we obtain residual \\(e_{i}\\) via:\n\n\n\\[y_{i} = \\hat{y}_{i} + e_{i} \\quad \\Rightarrow \\quad e_{i} = \\hat{y}_{i} - y_{i}\\]\n\n\nResidual = difference between observed and expected\nSince each observation has a fitted value, each observation has a residual\n\nIn the linear regression case, the residual is indicated by the vertical dashed line\n\nWhat is the ideal value for a residual?"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#residual-plot",
    "href": "slides/slides-23-slr-intro.html#residual-plot",
    "title": "Introduction to Simple Linear Regression",
    "section": "Residual plot",
    "text": "Residual plot\n\nResiduals are very helpful in evaluating how well a model fits a set of data\nResidual plot: original \\(x\\) values plotted against their corresponding residuals on \\(y\\)-axis"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#residual-plot-cont.",
    "href": "slides/slides-23-slr-intro.html#residual-plot-cont.",
    "title": "Introduction to Simple Linear Regression",
    "section": "Residual plot (cont.)",
    "text": "Residual plot (cont.)\nResidual plots can be useful for identifying characteristics/patterns that remain in the data even after fitting a model.\n\n\nJust because you fit a model to data, does not mean the model is a good fit!\n\n\n\n\n\n\n\n\n\nCan you identify any patterns remaining in the residuals?\n\nSorry! The residuals shown here are taken as \\(y_{i} - \\hat{y}_{i}\\)!"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#describing-linear-relationships",
    "href": "slides/slides-23-slr-intro.html#describing-linear-relationships",
    "title": "Introduction to Simple Linear Regression",
    "section": "Describing linear relationships",
    "text": "Describing linear relationships\nDifferent data may exhibit different strength of linear relationships:\n\n\n\n\n\n\nCan we quantify the strength of the linear relationship?"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#correlation",
    "href": "slides/slides-23-slr-intro.html#correlation",
    "title": "Introduction to Simple Linear Regression",
    "section": "Correlation",
    "text": "Correlation\n\nCorrelation is describes the strength of a linear relationship between two variables\n\nThe observed sample correlation is denoted by \\(R\\)\nFormula (not important): \\(R = \\frac{1}{n-1} \\sum_{i=1}^{n} \\left(\\frac{x_{i} - \\bar{x}}{s_x} \\right)\\left(\\frac{y_{i} - \\bar{y}}{s_y} \\right)\\)\n\n\n\n\n\nAlways takes a value between -1 and 1\n\n-1 = perfectly linear and negative\n1 = perfectly linear and positive\n0 = no linear relationship\n\nNonlinear trends, even when strong, sometimes produce correlations that do not reflect the strength of the relationship"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#different-lines",
    "href": "slides/slides-23-slr-intro.html#different-lines",
    "title": "Introduction to Simple Linear Regression",
    "section": "Different lines",
    "text": "Different lines\nThe following display the same set of 50 observations.\n\n\n\n\n\n\n\n\n\nWhich line would you say fits the data the best?\n\n\n\n\nThere are infinitely many choices of \\((b_{0}, b_{1})\\) that could be used to create a line\nWe want the BEST choice (i.e. the one that gives us the “line of best fit”)\n\n\nHow to define “best”?"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#line-of-best-fit",
    "href": "slides/slides-23-slr-intro.html#line-of-best-fit",
    "title": "Introduction to Simple Linear Regression",
    "section": "Line of best fit",
    "text": "Line of best fit\nOne way to define a “best” is to choose the specific values of \\((b_{0}, b_{1})\\) that minimize the total residuals across all \\(n\\) data points. Results in following possible criterion:\n\nLeast absolute criterion: minimize sum of residual magnitudes:\n\n\n\\[\n|e_{1} | + |e_{2}| + \\ldots + |e_{n}|\n\\]\n\n\n\nLeast squares criterion: minimize sum of squared residuals:\n\n\n\n\\[\ne_{1}^2 + e_{2}^2 +\\ldots + e_{n}^2\n\\]\n\n\nThe choice of \\((b_{0}, b_{1})\\) that satisfy least squares criterion yields the least squares line, and will be our criterion for “best”\nOn previous slide, yellow line is the least squares line, whereas pink line is the least absolute line"
  },
  {
    "objectID": "coding_practice/coding-practice-22-pivoting.html",
    "href": "coding_practice/coding-practice-22-pivoting.html",
    "title": "Pivoting + HT coding practice",
    "section": "",
    "text": "library(tidyverse)\ndiamond_price <- read_csv(\"https://raw.githubusercontent.com/midd-stat201-fall2024/midd-stat201-fall2024.github.io/refs/heads/main/data/diamonds.csv\")\n\n\nWrangle/manipulate the diamond_price data frame into the long format. Ensure that the variable names you end up choosing are informative/meaningful. Store the resulting data frame into a new data frame called diamond_price_long.\n\n\n\n\n\nUsing your new data frame, check if the conditions for the CLT-based hypothesis test are satisfied.\n\n\n\n\nAnswer:\n\n(Optional) Obtain your test statistic and p-value. Report your p-value using in-line code.\n\n\n\n\nAnswer:\n\n(Optional) Conduct your simulation-based hypothesis test for the same set of hypotheses. Report your p-value using in-line code.\n\n\n\n\nAnswer:\nOnce finished, knit once more and submit the HTML to Canvas!"
  },
  {
    "objectID": "live_code/pivoting.html",
    "href": "live_code/pivoting.html",
    "title": "Pivoting in R",
    "section": "",
    "text": "library(tidyverse)\nlibrary(openintro)"
  },
  {
    "objectID": "live_code/pivoting.html#pivot_longer",
    "href": "live_code/pivoting.html#pivot_longer",
    "title": "Pivoting in R",
    "section": "pivot_longer()",
    "text": "pivot_longer()\nLet’s take a look at first few observations of the relig_income data frame from the openintro package:\n\nrelig_income |>\n  head()\n\n# A tibble: 6 × 11\n  religion  `<$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` `$75-100k`\n  <chr>       <dbl>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl>      <dbl>\n1 Agnostic       27        34        60        81        76       137        122\n2 Atheist        12        27        37        52        35        70         73\n3 Buddhist       27        21        30        34        33        58         62\n4 Catholic      418       617       732       670       638      1116        949\n5 Don’t kn…      15        14        15        11        10        35         21\n6 Evangeli…     575       869      1064       982       881      1486        949\n# ℹ 3 more variables: `$100-150k` <dbl>, `>150k` <dbl>,\n#   `Don't know/refused` <dbl>\n\n\nThis data is currently in “wide” format: a row has more than one observation. That is, the same outcome variable appears in multiple columns. In the relig_income data, the outcome observation that spans across several columns is income range. The different incomes columns are essentially different levels of the same categorical variable.\nSuppose I want to obtain the conditional probability of income bracket by religion type. These probabilities are VERY difficult to obtain with the data frame as is. Would would be extremely useful is if we could create a single categorical variable for the income range.\nWe will manipulate the data frame to the “long” format: the outcome variable only exists in one column, and a second column/variable tells us the different levels. Each row has one observation, but the units of observation are repeated down one column.\n\n\nThis is helpful for us to perform group_by() or facet_wrap().\n\n\n\n\n\n\nTip\n\n\n\n\n\nWhat are the units of observation in the relig_income data?\n\n\n\nWe will do this with the pivot_longer() function. This function requires a couple of arguments:\n\ncols: which columns to pivot into a “longer” format. That is, the columns that we should “move”\nnames_to: a string character that provides the new column name for the categorical variable you are creating\nvalues_to: a string character that provides the new variable name for the response/outcome variable that is common across all levels of the categorical variable\n\n\nrelig_income_long <- relig_income |>\n  pivot_longer(cols = 2:11, names_to = \"income_range\",  values_to = \"count\")\nrelig_income_long \n\n# A tibble: 180 × 3\n   religion income_range       count\n   <chr>    <chr>              <dbl>\n 1 Agnostic <$10k                 27\n 2 Agnostic $10-20k               34\n 3 Agnostic $20-30k               60\n 4 Agnostic $30-40k               81\n 5 Agnostic $40-50k               76\n 6 Agnostic $50-75k              137\n 7 Agnostic $75-100k             122\n 8 Agnostic $100-150k            109\n 9 Agnostic >150k                 84\n10 Agnostic Don't know/refused    96\n# ℹ 170 more rows\n\n\n\n\nNote that the unit of observation is repeated down several rows!\nThe following achieve the same result:\n\nrelig_income |>\n  pivot_longer(cols = -c(1), names_to = \"income_range\",  values_to = \"count\")\n\n# A tibble: 180 × 3\n   religion income_range       count\n   <chr>    <chr>              <dbl>\n 1 Agnostic <$10k                 27\n 2 Agnostic $10-20k               34\n 3 Agnostic $20-30k               60\n 4 Agnostic $30-40k               81\n 5 Agnostic $40-50k               76\n 6 Agnostic $50-75k              137\n 7 Agnostic $75-100k             122\n 8 Agnostic $100-150k            109\n 9 Agnostic >150k                 84\n10 Agnostic Don't know/refused    96\n# ℹ 170 more rows\n\nrelig_income |>\n  pivot_longer(cols = !religion, names_to = \"income_range\",  values_to = \"count\")\n\n# A tibble: 180 × 3\n   religion income_range       count\n   <chr>    <chr>              <dbl>\n 1 Agnostic <$10k                 27\n 2 Agnostic $10-20k               34\n 3 Agnostic $20-30k               60\n 4 Agnostic $30-40k               81\n 5 Agnostic $40-50k               76\n 6 Agnostic $50-75k              137\n 7 Agnostic $75-100k             122\n 8 Agnostic $100-150k            109\n 9 Agnostic >150k                 84\n10 Agnostic Don't know/refused    96\n# ℹ 170 more rows\n\n\nNow it is extremely easy to obtain conditional probabilities:\n\nrelig_income_long |>\n  group_by(religion) |>\n  mutate(cond_prob = count/sum(count)) |>\n  filter(income_range == \"$30-40k\") |>\n  select(religion, income_range, cond_prob) |>\n  head()\n\n# A tibble: 6 × 3\n# Groups:   religion [6]\n  religion           income_range cond_prob\n  <chr>              <chr>            <dbl>\n1 Agnostic           $30-40k         0.0981\n2 Atheist            $30-40k         0.101 \n3 Buddhist           $30-40k         0.0827\n4 Catholic           $30-40k         0.0832\n5 Don’t know/refused $30-40k         0.0404\n6 Evangelical Prot   $30-40k         0.104"
  },
  {
    "objectID": "live_code/pivoting.html#pivot_wider",
    "href": "live_code/pivoting.html#pivot_wider",
    "title": "Pivoting in R",
    "section": "pivot_wider()",
    "text": "pivot_wider()\nWe can also pivot from long to wide format. Let’s look at the fish_encounters data.\n\nfish_encounters |>\n  head()\n\n# A tibble: 6 × 3\n  fish  station  seen\n  <fct> <fct>   <int>\n1 4842  Release     1\n2 4842  I80_1       1\n3 4842  Lisbon      1\n4 4842  Rstr        1\n5 4842  Base_TD     1\n6 4842  BCE         1\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nSuppose our observation unit of interest is fish. Explain why this data frame is in the long format.\n\n\n\nWe want to pivot the data such that each fish is an observation, and we can easily see which stations it was observed at. We can do this using the pivot_wider() function, which takes two arguments:\n\nnames_from: the name of the variable(s) in the data frame to get the name of the output column.\nvalues_from: the name of the variable(s) in the data frame to get the cell values from\n\nThat is, the input to names_from should be the categorical variable(s), and the different levels of this categorical variable will be the new column names. What should we fill these cells with? The values specified by values_from.\n\nfish_encounters |>\n  pivot_wider(names_from = station, values_from = seen)\n\n# A tibble: 19 × 12\n   fish  Release I80_1 Lisbon  Rstr Base_TD   BCE   BCW  BCE2  BCW2   MAE   MAW\n   <fct>   <int> <int>  <int> <int>   <int> <int> <int> <int> <int> <int> <int>\n 1 4842        1     1      1     1       1     1     1     1     1     1     1\n 2 4843        1     1      1     1       1     1     1     1     1     1     1\n 3 4844        1     1      1     1       1     1     1     1     1     1     1\n 4 4845        1     1      1     1       1    NA    NA    NA    NA    NA    NA\n 5 4847        1     1      1    NA      NA    NA    NA    NA    NA    NA    NA\n 6 4848        1     1      1     1      NA    NA    NA    NA    NA    NA    NA\n 7 4849        1     1     NA    NA      NA    NA    NA    NA    NA    NA    NA\n 8 4850        1     1     NA     1       1     1     1    NA    NA    NA    NA\n 9 4851        1     1     NA    NA      NA    NA    NA    NA    NA    NA    NA\n10 4854        1     1     NA    NA      NA    NA    NA    NA    NA    NA    NA\n11 4855        1     1      1     1       1    NA    NA    NA    NA    NA    NA\n12 4857        1     1      1     1       1     1     1     1     1    NA    NA\n13 4858        1     1      1     1       1     1     1     1     1     1     1\n14 4859        1     1      1     1       1    NA    NA    NA    NA    NA    NA\n15 4861        1     1      1     1       1     1     1     1     1     1     1\n16 4862        1     1      1     1       1     1     1     1     1    NA    NA\n17 4863        1     1     NA    NA      NA    NA    NA    NA    NA    NA    NA\n18 4864        1     1     NA    NA      NA    NA    NA    NA    NA    NA    NA\n19 4865        1     1      1    NA      NA    NA    NA    NA    NA    NA    NA\n\n\n\n\n\n\nNote that there are some NA values after pivoting. From the Help file, this is because misses were not directly recorded in the original form of the data. Try adding the argument values_fill = 0 to the pivot_wider() function."
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#linear-regression-model-1",
    "href": "slides/slides-23-slr-intro.html#linear-regression-model-1",
    "title": "Introduction to Simple Linear Regression",
    "section": "Linear regression model",
    "text": "Linear regression model\nRemember, our linear regression model is:\n\\[\ny = \\beta_{0} + \\beta_{1}x + \\epsilon\n\\]\n\nWhile not wrong, it can be good practice to be specific about an observation \\(i\\):\n\\[\ny_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\epsilon_{i}, \\qquad i = 1,\\ldots, n\n\\]\n\n\nHere, we are stating that each observation \\(i\\) has a specific:\n\nexplanatory variable value \\(x_{i}\\)\nresponse variable value \\(y_{i}\\)\nerror/randomness \\(\\epsilon_{i}\\)"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#conditions-for-the-least-squares-line-line",
    "href": "slides/slides-23-slr-intro.html#conditions-for-the-least-squares-line-line",
    "title": "Introduction to Simple Linear Regression",
    "section": "Conditions for the least squares line (LINE)",
    "text": "Conditions for the least squares line (LINE)\nLike when using CLT, we should check some conditions before saying a linear regression model is appropriate!\n\nAssume for now that \\(x\\) is continuous numerical.\n\n\nLinearity: data should show a linear trend between \\(x\\) and \\(y\\)\nIndependence: the observations \\(i\\) are independent of each other\n\ne.g. random sample\nNon-example: time-series data\n\nNormality/nearly normal residuals: the residuals should appear approximately Normal\n\nPossible violations: outliers, influential points (more on this later)\n\nEqual variability: variability of points around the least squares line remains roughly constant"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#running-example",
    "href": "slides/slides-23-slr-intro.html#running-example",
    "title": "Introduction to Simple Linear Regression",
    "section": "Running example",
    "text": "Running example\nWe will see how to check for these four LINE conditions using the cherry data from openintro.\n\n\n\n\n\n\n\ndiam\nvolume\n\n\n\n\n8.3\n10.3\n\n\n8.6\n10.3\n\n\n8.8\n10.2\n\n\n10.5\n16.4\n\n\n10.7\n18.8\n\n\n\n\n\n\n\nExplanatory variable \\(x\\): diam\nResponse variable \\(y\\): volume"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#linearity",
    "href": "slides/slides-23-slr-intro.html#linearity",
    "title": "Introduction to Simple Linear Regression",
    "section": "1. Linearity",
    "text": "1. Linearity\nAssess before fitting the linear regression model by making a scatterplot of \\(x\\) vs. \\(y\\):\n\n\n\n\n\n\nDoes there appear to be a linear relationship between diameter and volume?\n\nI would say yes"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#independence",
    "href": "slides/slides-23-slr-intro.html#independence",
    "title": "Introduction to Simple Linear Regression",
    "section": "2. Independence",
    "text": "2. Independence\nAssess before fitting the linear regression model by understanding how your data were sampled.\n\nThe cherry data do not explicitly say that the trees were randomly sampled, but it might be a reasonable assumption\n\n\nAn example where independence is violated:\n\n\n\n\n\n\n\n\nHere, the data are a time series, where observation at time point \\(i\\) depends on the observation at time \\(i-1\\).\n\nSuccessive/consecutive observations are highly correlated"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#nearly-normal-residuals",
    "href": "slides/slides-23-slr-intro.html#nearly-normal-residuals",
    "title": "Introduction to Simple Linear Regression",
    "section": "3. Nearly normal residuals",
    "text": "3. Nearly normal residuals\nAssess after fitting the model by obtaining residuals and making a histogram.\n\nRemember, residuals are \\(\\hat{y}_{i} - y_{i}\\)\n\n\n\n\n\ncherry |>\n  mutate(volume_hat = -36.94 + 5.07*diam) |>\n  mutate(residual = volume_hat - volume) \n\n\n\n\n\n \n  \n    diam \n    volume \n    volume_hat \n    residual \n  \n \n\n  \n    8.3 \n    10.3 \n    5.108 \n    -5.192 \n  \n  \n    8.6 \n    10.3 \n    6.628 \n    -3.672 \n  \n  \n    8.8 \n    10.2 \n    7.641 \n    -2.559 \n  \n  \n    10.5 \n    16.4 \n    16.253 \n    -0.147 \n  \n  \n    10.7 \n    18.8 \n    17.266 \n    -1.534 \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo the residuals appear approximately Normal?\n\nI think so!"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#equal-variance",
    "href": "slides/slides-23-slr-intro.html#equal-variance",
    "title": "Introduction to Simple Linear Regression",
    "section": "4. Equal variance",
    "text": "4. Equal variance\nAssess after fitting the model by examining a residual plot and looking for patterns.\n\n\nA good residual plot:\n\n\n\n\n\n\nA bad residual plot:\n\n\n\n\n\n\n\nWe usually have a horizontal line at 0 to compare residuals to"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#fitting-the-model",
    "href": "slides/slides-23-slr-intro.html#fitting-the-model",
    "title": "Introduction to Simple Linear Regression",
    "section": "Fitting the model",
    "text": "Fitting the model\n\n\n\nAt this point, it is time to actually fit our model\n\\[\n\\text{volume} = \\beta_{0} + \\beta_{1} \\text{diameter} +\\epsilon\n\\]\n\nAfter fitting the model, we get the following estimates: \\(b_{0}= -36.94\\) and \\(b_{1} = 5.07\\). So our fitted model is:\n\n\n\\[\n\\widehat{\\text{volume}} = -36.94 + 5.07 \\times \\text{diameter}\n\\]\n\nRemember: the “hat” denotes an estimated/fitted value!\n\n\n\nWe will soon see how \\(b_{0}\\) and \\(b_{1}\\) are calculated and how to interpret them\nThe next two checks can only occur after fitting the model."
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#equal-variance-cont.",
    "href": "slides/slides-23-slr-intro.html#equal-variance-cont.",
    "title": "Introduction to Simple Linear Regression",
    "section": "4. Equal variance (cont.)",
    "text": "4. Equal variance (cont.)\nLet’s examine the residual plot of our fitted model for the cherry data:\n\n\n\n\n\n\nBased on this plot, I would say there is a definite pattern in the residuals and equal variance condition is not perfectly met.\n\nSome of the variability in the errors appear related to diameter"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#parameter-estimates",
    "href": "slides/slides-23-slr-intro.html#parameter-estimates",
    "title": "Introduction to Simple Linear Regression",
    "section": "Parameter estimates",
    "text": "Parameter estimates\n\nLike in previous topics, we have to estimate the parameters using data\nWe want to estimate \\(\\beta_{0}\\) and \\(\\beta_{1}\\) using the \\((x_{i}, y_{i})\\)\n\nIn practice, we let software do this for us\n\nHowever, we can derive the least-squares estimates using properties of the least-squares line"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#estimating-slope-and-intercept",
    "href": "slides/slides-23-slr-intro.html#estimating-slope-and-intercept",
    "title": "Introduction to Simple Linear Regression",
    "section": "Estimating slope and intercept",
    "text": "Estimating slope and intercept\n\n\nFirst obtain \\(b_{1}\\):\n\n\\[\nb_{1} =\\frac{s_{y}}{s_{x}} R\n\\]\nwhere:\n\n\n\\(s_{x}\\) and \\(s_{y}\\) are the sample standard deviations of the explanatory and response variables\n\\(R\\) is the correlation between \\(x\\) and \\(y\\)\n\n\n\nThen obtain \\(b_{0}\\):\n\n\\[b_{0} = \\bar{y} - b_{1} \\bar{x}\\] where\n\n\n\\(\\bar{y}\\) is the sample mean of the response variable\n\\(x\\) is the sample mean of the explanatory variable\n\n\n\n\n\n\nTake STAT 0211 or 0311 to see where these formulas come from!"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#fitting-cherry-model",
    "href": "slides/slides-23-slr-intro.html#fitting-cherry-model",
    "title": "Introduction to Simple Linear Regression",
    "section": "Fitting cherry model",
    "text": "Fitting cherry model\nVerify estimates \\(b_{0} = -36.94\\) and \\(b_{1} = 5.07\\) from our model for the cherry data:\n\ncherry |>\n  pivot_longer(cols = c(diam, volume), names_to = \"variable\", values_to = \"val\") |>\n  select(-height) |>\n  group_by(variable) |>\n  summarise(mean = mean(val), s = sd(val)) \n\n\n\n\n\n \n  \n    variable \n    mean \n    s \n  \n \n\n  \n    diam \n    13.248 \n    3.138 \n  \n  \n    volume \n    30.171 \n    16.438 \n  \n\n\n\n\n\n\nR <- cor(cherry$diam, cherry$volume)\nR\n\n[1] 0.9671194\n\n\n\n\n\n\n\n\n\nSet-up the calculations:\n\n\\(b_{1} = \\frac{s_{y}}{s_{x}} R\\)\n\\(b_{0} = \\bar{y} -b_{1} \\bar{x}\\)\n\n\n\n\n\n\\(b_{1} = \\frac{16.438}{3.138} \\times 0.967 = 5.07\\)\n\\(b_{0} = 30.171 - 5.07 \\times 13.248 = -36.94\\)"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#fitting-cherry-model-by-hand",
    "href": "slides/slides-23-slr-intro.html#fitting-cherry-model-by-hand",
    "title": "Introduction to Simple Linear Regression",
    "section": "Fitting cherry model (by hand)",
    "text": "Fitting cherry model (by hand)\nVerify estimates \\(b_{0} = -36.94\\) and \\(b_{1} = 5.07\\) from our model for the cherry data:\n\ncherry |>\n  pivot_longer(cols = c(diam, volume), names_to = \"variable\", values_to = \"val\") |>\n  select(-height) |>\n  group_by(variable) |>\n  summarise(mean = mean(val), s = sd(val)) \n\n\n\n\n\n \n  \n    variable \n    mean \n    s \n  \n \n\n  \n    diam \n    13.248 \n    3.138 \n  \n  \n    volume \n    30.171 \n    16.438 \n  \n\n\n\n\n\n\nR <- cor(cherry$diam, cherry$volume)\nR\n\n[1] 0.9671194\n\n\n\n\n\n\n\n\n\nSet-up the calculations:\n\n\\(b_{1} = \\frac{s_{y}}{s_{x}} R\\)\n\\(b_{0} = \\bar{y} -b_{1} \\bar{x}\\)\n\n\n\n\n\n\\(b_{1} = \\frac{16.438}{3.138} \\times 0.967 = 5.07\\)\n\\(b_{0} = 30.171 - 5.07 \\times 13.248 = -36.94\\)"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html#residual-cont.",
    "href": "slides/slides-23-slr-intro.html#residual-cont.",
    "title": "Introduction to Simple Linear Regression",
    "section": "Residual (cont.)",
    "text": "Residual (cont.)\n\n\n\n\n\n\nResidual values for the three highlighted observations:\n\n\n\n\n \n  \n    x \n    y \n    y_hat \n    residual \n  \n \n\n  \n    -2.991 \n    2.481 \n    -0.130 \n    -2.611 \n  \n  \n    -1.005 \n    -1.302 \n    0.691 \n    1.994 \n  \n  \n    3.990 \n    3.929 \n    2.757 \n    -1.172"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#linear-regression-model",
    "href": "slides/slides-24-slr-interpretation.html#linear-regression-model",
    "title": "Introduction to Simple Linear Regression",
    "section": "Linear regression model",
    "text": "Linear regression model\n\\[\ny = \\beta_{0} + \\beta_{1} x + \\epsilon\n\\]\n\nWe have two variables:\n\n\\(y\\) is response variable. Must be continuous numerical.\n\\(x\\) is explanatory variable, also called the predictor variable\n\nCan be numerical or categorical\n\n\n\\(\\beta_{0}\\) and \\(\\beta_{1}\\) are the model parameters (intercept and slope)\n\nEstimated using the data, with point estimates \\(b_{0}\\) and \\(b_{1}\\)\n\n\\(\\epsilon\\) (epsilon) represents the error\n\nAccounts for variability: we do not expect all data to fall perfectly on the line!\nSometimes we drop the \\(\\epsilon\\) term for convenience"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#linear-relationship",
    "href": "slides/slides-24-slr-interpretation.html#linear-relationship",
    "title": "Introduction to Simple Linear Regression",
    "section": "Linear relationship",
    "text": "Linear relationship\nSuppose we have the following data:\n\n\n\n\n\n\nObservations won’t fall exactly on a line, but do fall around a straight line, so maybe a linear relationship makes sense!"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#fitted-values",
    "href": "slides/slides-24-slr-interpretation.html#fitted-values",
    "title": "Introduction to Simple Linear Regression",
    "section": "Fitted values",
    "text": "Fitted values\nSuppose we have some specific estimates \\(b_0\\) and \\(b_{1}\\). We could fit the linear relationship using these values as:\n\\[\n\\hat{y} = b_{0} + b_{1} x\n\\]\n\nThe hat on \\(y\\) signifies that this is an estimate: the estimated/fitted value of \\(y\\) given these specific values of \\(x\\), \\(b_{0}\\) and \\(b_{1}\\)\n\nWe observe \\(y\\), but can obtain a corresponding estimate \\(\\hat{y}\\)\n\nNote that the fitted value is obtained without the error"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#fitted-values-cont.",
    "href": "slides/slides-24-slr-interpretation.html#fitted-values-cont.",
    "title": "Introduction to Simple Linear Regression",
    "section": "Fitted values (cont.)",
    "text": "Fitted values (cont.)\n\n\n\n\n\n\nSuppose our estimated line is the yellow one\nEvery observed value \\(y_{i}\\) has a corresponding fitted value \\(\\hat{y}_{i}\\); the above plot just shows three specific examples"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#residual",
    "href": "slides/slides-24-slr-interpretation.html#residual",
    "title": "Introduction to Simple Linear Regression",
    "section": "Residual",
    "text": "Residual\nResiduals are the remaining variation in the data after fitting a model.\n\\[\n\\text{data} = \\text{fit} + \\text{residual}\n\\]\n\nFor each observation \\(i\\), we obtain residual \\(e_{i}\\) via:\n\n\n\\[y_{i} = \\hat{y}_{i} + e_{i} \\quad \\Rightarrow \\quad e_{i} = \\hat{y}_{i} - y_{i}\\]\n\n\nResidual = difference between observed and expected\nSince each observation has a fitted value, each observation has a residual\n\nIn the linear regression case, the residual is indicated by the vertical dashed line\n\nWhat is the ideal value for a residual?"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#residual-cont.",
    "href": "slides/slides-24-slr-interpretation.html#residual-cont.",
    "title": "Introduction to Simple Linear Regression",
    "section": "Residual (cont.)",
    "text": "Residual (cont.)\n\n\n\n\n\n\nResidual values for the three highlighted observations:\n\n\n\n\n \n  \n    x \n    y \n    y_hat \n    residual \n  \n \n\n  \n    -2.991 \n    2.481 \n    -0.130 \n    -2.611 \n  \n  \n    -1.005 \n    -1.302 \n    0.691 \n    1.994 \n  \n  \n    3.990 \n    3.929 \n    2.757 \n    -1.172"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#residual-plot",
    "href": "slides/slides-24-slr-interpretation.html#residual-plot",
    "title": "Introduction to Simple Linear Regression",
    "section": "Residual plot",
    "text": "Residual plot\n\nResiduals are very helpful in evaluating how well a model fits a set of data\nResidual plot: original \\(x\\) values plotted against their corresponding residuals on \\(y\\)-axis"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#residual-plot-cont.",
    "href": "slides/slides-24-slr-interpretation.html#residual-plot-cont.",
    "title": "Introduction to Simple Linear Regression",
    "section": "Residual plot (cont.)",
    "text": "Residual plot (cont.)\nResidual plots can be useful for identifying characteristics/patterns that remain in the data even after fitting a model.\n\n\nJust because you fit a model to data, does not mean the model is a good fit!\n\n\n\n\n\n\n\n\n\nCan you identify any patterns remaining in the residuals?\n\nSorry! The residuals shown here are taken as \\(y_{i} - \\hat{y}_{i}\\)!"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#describing-linear-relationships",
    "href": "slides/slides-24-slr-interpretation.html#describing-linear-relationships",
    "title": "Introduction to Simple Linear Regression",
    "section": "Describing linear relationships",
    "text": "Describing linear relationships\nDifferent data may exhibit different strength of linear relationships:\n\n\n\n\n\n\nCan we quantify the strength of the linear relationship?"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#correlation",
    "href": "slides/slides-24-slr-interpretation.html#correlation",
    "title": "Introduction to Simple Linear Regression",
    "section": "Correlation",
    "text": "Correlation\n\nCorrelation is describes the strength of a linear relationship between two variables\n\nThe observed sample correlation is denoted by \\(R\\)\nFormula (not important): \\(R = \\frac{1}{n-1} \\sum_{i=1}^{n} \\left(\\frac{x_{i} - \\bar{x}}{s_x} \\right)\\left(\\frac{y_{i} - \\bar{y}}{s_y} \\right)\\)\n\n\n\n\n\nAlways takes a value between -1 and 1\n\n-1 = perfectly linear and negative\n1 = perfectly linear and positive\n0 = no linear relationship\n\nNonlinear trends, even when strong, sometimes produce correlations that do not reflect the strength of the relationship"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#different-lines",
    "href": "slides/slides-24-slr-interpretation.html#different-lines",
    "title": "Introduction to Simple Linear Regression",
    "section": "Different lines",
    "text": "Different lines\nThe following display the same set of 50 observations.\n\n\n\n\n\n\n\n\n\nWhich line would you say fits the data the best?\n\n\n\n\nThere are infinitely many choices of \\((b_{0}, b_{1})\\) that could be used to create a line\nWe want the BEST choice (i.e. the one that gives us the “line of best fit”)\n\n\nHow to define “best”?"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#line-of-best-fit",
    "href": "slides/slides-24-slr-interpretation.html#line-of-best-fit",
    "title": "Introduction to Simple Linear Regression",
    "section": "Line of best fit",
    "text": "Line of best fit\nOne way to define a “best” is to choose the specific values of \\((b_{0}, b_{1})\\) that minimize the total residuals across all \\(n\\) data points. Results in following possible criterion:\n\nLeast absolute criterion: minimize sum of residual magnitudes:\n\n\n\\[\n|e_{1} | + |e_{2}| + \\ldots + |e_{n}|\n\\]\n\n\n\nLeast squares criterion: minimize sum of squared residuals:\n\n\n\n\\[\ne_{1}^2 + e_{2}^2 +\\ldots + e_{n}^2\n\\]\n\n\nThe choice of \\((b_{0}, b_{1})\\) that satisfy least squares criterion yields the least squares line, and will be our criterion for “best”\nOn previous slide, yellow line is the least squares line, whereas pink line is the least absolute line"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#linear-regression-model-1",
    "href": "slides/slides-24-slr-interpretation.html#linear-regression-model-1",
    "title": "Introduction to Simple Linear Regression",
    "section": "Linear regression model",
    "text": "Linear regression model\nRemember, our linear regression model is:\n\\[\ny = \\beta_{0} + \\beta_{1}x + \\epsilon\n\\]\n\nWhile not wrong, it can be good practice to be specific about an observation \\(i\\):\n\\[\ny_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\epsilon_{i}, \\qquad i = 1,\\ldots, n\n\\]\n\n\nHere, we are stating that each observation \\(i\\) has a specific:\n\nexplanatory variable value \\(x_{i}\\)\nresponse variable value \\(y_{i}\\)\nerror/randomness \\(\\epsilon_{i}\\)"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#conditions-for-the-least-squares-line-line",
    "href": "slides/slides-24-slr-interpretation.html#conditions-for-the-least-squares-line-line",
    "title": "Introduction to Simple Linear Regression",
    "section": "Conditions for the least squares line (LINE)",
    "text": "Conditions for the least squares line (LINE)\nLike when using CLT, we should check some conditions before saying a linear regression model is appropriate!\n\nAssume for now that \\(x\\) is continuous numerical.\n\n\nLinearity: data should show a linear trend between \\(x\\) and \\(y\\)\nIndependence: the observations \\(i\\) are independent of each other\n\ne.g. random sample\nNon-example: time-series data\n\nNormality/nearly normal residuals: the residuals should appear approximately Normal\n\nPossible violations: outliers, influential points (more on this later)\n\nEqual variability: variability of points around the least squares line remains roughly constant"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#running-example",
    "href": "slides/slides-24-slr-interpretation.html#running-example",
    "title": "Introduction to Simple Linear Regression",
    "section": "Running example",
    "text": "Running example\nWe will see how to check for these four LINE conditions using the cherry data from openintro.\n\n\n\n\n\n\n\ndiam\nvolume\n\n\n\n\n8.3\n10.3\n\n\n8.6\n10.3\n\n\n8.8\n10.2\n\n\n10.5\n16.4\n\n\n10.7\n18.8\n\n\n\n\n\n\n\nExplanatory variable \\(x\\): diam\nResponse variable \\(y\\): volume"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#linearity",
    "href": "slides/slides-24-slr-interpretation.html#linearity",
    "title": "Introduction to Simple Linear Regression",
    "section": "1. Linearity",
    "text": "1. Linearity\nAssess before fitting the linear regression model by making a scatterplot of \\(x\\) vs. \\(y\\):\n\n\n\n\n\n\nDoes there appear to be a linear relationship between diameter and volume?\n\nI would say yes"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#independence",
    "href": "slides/slides-24-slr-interpretation.html#independence",
    "title": "Introduction to Simple Linear Regression",
    "section": "2. Independence",
    "text": "2. Independence\nAssess before fitting the linear regression model by understanding how your data were sampled.\n\nThe cherry data do not explicitly say that the trees were randomly sampled, but it might be a reasonable assumption\n\n\nAn example where independence is violated:\n\n\n\n\n\n\n\n\nHere, the data are a time series, where observation at time point \\(i\\) depends on the observation at time \\(i-1\\).\n\nSuccessive/consecutive observations are highly correlated"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#fitting-the-model",
    "href": "slides/slides-24-slr-interpretation.html#fitting-the-model",
    "title": "Introduction to Simple Linear Regression",
    "section": "Fitting the model",
    "text": "Fitting the model\n\n\n\nAt this point, it is time to actually fit our model\n\\[\n\\text{volume} = \\beta_{0} + \\beta_{1} \\text{diameter} +\\epsilon\n\\]\n\nAfter fitting the model, we get the following estimates: \\(b_{0}= -36.94\\) and \\(b_{1} = 5.07\\). So our fitted model is:\n\n\n\\[\n\\widehat{\\text{volume}} = -36.94 + 5.07 \\times \\text{diameter}\n\\]\n\nRemember: the “hat” denotes an estimated/fitted value!\n\n\n\nWe will soon see how \\(b_{0}\\) and \\(b_{1}\\) are calculated and how to interpret them\nThe next two checks can only occur after fitting the model."
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#nearly-normal-residuals",
    "href": "slides/slides-24-slr-interpretation.html#nearly-normal-residuals",
    "title": "Introduction to Simple Linear Regression",
    "section": "3. Nearly normal residuals",
    "text": "3. Nearly normal residuals\nAssess after fitting the model by obtaining residuals and making a histogram.\n\nRemember, residuals are \\(\\hat{y}_{i} - y_{i}\\)\n\n\n\n\n\ncherry |>\n  mutate(volume_hat = -36.94 + 5.07*diam) |>\n  mutate(residual = volume_hat - volume) \n\n\n\n\n\n \n  \n    diam \n    volume \n    volume_hat \n    residual \n  \n \n\n  \n    8.3 \n    10.3 \n    5.108 \n    -5.192 \n  \n  \n    8.6 \n    10.3 \n    6.628 \n    -3.672 \n  \n  \n    8.8 \n    10.2 \n    7.641 \n    -2.559 \n  \n  \n    10.5 \n    16.4 \n    16.253 \n    -0.147 \n  \n  \n    10.7 \n    18.8 \n    17.266 \n    -1.534 \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo the residuals appear approximately Normal?\n\nI think so!"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#equal-variance",
    "href": "slides/slides-24-slr-interpretation.html#equal-variance",
    "title": "Introduction to Simple Linear Regression",
    "section": "4. Equal variance",
    "text": "4. Equal variance\nAssess after fitting the model by examining a residual plot and looking for patterns.\n\n\nA good residual plot:\n\n\n\n\n\n\nA bad residual plot:\n\n\n\n\n\n\n\nWe usually have a horizontal line at 0 to compare residuals to"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#equal-variance-cont.",
    "href": "slides/slides-24-slr-interpretation.html#equal-variance-cont.",
    "title": "Introduction to Simple Linear Regression",
    "section": "4. Equal variance (cont.)",
    "text": "4. Equal variance (cont.)\nLet’s examine the residual plot of our fitted model for the cherry data:\n\n\n\n\n\n\nBased on this plot, I would say the equal variance condition is not perfectly met.\n\nSome of the variability in the errors appear related to diameter"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#parameter-estimates",
    "href": "slides/slides-24-slr-interpretation.html#parameter-estimates",
    "title": "SLR coefficient estimates",
    "section": "Parameter estimates",
    "text": "Parameter estimates\n\nLike in previous topics, we have to estimate the parameters using data\nWe want to estimate \\(\\beta_{0}\\) and \\(\\beta_{1}\\) using the \\((x_{i}, y_{i})\\)\n\nIn practice, we let software do this for us\n\nHowever, we can derive the least-squares estimates using properties of the least-squares line"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#estimating-slope-and-intercept",
    "href": "slides/slides-24-slr-interpretation.html#estimating-slope-and-intercept",
    "title": "SLR coefficient estimates",
    "section": "Estimating slope and intercept",
    "text": "Estimating slope and intercept\n\n\nFirst obtain \\(b_{1}\\):\n\n\\[\nb_{1} =\\frac{s_{y}}{s_{x}} R\n\\]\nwhere:\n\n\n\\(s_{x}\\) and \\(s_{y}\\) are the sample standard deviations of the explanatory and response variables\n\\(R\\) is the correlation between \\(x\\) and \\(y\\)\n\n\n\nThen obtain \\(b_{0}\\):\n\n\\[b_{0} = \\bar{y} - b_{1} \\bar{x}\\] where\n\n\n\\(\\bar{y}\\) is the sample mean of the response variable\n\\(x\\) is the sample mean of the explanatory variable\n\n\n\n\n\n\nTake STAT 0211 or 0311 to see where these formulas come from!"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#fitting-cherry-model-by-hand",
    "href": "slides/slides-24-slr-interpretation.html#fitting-cherry-model-by-hand",
    "title": "SLR coefficient estimates",
    "section": "Fitting cherry model (by hand)",
    "text": "Fitting cherry model (by hand)\n\n\n\nVerify estimates \\(b_{0} = -36.94\\) and \\(b_{1} = 5.07\\) from our model for the cherry data:\n\n\n\ncherry |>\n  pivot_longer(cols = c(diam, volume), \n               names_to = \"variable\", \n               values_to = \"val\") |>\n  select(-height) |>\n  group_by(variable) |>\n  summarise(mean = mean(val), s = sd(val)) \n\n\n\n\n\n \n  \n    variable \n    mean \n    s \n  \n \n\n  \n    diam \n    13.248 \n    3.138 \n  \n  \n    volume \n    30.171 \n    16.438 \n  \n\n\n\n\n\n\n\nR <- cor(cherry$diam, cherry$volume)\nR\n\n[1] 0.9671194\n\n\n\nWhat does this value of \\(R\\) tell us?\n\n\n\n\n\n\n\n\n\n\nSet-up the calculations:\n\n\\(b_{1} = \\frac{s_{y}}{s_{x}} R\\)\n\\(b_{0} = \\bar{y} -b_{1} \\bar{x}\\)\n\n\n\n\n\n\\(b_{1} = \\frac{16.438}{3.138} \\times 0.967 = 5.07\\)\n\\(b_{0} = 30.171 - 5.07 \\times 13.248 = -36.94\\)\nWhat do these numbers really mean?"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#housekeeping",
    "href": "slides/slides-24-slr-interpretation.html#housekeeping",
    "title": "SLR coefficient estimates",
    "section": "Housekeeping",
    "text": "Housekeeping\n\n\n\n\nNo TA hours tonight\nWill discuss details of Midterm 2 next week!\nRevisions for proposals due Saturday 11:59pm"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#intercept-interpretation",
    "href": "slides/slides-24-slr-interpretation.html#intercept-interpretation",
    "title": "SLR coefficient estimates",
    "section": "Intercept interpretation",
    "text": "Intercept interpretation\nOur fitted model is \\(\\hat{y} = b_{0} + b_{1}x\\).\n\nTo interpret the estimate of the intercept coefficient \\(b_{0}\\), simply plug in \\(x= 0\\):\n\n\\[\n\\hat{y} = b_{0} + b_{1} x = b_{0} + b_{1}(0) = b_{0}\n\\]\n\nSo, the intercept describes the average/expected value of the response variable \\(y\\) if \\(x=0\\)"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#intercept-in-cherry-model",
    "href": "slides/slides-24-slr-interpretation.html#intercept-in-cherry-model",
    "title": "SLR coefficient estimates",
    "section": "Intercept in cherry model",
    "text": "Intercept in cherry model\n\\[\n\\widehat{\\text{volume}} = -36.94 + 5.07 \\times \\text{diameter}\n\\]\n\nInterpretation of intercept in context: for a tree with a diameter of 0 inches, the expected volume would be -36.94 cubic feet\n\nThis interpretation is mathematically correct, but practically speaking is useless\n\nThe intercept’s interpretation only makes sense when a value of \\(x=0\\) for the explanatory variable is plausible!\n\nThis is typically not the case/relevant in many applications\nTrees with 0 diameter are not able to sampled"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#slope-interpretation",
    "href": "slides/slides-24-slr-interpretation.html#slope-interpretation",
    "title": "SLR coefficient estimates",
    "section": "Slope interpretation",
    "text": "Slope interpretation\n\nLet \\(\\hat{y}_{1}\\) be the estimated response for a given value of \\(x\\), so \\(\\hat{y}_{1} = b_{0} + b_{1} x\\)\nWhat happens when we increase \\(x\\) by 1?\nLet \\(\\hat{y}_{2}\\) be the estimated response for \\(x +1\\):\n\n\n\\[\n\\begin{align*}\n\\hat{y}_{2} &= b_{0} + b_{1} (x + 1)  \\\\\n&= \\color{orange}{b_{0} + b_{1}x}  + b_{1} \\\\\n&= \\color{orange}{\\hat{y}_{1}} + b_{1} \\Rightarrow \\\\\nb_{1} &= \\hat{y}_{2} - \\hat{y}_{1}\n\\end{align*}\n\\]\n\n\nInterpretation: for a 1 unit increase in the explanatory variable \\(x\\), we expect the response variable to change by \\(b_{1}\\) units"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#slope-in-cherry-model",
    "href": "slides/slides-24-slr-interpretation.html#slope-in-cherry-model",
    "title": "SLR coefficient estimates",
    "section": "Slope in cherry model",
    "text": "Slope in cherry model\n\\[\n\\widehat{\\text{volume}} = -36.94 + 5.07 \\times \\text{diameter}\n\\]\n\nInterpretation in context: for every 1 inch increase in diameter, we expect that volume of cherry trees to increase by 5.07 cubic feet"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#example-elmhurst",
    "href": "slides/slides-24-slr-interpretation.html#example-elmhurst",
    "title": "SLR coefficient estimates",
    "section": "Example: elmhurst",
    "text": "Example: elmhurst\nThe elmhurst dataset from openintro provides a random sample of 50 students gift aid for students at Elmhurst College.\n\nWe will examine the relationship between the family income of the student and the gift aid that student received (in $1000s)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAre the first two conditions of LINE satisfied?"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#example-elmhurst-cont.",
    "href": "slides/slides-24-slr-interpretation.html#example-elmhurst-cont.",
    "title": "SLR coefficient estimates",
    "section": "Example: elmhurst (cont.)",
    "text": "Example: elmhurst (cont.)\n\n\n\n\n\nWe run the model in R, and the output looks something like this:\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    24.319 \n    1.291 \n    18.831 \n    0 \n  \n  \n    family_income \n    -0.043 \n    0.011 \n    -3.985 \n    0 \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe values in the estimate column are our \\(b_{0}\\) and \\(b_{1}\\):\n\n\n\\(b_{0} =\\) ? and \\(b_{1} =\\) ?\nWhat do you think the second column is?\n\n\n\nWrite out our fitted model in context"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#example-elmhurst-model",
    "href": "slides/slides-24-slr-interpretation.html#example-elmhurst-model",
    "title": "SLR coefficient estimates",
    "section": "Example: elmhurst model",
    "text": "Example: elmhurst model\n\\[\n\\widehat{\\text{aid}} = 24.319 + -0.043 \\times \\text{family_income}\n\\]\n\nBefore we interpret the coefficients, we should verify that the linear model is appropriate for the data!\n\n\n\n\n\n\n\n\n\n\nDo you believe the last two conditions of LINE are satisfied?"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#example-elmhurst-interpretation",
    "href": "slides/slides-24-slr-interpretation.html#example-elmhurst-interpretation",
    "title": "SLR coefficient estimates",
    "section": "Example: elmhurst interpretation",
    "text": "Example: elmhurst interpretation\n\\[\n\\widehat{\\text{aid}} = 24.319 + -0.043 \\times \\text{family_income}\n\\]\n\n\n\n\nInterpret the slope in context\nInterpret the intercept in context\nIs the meaning of the intercept relevant?\n\n\n\n\nSlope: for every $1000 increase in family income, we expect that the student’s gift aid will decrease by $43.\nIntercept: for a student whose family income is $0, we expect that average amount of aid they will receive is $2.4319^{4}\nSince a family could have an income of $0, the intercept does seem relevant"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#words-of-caution",
    "href": "slides/slides-24-slr-interpretation.html#words-of-caution",
    "title": "SLR coefficient estimates",
    "section": "Words of caution",
    "text": "Words of caution\n\nThe estimates from the fitted model will always be imperfect\n\nThe linear equation is good at capturing trends, no individual outcome will be perfectly predicted\n\nDo not try to use the model for \\(x\\) values beyond the range of the observed \\(x\\)!\n\nThe true relationship between \\(x\\) and \\(y\\) is almost always much more complex than our simple line\nWe do not know how the relationship behaves outside our limited window"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#extrapolation",
    "href": "slides/slides-24-slr-interpretation.html#extrapolation",
    "title": "SLR coefficient estimates",
    "section": "Extrapolation",
    "text": "Extrapolation\nSuppose we would like to use our fitted model to estimate the expected gift aid for someone whose family income is $1,000,000:\n\n\nFind the estimated gift aid (careful with units)\n\n\n\\(\\widehat{\\text{aid}} = 24.319 + -0.043 \\times 1000 = -18.681\\)\nThis is ridiculous!\n\nThis is an example of extrapolation: using the model to estimate values outside the scope of the original data\n\nWe should never extrapolate!"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#describing-the-fit",
    "href": "slides/slides-24-slr-interpretation.html#describing-the-fit",
    "title": "SLR coefficient estimates",
    "section": "Describing the fit",
    "text": "Describing the fit\n\nRecall sample correlation \\(R\\) describes the linear relationship between variables \\(x\\) and \\(y\\)\nWe typically use the coefficient of determination or \\(R^2\\) (R-squared) to describe strength of linear fit\n\nDescribes amount of variation in \\(y\\) that is explained by predictor \\(x\\) in the least squares line\n\nIt turns out that \\(R^2\\) in SLR is exactly … \\(R\\) squared (i.e. the square of the sample correlation)\n\n\nWhat are the possible values of \\(R^2\\)? What are desirable values of \\(R^2\\)?"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#example-elmhurst-model-fit",
    "href": "slides/slides-24-slr-interpretation.html#example-elmhurst-model-fit",
    "title": "SLR coefficient estimates",
    "section": "Example: elmhurst model fit",
    "text": "Example: elmhurst model fit\n\n\n\n\nThe sample correlation between family income and aid is \\(R=\\) -0.499\nSo the coefficient of determination is \\(R^2 = (-0.499)^2 = 0.249\\)\n\nInterpretation: using a linear model, about 24.9% of the variability in aid received by the student is explained by family income"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#categorical-predictor-with-two-levels",
    "href": "slides/slides-24-slr-interpretation.html#categorical-predictor-with-two-levels",
    "title": "SLR coefficient estimates",
    "section": "Categorical predictor with two levels",
    "text": "Categorical predictor with two levels\n\nRemember that the different groupings/categories of categorical variables are called levels\n\nNow assume that \\(x\\) is categorical with two levels\n\nRunning example: the possum data from openintro\n\nResponse variable: tail_l (tail length in cm)\nExplanatory variable: pop (either Vic or other)\n\nMaybe we would think to write our regression as\n\n\n\\[\\text{tail length} = \\beta_{0} + \\beta_{1} \\text{pop} + \\epsilon\\]\n\n\n\nWhy doesn’t this work?\n\n\nFunctions require a numerical input!"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#indicator-variables",
    "href": "slides/slides-24-slr-interpretation.html#indicator-variables",
    "title": "SLR coefficient estimates",
    "section": "Indicator variables",
    "text": "Indicator variables\nWe need a mechanism to convert the categorical levels into numerical form!\n\n\n\nThis is achieved through an indicator variable which takes the value 1 for one specific level and the value 0 otherwise:\n\n\n\\[\n\\text{pop_other} = \\begin{cases}\n0 & \\text{ if  pop = Vic} \\\\\n1 & \\text{ if  pop = other}\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n \n  \n    tail_l \n    pop \n    pop_new \n  \n \n\n  \n    38.0 \n    other \n    1 \n  \n  \n    34.0 \n    Vic \n    0 \n  \n  \n    36.0 \n    Vic \n    0 \n  \n  \n    36.5 \n    Vic \n    0 \n  \n  \n    41.5 \n    other \n    1 \n  \n\n\n\n\n\n\n\n\n\nThe level that corresponds to 0 is called the base level\n\nSo Vic is the base level\nChoosing which level is the base level can sometimes be important"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#example-possum-model",
    "href": "slides/slides-24-slr-interpretation.html#example-possum-model",
    "title": "SLR coefficient estimates",
    "section": "Example: possum model",
    "text": "Example: possum model\nThis yields the SLR model\n\\[\\text{tail length} = \\beta_{0} + \\beta_{1} \\text{pop_other} + \\epsilon\\]\n\nOur estimates are as follows:\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    35.935 \n    0.253 \n    142.065 \n    0 \n  \n  \n    popother \n    1.927 \n    0.339 \n    5.690 \n    0 \n  \n\n\n\n\n\n\n\n\nWrite out the equation of our fitted model"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#interpretation-of-coefficients",
    "href": "slides/slides-24-slr-interpretation.html#interpretation-of-coefficients",
    "title": "SLR coefficient estimates",
    "section": "Interpretation of coefficients",
    "text": "Interpretation of coefficients\nOur fitted model is:\n\\[\\widehat{\\text{tail length}} = 35.935 + 1.927 \\times \\text{pop_other}\\]\n\nLet’s interpret the intercept by plugging in \\(0\\) for the explanatory variable:\n\n\n\\[\\widehat{\\text{tail length}} = 35.935 + 1.927\\times 0 = 35.935\\]\n\n\nBut wait, when is \\(\\text{pop_other} = 0\\)? When the possum is from Victoria!\nSo when \\(x\\) is categorical, the interpretation of \\(b_{0}\\) is the expected value of the response variable for the base level of \\(x\\)\n\n\n\n\n\nInterpret \\(b_{0}\\) in context\n\n\n\n\nThe expected tail length of possums from Victoria is 35.935 cm"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#intercept-for-categorical-x",
    "href": "slides/slides-24-slr-interpretation.html#intercept-for-categorical-x",
    "title": "SLR coefficient estimates",
    "section": "Intercept for categorical \\(x\\)",
    "text": "Intercept for categorical \\(x\\)\nOur fitted model is:\n\\[\\widehat{\\text{tail length}} = 35.935 + 1.927 \\times \\text{pop_other}\\]\n\nLet’s interpret the intercept by plugging in \\(0\\) for the explanatory variable:\n\n\n\\[\\widehat{\\text{tail length}} = 35.935 + 1.927\\times 0 = 35.935\\]\n\n\nBut wait, when is \\(\\text{pop_other} = 0\\)? When the possum is from Victoria!\n\nSo when \\(x\\) is categorical, the interpretation of \\(b_{0}\\) is the expected value of the response variable for the base level of \\(x\\)\n\n\nInterpret \\(b_{0}\\) in context\n\n\nThe expected tail length of possums from Victoria is 35.935 cm"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#slope-for-categorical-x",
    "href": "slides/slides-24-slr-interpretation.html#slope-for-categorical-x",
    "title": "SLR coefficient estimates",
    "section": "Slope for categorical \\(x\\)",
    "text": "Slope for categorical \\(x\\)\n\\[\\widehat{\\text{tail length}} = 35.935 + 1.927\\times \\text{pop_other}\\]\n\\[\n\\text{pop_other} = \\begin{cases}\n0 & \\text{ if  pop = Vic} \\\\\n1 & \\text{ if  pop = other}\n\\end{cases}\n\\]\n\nRemember, the slope coefficient is interpreted as the expected change in \\(y\\) for a one unit increase in \\(x\\)\n\nWhat does it mean for the indicator variable to increase by one unit here?\n\n\n\\(\\text{pop_other}\\) increases by one unit by going from 0 to 1. This corresponds to a pop value of “other”\n\n\nWhen \\(x\\) is categorical, the interpretation of \\(b_{1}\\) is the expected change in \\(y\\) when moving from the base level to the non-base level\n\n\nTry interpreting \\(b_{1}\\) in context!"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#slope-for-categorical-x-cont.",
    "href": "slides/slides-24-slr-interpretation.html#slope-for-categorical-x-cont.",
    "title": "SLR coefficient estimates",
    "section": "Slope for categorical \\(x\\) (cont.)",
    "text": "Slope for categorical \\(x\\) (cont.)\n\\[\\widehat{\\text{tail length}} = 35.935 + 1.927\\times \\text{pop_other}\\]\n\\[\n\\text{pop_other} = \\begin{cases}\n0 & \\text{ if  pop = Vic} \\\\\n1 & \\text{ if  pop = other}\n\\end{cases}\n\\]\n\nInterpretation of slope: possums from outside of Victoria are expected to have tail lengths about 1.927 cm longer than possums from Victoria\nNote: interpretations for \\(b_{0}\\) and \\(b_{1}\\) for categorical \\(x\\) are the same as for numerical \\(x\\), but they have more specific/nuanced interpretations when placed in context"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#remark",
    "href": "slides/slides-24-slr-interpretation.html#remark",
    "title": "SLR coefficient estimates",
    "section": "Remark",
    "text": "Remark\n\nWhen \\(x\\) is categorical, the LINE conditions still need to hold\nWhen \\(x\\) only has two levels, the Linearity assumption will always be satisfied\nWe need to evaluate Nearly normal residuals and Equal variance for each level:"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#possum-example-cont.",
    "href": "slides/slides-24-slr-interpretation.html#possum-example-cont.",
    "title": "SLR coefficient estimates",
    "section": "possum example (cont.)",
    "text": "possum example (cont.)"
  },
  {
    "objectID": "live_code/slr1.html",
    "href": "live_code/slr1.html",
    "title": "Linear regression in R",
    "section": "",
    "text": "library(tidyverse)\nlibrary(openintro)\n\n\nlm() function\nThe function that will obtain the coefficients for the least-squares line is the lm() function. The syntax is as follows:\nlm(response ~ explanatory, data)\n\n\nExample 1: cherry data with continuous \\(x\\)\nLet’s once again consider the cherry data from openintro, where we want to fit the model\n\\[\\text{volume} = \\beta_{0} + \\beta_{1} \\times \\text{diameter} + \\epsilon\\]\nWhat does this look like in R?\n\nlm(volume ~ diam, data = cherry)\n\nNote that the variables have to be spelled as they appear in the data frame!\nThe output from this line of code is:\n\n\n\nCall:\nlm(formula = volume ~ diam, data = cherry)\n\nCoefficients:\n(Intercept)         diam  \n    -36.943        5.066  \n\n\nThis isn’t the most informative of output, so what we will do is use an additional function called summary() that will give us much more information!\nWe will first store the output from lm() as a variable called cherry_lm:\n\ncherry_lm <- lm(volume ~ diam, data = cherry)\n\nThen we will use the summary() function and pass in the linear model:\n\nsummary(cherry_lm)\n\n\nCall:\nlm(formula = volume ~ diam, data = cherry)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.065 -3.107  0.152  3.495  9.587 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -36.9435     3.3651  -10.98 7.62e-12 ***\ndiam          5.0659     0.2474   20.48  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.252 on 29 degrees of freedom\nMultiple R-squared:  0.9353,    Adjusted R-squared:  0.9331 \nF-statistic: 419.4 on 1 and 29 DF,  p-value: < 2.2e-16\n\n\nThere’s a lot more information here! We can now see the \\(b_{0}\\) and \\(b_{1}\\) estimates, along with some extra information. In particular, the “Multiple R-squared” quantity is the coefficient of determination \\(R^2\\)!\n\n\nExample 2: possum data with categorical \\(x\\)\nThe nice thing about lm() is that it will automatically convert categorical variables to indicator variables!\nLet’s re-visit the possum model:\n\\[\\text{tail length} = \\beta_{0} + \\beta_{1} \\times \\text{pop-other} + \\epsilon\\] \\[\n\\text{pop-other} = \\begin{cases}\n0 & \\text{ if  pop = Vic} \\\\\n1 & \\text{ if  pop = other}\n\\end{cases}\n\\]\nWe can use the lm() function just as before:\n\npossum_lm <- lm(tail_l ~ pop, data = possum)\n\nSometimes we just want the coefficients. The coef() function will output the coefficients as a vector. These can be nice for reproducibility and in-line code:\n\ncoef(possum_lm)\n\n(Intercept)    popother \n  35.934783    1.927286"
  },
  {
    "objectID": "slides/slides-23-slr-intro.html",
    "href": "slides/slides-23-slr-intro.html",
    "title": "Introduction to Simple Linear Regression",
    "section": "",
    "text": "Homework 7 due tonight!\nLast problem set is assigned today! Atypical due date: Wednesday 11/13"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html",
    "href": "slides/slides-24-slr-interpretation.html",
    "title": "SLR coefficient estimates",
    "section": "",
    "text": "No TA hours tonight\nWill discuss details of Midterm 2 next week!\nRevisions for proposals due Saturday 11:59pm"
  },
  {
    "objectID": "slides/slides-24-slr-interpretation.html#assessing-linear-fit",
    "href": "slides/slides-24-slr-interpretation.html#assessing-linear-fit",
    "title": "SLR coefficient estimates",
    "section": "Assessing linear fit",
    "text": "Assessing linear fit\n\nWhen \\(x\\) is categorical, the LINE conditions still need to hold\nWhen \\(x\\) only has two levels, the Linearity assumption will always be satisfied\nWe need to evaluate Nearly normal residuals and Equal variance for each level:"
  },
  {
    "objectID": "practice_probs/practice-24-slr.html",
    "href": "practice_probs/practice-24-slr.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "\\((^*)\\) Researchers studying anthropometry collected body girth measurements and skeletal diameter measurements, as well as age, weight, height and gender for 507 physically active individuals. They are interested in the relationship between height (cm) and shoulder girth (cm). They would like create linear regression model for height using shoulder girth as the predictor.\n\n\n\n\n\n\nThe mean shoulder girth is 107.20 cm with a standard deviation of 10.37 cm. The mean height is 171.14 cm with a standard deviation of 9.41 cm. The correlation between height and shoulder girth is 0.67.\n\nWrite the equation of the fitted regression line for predicting height.\nInterpret the slope and intercept in context.\nCalculate the \\(R^2\\) of the regression line and interpret it in context.\nA one year old has a shoulder girth of 56 cm. Would it be appropriate to use this linear model to predict the height of this child? If so, obtain the predicted height. If not, explain why not.\n\n2. Data on a random sample of 100 births for babies in North Carolina were obtained. We’d like to fit a linear regression model for the birth weight of the baby in pounds (weight) based on if the baby was born premature (premature). The premature variable is categorical with two levels: “premie” (if baby was born premature) and “full term” (if the baby was not born premature).\nWe run the model in R. The estimated coefficients are as follows:\n\nbirths_lm <- lm(weight ~ premature, data = births)\ncoef(births_lm)\n\n    (Intercept) prematurepremie \n       7.426357       -2.716833 \n\n\n\nWhich level of the variable premature is the base level? How can you tell?\nWrite out the fitted model for estimated birth weight of a baby.\nInterpret the slope and intercept of the model in context.\nWhat is the estimated weight of a baby born premature?"
  },
  {
    "objectID": "slides/slides-25-slr-inference.html#housekeeping",
    "href": "slides/slides-25-slr-inference.html#housekeeping",
    "title": "Inference in SLR",
    "section": "Housekeeping",
    "text": "Housekeeping\n\n\n\n\nLast set of homework problems are released today!\nOffice hours changed this week:\n\nToday: 2-3pm only\nWednesday 4-5pm\nFriday: cancelled, moved to next week before midterm"
  },
  {
    "objectID": "slides/slides-25-slr-inference.html#variability-of-coefficient-estimates",
    "href": "slides/slides-25-slr-inference.html#variability-of-coefficient-estimates",
    "title": "Inference in SLR",
    "section": "Variability of coefficient estimates",
    "text": "Variability of coefficient estimates\n\nRemember, a linear regression is fit using a sample of data\nDifferent samples from the same population will yield different point estimates of \\((b_{0}, b_{1})\\)\n\nI will generate 30 data points under the following model: \\(y = 1 + 0.5x+\\epsilon\\)\n\nHow? Randomly generate some \\(x\\) and \\(\\epsilon\\) values and then plug into model to get corresponding \\(y\\)\n\nFit SLR to these \\((x,y)\\) data, and obtain estimates \\((b_{0}, b_{1})\\)\nRepeat this 50 times"
  },
  {
    "objectID": "slides/slides-25-slr-inference.html#variability-of-coefficient-estimates-1",
    "href": "slides/slides-25-slr-inference.html#variability-of-coefficient-estimates-1",
    "title": "Inference in SLR",
    "section": "Variability of coefficient estimates",
    "text": "Variability of coefficient estimates"
  },
  {
    "objectID": "slides/slides-25-slr-inference.html#what-are-we-interested-in",
    "href": "slides/slides-25-slr-inference.html#what-are-we-interested-in",
    "title": "Inference in SLR",
    "section": "What are we interested in?",
    "text": "What are we interested in?\nRemember: we fit SLR to understand how \\(x\\) is (linearly) related to \\(y\\):\n\\[\ny = \\beta_{0} + \\beta_{1} x + \\epsilon\n\\]\n\n\nWhat would a value of \\(\\beta_{1} = 0\\) mean?\n\n\nIf \\(\\beta_{1} = 0\\), then the effect of \\(x\\) disappears and there is in fact no linear relationship between \\(x\\) and \\(y\\)\n\nWe don’t know \\(\\beta_{1}\\), so let’s look at estimate \\(b_{1}\\):\n\n\nIs an estimate of \\(b_{1}= 0\\) a convincing evidence of no relationship? What if \\(b_{1} = 0.1\\)?\n\nIt depends! We saw that \\(b_{1}\\) varies by sample! So let’s perform inference for \\(\\beta_{1}\\)"
  },
  {
    "objectID": "slides/slides-25-slr-inference.html#running-example-teaching-evaluations",
    "href": "slides/slides-25-slr-inference.html#running-example-teaching-evaluations",
    "title": "Inference in SLR",
    "section": "Running example: teaching evaluations",
    "text": "Running example: teaching evaluations\nData on 463 courses at UT Austin were obtained to answer the question: “What factors explain differences in instructor teaching evaluation scores?”\n\nOne hypothesis was that more attractive instructors receive better teaching evaluations\nWe will look at the variables:\n\nscore: course instructor’s average teaching score, where average is calculated from all students in that course. Scores ranged from 1-5, with 1 being lowest.\nbty_avg: course instructor’s average “beauty” score, where average is calculated from six student evaluations of “beauty”. Scores ranged from 1-10, with 1 being lowest.\n\n\n\n\n\nDoes this line really have a non-zero slope?"
  },
  {
    "objectID": "slides/slides-25-slr-inference.html#hypothesis-test-for-slope",
    "href": "slides/slides-25-slr-inference.html#hypothesis-test-for-slope",
    "title": "Inference in SLR",
    "section": "Hypothesis test for slope",
    "text": "Hypothesis test for slope\n\nWe have the following hypotheses:\n\n\\(H_{0}: \\beta_{1} = 0\\): the true linear model has slope zero\n\\(H_{A}: \\beta_{1} \\neq 0\\): the true linear model has a non-zero slope. An instructor’s average beauty score is predictive of their average teaching evaluation score.\n\nTo assess, we do what we usually do:\n\nCheck if methods are appropriate\nIf so: obtain an estimate, identify/estimate standard error of the estimate, find an appropriate test statistic, and calculate p-value\n\n\nThe output from lm() actually does all of this for us, but we will see how the test statistic and p-value are calculated!"
  },
  {
    "objectID": "slides/slides-25-slr-inference.html#teaching-evaluations-model-assessment",
    "href": "slides/slides-25-slr-inference.html#teaching-evaluations-model-assessment",
    "title": "Inference in SLR",
    "section": "Teaching evaluations: model assessment",
    "text": "Teaching evaluations: model assessment\nWe fit the model in R, and obtain the following plots.\n\nAre all conditions of LINE met?"
  },
  {
    "objectID": "slides/slides-25-slr-inference.html#looking-at-lm-output",
    "href": "slides/slides-25-slr-inference.html#looking-at-lm-output",
    "title": "Inference in SLR",
    "section": "Looking at lm() output",
    "text": "Looking at lm() output\n\nlibrary(broom)\neval_mod <- lm(score ~ bty_avg, data = evals)\neval_mod |>\n  tidy()\n\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    3.880338 \n    0.0761430 \n    50.961213 \n    0.00e+00 \n  \n  \n    bty_avg \n    0.066637 \n    0.0162912 \n    4.090382 \n    5.08e-05 \n  \n\n\n\n\n\n\nAssuming the linear model is appropriate, interpret the coefficients!\n\n\nIntercept: an instructor with an average beauty score of 0 would be expected to have an average evaluation score of 3.88\nSlope: for every one point increase in average beauty score an instructor receives, their evaluation score is expected to incrase by 0.067 points"
  },
  {
    "objectID": "slides/slides-25-slr-inference.html#making-conclusion",
    "href": "slides/slides-25-slr-inference.html#making-conclusion",
    "title": "Inference in SLR",
    "section": "Making conclusion",
    "text": "Making conclusion\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    3.880338 \n    0.0761430 \n    50.961213 \n    0.00e+00 \n  \n  \n    bty_avg \n    0.066637 \n    0.0162912 \n    4.090382 \n    5.08e-05 \n  \n\n\n\n\n\n\n\n\n\nWhat would your p-value be if your alternative was \\(H_{A}: \\beta_{1} > 0\\)? What would your conclusion be?\n\n\n\n\nThe p-value would simply be half of the p-value reported in the table above\nThe data provide convincing evidence that there is a positive relationship between instructor’s beauty score and evaluation score."
  },
  {
    "objectID": "slides/slides-25-slr-inference.html#confidence-intervals",
    "href": "slides/slides-25-slr-inference.html#confidence-intervals",
    "title": "Inference in SLR",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    3.880338 \n    0.0761430 \n    50.961213 \n    0.00e+00 \n  \n  \n    bty_avg \n    0.066637 \n    0.0162912 \n    4.090382 \n    5.08e-05 \n  \n\n\n\n\n\nWe can also construct confidence intervals using the output from lm()! Remember:\n\\[\n\\text{CI} = \\text{point est.} \\pm \\text{critical value} \\times \\text{SE}\n\\]\n\nCritical value also comes from \\(t_{n-2}\\) distribution\nSuppose we want a 95% confidence intervals for \\(\\beta_{1}\\):\n\n\nWhat code would you use to obtain critical value?\n\nqt(0.975, 461) = 1.97\n\nSo our 95% CI for \\(\\beta_{1}\\) is: \\(0.067 \\pm 1.97 \\times 0.016 = (0.035, 0.099)\\)"
  },
  {
    "objectID": "slides/slides-25-slr-inference.html#running-example-evals-data",
    "href": "slides/slides-25-slr-inference.html#running-example-evals-data",
    "title": "Inference in SLR",
    "section": "Running example: evals data",
    "text": "Running example: evals data\nData on 463 courses at UT Austin were obtained to answer the question: “What factors explain differences in instructor teaching evaluation scores?”\n\nOne hypothesis was that more attractive instructors receive better teaching evaluations\nWe will look at the variables:\n\nscore: course instructor’s average teaching score, where average is calculated from all students in that course. Scores ranged from 1-5, with 1 being lowest.\nbty_avg: course instructor’s average “beauty” score, where average is calculated from six student evaluations of “beauty”. Scores ranged from 1-10, with 1 being lowest."
  },
  {
    "objectID": "slides/slides-25-slr-inference.html#teaching-evaluations-data",
    "href": "slides/slides-25-slr-inference.html#teaching-evaluations-data",
    "title": "Inference in SLR",
    "section": "Teaching evaluations data",
    "text": "Teaching evaluations data\n\n\nDoes this line really have a non-zero slope?"
  },
  {
    "objectID": "slides/slides-25-slr-inference.html#remarks",
    "href": "slides/slides-25-slr-inference.html#remarks",
    "title": "Inference in SLR",
    "section": "Remarks",
    "text": "Remarks\n\nNote: for \\(\\beta_{1}\\), the null hypothesis is always of the form \\(H_{0}: \\beta_{1} = 0\\)\nLINE conditions must be met for underlying mathematical and probability theory to hold here! If not met, simulation-based methods would be a better choice\nHere, the Independence conditions did not seem to be met\n\nTake STAT 412 or other course to learn how to incorporate dependencies between observations!\n\nSo what can we say?\n\nThe results suggested by our inference should be viewed as preliminary, and not conclusive\nFurther investigation is certainly warranted!\nChecking LINE can be very subjective, but that’s how real-world analysis will be!"
  },
  {
    "objectID": "slides/slides-25-slr-inference.html#looking-at-lm-output-1",
    "href": "slides/slides-25-slr-inference.html#looking-at-lm-output-1",
    "title": "Inference in SLR",
    "section": "Looking at lm() output",
    "text": "Looking at lm() output\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    3.880338 \n    0.0761430 \n    50.961213 \n    0.00e+00 \n  \n  \n    bty_avg \n    0.066637 \n    0.0162912 \n    4.090382 \n    5.08e-05 \n  \n\n\n\n\n\n\n\n\nestimate: the observed point estimate (\\(b_{0}\\) or \\(b_{1}\\))\nstd.error: the estimated standard error of the estimate\n\n\n\nstatistic: the value of the test statistic\np.value: p-value associated with the two-sided alternative \\(H_{A}: \\beta_{1} \\neq 0\\)\n\n\n\n\nLet’s confirm the test statistic calculation:\n\n\n\\[\nt = \\frac{\\text{observed} - \\text{null}}{\\text{SE}_{0}} =\\frac{b_{1,obs} - \\beta_{1, 0}}{\\widehat{\\text{SE}}_{0}} = \\frac{0.066637 - 0}{0.0162912} = 4.0903823 \\sim t_{df}\n\\]\nwhere \\(df = n-2\\)"
  },
  {
    "objectID": "slides/slides-25-slr-inference.html#looking-at-lm-output-cont.",
    "href": "slides/slides-25-slr-inference.html#looking-at-lm-output-cont.",
    "title": "Inference in SLR",
    "section": "Looking at lm() output (cont.)",
    "text": "Looking at lm() output (cont.)\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    3.880338 \n    0.0761430 \n    50.961213 \n    0.00e+00 \n  \n  \n    bty_avg \n    0.066637 \n    0.0162912 \n    4.090382 \n    5.08e-05 \n  \n\n\n\n\n\nLet’s confirm the p-value calculation:\n\\[\\text{p-value} = \\text{Pr}(T \\geq 4.09) + \\text{Pr}(T \\leq -4.09)\\] where \\(T \\sim t_{461}\\)\n\nSo our p-value is: 2 * (1 - pt(4.09, 461)) = 5.0827307^{-5}"
  },
  {
    "objectID": "slides/slides-25-slr-inference.html#p-value-and-conclusion",
    "href": "slides/slides-25-slr-inference.html#p-value-and-conclusion",
    "title": "Inference in SLR",
    "section": "p-value and conclusion",
    "text": "p-value and conclusion\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    3.880338 \n    0.0761430 \n    50.961213 \n    0.00e+00 \n  \n  \n    bty_avg \n    0.066637 \n    0.0162912 \n    4.090382 \n    5.08e-05 \n  \n\n\n\n\n\nLet’s confirm the p-value calculation:\n\\[\\text{p-value} = \\text{Pr}(T \\geq 4.09) + \\text{Pr}(T \\leq -4.09)\\] where \\(T \\sim t_{461}\\)\n\nSo our p-value is: 2 * (1 - pt(4.09, 461)) = 5.0827307^{-5}\nAssuming the LINE conditions are met: since our p-value 5.0827307^{-5} is extremely small, we would reject \\(H_{0}\\) at any reasonable significant level. Thus, the data provide convincing evidence that there is a linear relationship between instructor’s beauty score and evaluation score."
  },
  {
    "objectID": "slides/slides-25-slr-inference.html#different-h_a",
    "href": "slides/slides-25-slr-inference.html#different-h_a",
    "title": "Inference in SLR",
    "section": "Different \\(H_{A}\\)",
    "text": "Different \\(H_{A}\\)\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    3.880338 \n    0.0761430 \n    50.961213 \n    0.00e+00 \n  \n  \n    bty_avg \n    0.066637 \n    0.0162912 \n    4.090382 \n    5.08e-05 \n  \n\n\n\n\n\n\n\n\nWhat would your p-value be if your alternative was \\(H_{A}: \\beta_{1} > 0\\)? What would your conclusion be?\n\n\n\\(\\text{Pr}(T \\geq 4.09)\\) = (1-pt(4.09, 461) = 2.5413653^{-5}\nThe data provide convincing evidence that there is a positive relationship between instructor’s beauty score and evaluation score.\n\n\n\nWhat would your p-value be if your alternative was \\(H_{A}: \\beta_{1} < 0\\)? What would your conclusion be?\n\n\n\\(\\text{Pr}(T \\leq 4.09)\\) = pt(4.09, 461) = 0.9999745\nThe data do not provide convincing evidence that there is a negative relationship between instructor’s beauty score and evaluation score."
  },
  {
    "objectID": "slides/slides-25-slr-inference.html#recap",
    "href": "slides/slides-25-slr-inference.html#recap",
    "title": "Inference in SLR",
    "section": "Recap",
    "text": "Recap\n\nLearned how to interpret slope and intercept of fitted model\n\n\\(b_0\\) is expected value of response when \\(x=0\\)\n\\(b_{1}\\) is expected change in \\(y\\) for a one unit increase in \\(x\\)\n\nWhen explanatory \\(x\\) is categorical, we have a slightly more nuanced interpretation\nCoefficient of determination \\(R^2\\) assesses strength of linear model fit"
  },
  {
    "objectID": "live_code/slr_extras.html",
    "href": "live_code/slr_extras.html",
    "title": "broom and factors for SLR",
    "section": "",
    "text": "library(tidyverse)\nlibrary(openintro)\nLet’s bring back our cherry_lm model:"
  },
  {
    "objectID": "live_code/slr1.html#pretty-output-using-broom",
    "href": "live_code/slr1.html#pretty-output-using-broom",
    "title": "Linear regression in R",
    "section": "Pretty output using broom",
    "text": "Pretty output using broom\nThe broom package has a function that turns the output from lm() into tidy, data frame form. We simply pass in the fitted model into the function of interest!\n\n\nInstall the package either by typing install.packages(\"broom\") in your Console, or in the Packages pane.\n\ntidy()\nThe function tidy() turns the information about the coefficients into a nice data frame:\n\nlibrary(broom)\ntidy(cherry_lm)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -36.9      3.37      -11.0 7.62e-12\n2 diam            5.07     0.247      20.5 8.64e-19\n\n\nSince this is in data frame, each column is a variable, and all of our dplyr wrangling functions work!\n\ntidy(cherry_lm) |>\n  pull(estimate)\n\n[1] -36.943459   5.065856\n\n\n\n\nglance()\nThe function glance() turns the extra information about the model fit into nice data frame:\n\nglance(cherry_lm)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.935         0.933  4.25      419. 8.64e-19     1  -87.8  182.  186.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\nglance(cherry_lm) |>\n  pull(r.squared)\n\n[1] 0.9353199\n\n\n\n\naugment()\nThe function augment() adds information about observations to the dataset:\n\naugment(cherry_lm) |>\n  slice(1:3)\n\n# A tibble: 3 × 8\n  volume  diam .fitted .resid   .hat .sigma .cooksd .std.resid\n   <dbl> <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>      <dbl>\n1   10.3   8.3    5.10   5.20 0.115    4.20  0.110       1.30 \n2   10.3   8.6    6.62   3.68 0.105    4.26  0.0492      0.914\n3   10.2   8.8    7.64   2.56 0.0992   4.30  0.0222      0.635\n\n\nYou can see the original x and y values (diam and volume), as well as:\n\n.fitted: the fitted (estimated) values \\(\\hat{y}\\) for the corresponding observation\n.resid: the residual for the observation (taken as \\(y - \\hat{y}\\)…)\n\n\n\nThe periods are important!\nWe can use the output from augment to plot residuals:\n\naugment(cherry_lm) |>\n  ggplot(aes(x = diam, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\")"
  },
  {
    "objectID": "midterms.html#preparation-1",
    "href": "midterms.html#preparation-1",
    "title": "Midterms",
    "section": "Preparation",
    "text": "Preparation\n\nThe best preparation you can do for the midterm is to go back through your notes and homework and be honest with yourself about what you do/don’t understand. This means going through the painful process of looking at feedback on Canvas. For the concepts that you need to practice more, try more problems (see below)!\nAnother way to prepare is to create your own study guide with summaries and examples of important concepts. As you study, it would be a good idea to compile a list of questions that you might have.\nWork on the practice problems that were distributed at the end of classes but not assigned.\nFor extra practice, additional review problems will soon be made available below. These questions are not necessarily representative of the typical scope and difficulty of individual exam questions. This review is not comprehensive, nor does it represent the expected amount of time for it will take for you to complete the midterm.\n\nExtra problems here and here\nVideo recap of probability"
  },
  {
    "objectID": "midterms.html#midterm-1",
    "href": "midterms.html#midterm-1",
    "title": "Midterms",
    "section": "Midterm 1",
    "text": "Midterm 1\n\nLogistics\n\nWhen and where: Thursday, October 10 during class (75 minutes)\nWhat: content through end of Week 4 (i.e. through Simpson’s Paradox)\n\nYou will not be asked to write code, but you may be asked to read or critique it\n\nImportant: Prof. Tang will be proctoring the exam\nYou will not have a problem set assigned this week!\nProf. Tang will move Friday’s office hours to Wednesday 10/09 3-4pm instead.\n\nYou should bring a calculator to the midterm. If you do not have one, the department can provide very basic calculators."
  },
  {
    "objectID": "midterms.html#preparation-2",
    "href": "midterms.html#preparation-2",
    "title": "Midterms",
    "section": "Preparation",
    "text": "Preparation\n\nThe best preparation you can do for the midterm is to go back through your notes and homework and be honest with yourself about what you do/don’t understand. This means going through the painful process of looking at feedback on Canvas. For the concepts that you need to practice more, try more problems (see below)!\nAnother way to prepare is to create your own study guide with summaries and examples of important concepts. As you study, it would be a good idea to compile a list of questions that you might have.\nWork on the practice problems that were distributed at the end of classes but not assigned.\nFor extra practice, additional review problems will soon be made available below. These questions are not necessarily representative of the typical scope and difficulty of individual exam questions. This review is not comprehensive, nor does it represent the expected amount of time for it will take for you to complete the midterm.\n\nExtra problems here and here\nVideo recap of probability"
  },
  {
    "objectID": "homework/hw8_r.html",
    "href": "homework/hw8_r.html",
    "title": "STAT 201: Homework 8 (R)",
    "section": "",
    "text": "We will examine data that were collected at Baystate Medical Center, Springfield, MA during 1986 on the birth weights of 189 babies, along with descriptive information about the mother. Researchers wanted to learn about risk factors for underweight babies. The variables are:\n\nlow: indicator of bwt less than 2.5 kg\nage: mother’s age in years\nlwt: mother’s weight in pounds at last menstrual period\nrace: mother’s race (1 = white, 2 = black, 3 = other)\nsmoke: smoking status during pregnancy (no or yes)\nptl: number of previous premature labours\nht: history of hypertension (0 = no, 1 = yes)\nui: presence of uterine irritability (0 = no, 1 = yes)\nftv: number of physician visits during the first trimester\nbwt: birth weight in grams\n\n\nLoad in the tidyverse and broom packages below. Then run the code chunk and take a look at the data.\n\n\nlibrary(readr)\nbirthwt <- read.csv(\"https://raw.githubusercontent.com/midd-stat201-fall2024/midd-stat201-fall2024.github.io/refs/heads/main/data/birthwt.csv\")\n# load other necessary packages here\n\n\nModify your data frame so that it only contains data for mothers with no history of hypertension.\n\n\n\n\n\nUsing your new data frame, fit a linear regression model for the birth weight of the baby using the smoke status of the mother as the explanatory variable. Store this as an object called birth_lm, then display summary information of the coefficients of birth_lm (“tidy” or not are both fine).\n\n\n\n\n\nProvide an example/scenario where the independence condition would be violated.\n\nAnswer:\n\nCheck if the normality condition is satisfied. Briefly explain in words why or why not. Be sure to have informative axis labels and title. Make sure the residuals are of the form \\(\\hat{y} - y\\).\n\n\n\n\nAnswer:\n\nBased off the model output, is there evidence of a linear relationship between the mother’s smoke status and the birth weight of the baby (for mothers with no history of hypertension)? Why or why not?\n\nAnswer:\n\nUsing code, obtain the \\(R^2\\) of the model and store it as a variable called r2. Using in-line code, report the the \\(R^2\\) of the model and interpret what it means in context.\n\n\n\n\nAnswer:\n\nDoes a significant linear relationship between \\(x\\) and \\(y\\) imply that a model is actually useful? Use your answers above to answer this question.\n\nAnswer:"
  },
  {
    "objectID": "coding_practice/coding-practice-25-slr.html",
    "href": "coding_practice/coding-practice-25-slr.html",
    "title": "SLR coding practice",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(openintro)\n\nLoading required package: airports\nLoading required package: cherryblossom\nLoading required package: usdata\n\nlibrary(broom)\n\n\nUsing the possum data again, fit a linear regression for the length of heads for a possum using their tail lengths as a predictor. Store the model output as a variable called possum_lm and display information about the coefficients in tidy form using the appropriate function.\n\n\n\n\n\nCreate a residual plot of the fitted model using the augment() function and ggplot(). Try adding a horizontal dashed line at 0!\n\n\n\n\n\nSuppose we found a tail of a possum on the ground! We measure the tail to be 36.1 cm. Being as reproducible as possible, obtain the estimated head length of this squirrel by doing the following:\n\n\nCreate variable called x_pred that stores the value of the explanatory variable we’d like to obtain a prediction for.\nObtain the estimated coefficients from the tidy output by pull()-ing the relevant column. Store them as a variable called possum_coeffs.\nIndexing coeffs (i.e. using bracket notation) and using x, obtain the estimated head length and store this as a value called head_l_pred.\n\n\n# create x\n\n# create possum_coeffs\n\n# create head_l_pred\n\nReport your answer using in-line code.\nAnswer:"
  },
  {
    "objectID": "live_code/slr_extras.html#pretty-output-using-broom",
    "href": "live_code/slr_extras.html#pretty-output-using-broom",
    "title": "broom and factors for SLR",
    "section": "Pretty output using broom",
    "text": "Pretty output using broom\nThe broom package has a function that turns the output from lm() into tidy, data frame form. We simply pass in the fitted model into the function of interest!\n\n\nInstall the package either by typing install.packages(\"broom\") in your Console, or in the Packages pane.\n\ntidy()\nThe function tidy() turns the information about the coefficients into a nice data frame:\n\nlibrary(broom)\ntidy(cherry_lm)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -36.9      3.37      -11.0 7.62e-12\n2 diam            5.07     0.247      20.5 8.64e-19\n\n\nSince this is in data frame, each column is a variable, and all of our dplyr wrangling functions work!\n\ntidy(cherry_lm) |>\n  pull(estimate)\n\n[1] -36.943459   5.065856\n\n\n\n\nglance()\nThe function glance() turns the extra information about the model fit into nice data frame:\n\nglance(cherry_lm)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.935         0.933  4.25      419. 8.64e-19     1  -87.8  182.  186.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\nglance(cherry_lm) |>\n  pull(r.squared)\n\n[1] 0.9353199\n\n\n\n\naugment()\nThe function augment() adds information about observations to the dataset:\n\naugment(cherry_lm) |>\n  slice(1:3)\n\n# A tibble: 3 × 8\n  volume  diam .fitted .resid   .hat .sigma .cooksd .std.resid\n   <dbl> <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>      <dbl>\n1   10.3   8.3    5.10   5.20 0.115    4.20  0.110       1.30 \n2   10.3   8.6    6.62   3.68 0.105    4.26  0.0492      0.914\n3   10.2   8.8    7.64   2.56 0.0992   4.30  0.0222      0.635\n\n\nYou can see the original x and y values (diam and volume), as well as:\n\n.fitted: the fitted (estimated) values \\(\\hat{y}\\) for the corresponding observation\n.resid: the residual for the observation (taken as \\(y - \\hat{y}\\)…)\n\n\n\nThe periods are important!\nWe can use the output from augment to plot residuals:\n\naugment(cherry_lm) |>\n  ggplot(aes(x = diam, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\")"
  },
  {
    "objectID": "live_code/slr_extras.html#factors-in-r",
    "href": "live_code/slr_extras.html#factors-in-r",
    "title": "broom and factors for SLR",
    "section": "factors in R",
    "text": "factors in R\nHow does lm() choose which level should be the base level?\n\nPre-specified as factor: In R, variables can be coded as “factor” variables where there is a specific numeric ordering under the hood. How can we tell? Using the str() function, we can find the data structure of a given variable:\n\nstr(possum$pop)\n\n Factor w/ 2 levels \"Vic\",\"other\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\nWe can see that the pop variable has two levels, and the order goes “Vic” then “other”. So “Vic” is taken as the base level.\n\n\n\n\n\n\nTip\n\n\n\n\n\nIf you want a different base level, we can change it using mutate()!\n\npossum |> \n  mutate(pop = factor(pop, levels = c(\"other\", \"Vic\"))) |>\n  pull(pop) |>\n  str()\n\n Factor w/ 2 levels \"other\",\"Vic\": 2 2 2 2 2 2 2 2 2 2 ...\n\n\n\n\n\nNot-specified: if your categorical variable is coded as a character/string variable and not a factor, the default base level is the first level in alphabetic order.\n\n\n\n\n\n\nTip\n\n\n\n\n\nIf you don’t like this behavior, you can mutate() any variable to be a factor variable.\n\ndata.frame(fruit = c(\"apple\", \"kiwi\")) |>\n  mutate(fruit_factor = factor(fruit, levels = c(\"kiwi\", \"apple\"))) |>\n  pull(fruit_factor) |>\n  str()\n\n Factor w/ 2 levels \"kiwi\",\"apple\": 2 1"
  },
  {
    "objectID": "slides/slides-25-slr-inference.html",
    "href": "slides/slides-25-slr-inference.html",
    "title": "Inference in SLR",
    "section": "",
    "text": "Last set of homework problems are released today!\nOffice hours changed this week:\n\nToday: 2-3pm only\nWednesday 4-5pm\nFriday: cancelled, moved to next week before midterm"
  },
  {
    "objectID": "live_code/slr_bootstrap.html",
    "href": "live_code/slr_bootstrap.html",
    "title": "Bootstrapping for slope",
    "section": "",
    "text": "library(tidyverse)\nlibrary(openintro)\nlibrary(broom)"
  },
  {
    "objectID": "slides/slides-26-slr-simulation.html",
    "href": "slides/slides-26-slr-simulation.html",
    "title": "Inference in SLR",
    "section": "",
    "text": "Last set of homework problems are released today!\nOffice hours changed this week:\n\nToday: 2-3pm only\nWednesday 4-5pm\nFriday: cancelled, moved to next week before midterm"
  },
  {
    "objectID": "slides/slides-26-slr-simulation.html#recap",
    "href": "slides/slides-26-slr-simulation.html#recap",
    "title": "Simulation-based inference in SLR",
    "section": "Recap",
    "text": "Recap\n\nPoint estimates \\((b_{0}, b_{1})\\) also have variability as their specific values depend on a given set of data\nWe saw how to use output from lm() to test hypotheses about and create confidence intervals for \\(\\beta_{0}\\) and \\(\\beta_{1}\\)\n\nRelies on LINE conditions being met\n\nLet’s turn to simulation-based techniques (good refresher before midterm!)"
  },
  {
    "objectID": "slides/slides-26-slr-simulation.html#variability-of-coefficient-estimates",
    "href": "slides/slides-26-slr-simulation.html#variability-of-coefficient-estimates",
    "title": "Inference in SLR",
    "section": "Variability of coefficient estimates",
    "text": "Variability of coefficient estimates\n\nRemember, a linear regression is fit using a sample of data\nDifferent samples from the same population will yield different point estimates of \\((b_{0}, b_{1})\\)\n\nI will generate 30 data points under the following model: \\(y = 1 + 0.5x+\\epsilon\\)\n\nHow? Randomly generate some \\(x\\) and \\(\\epsilon\\) values and then plug into model to get corresponding \\(y\\)\n\nFit SLR to these \\((x,y)\\) data, and obtain estimates \\((b_{0}, b_{1})\\)\nRepeat this 50 times"
  },
  {
    "objectID": "slides/slides-26-slr-simulation.html#variability-of-coefficient-estimates-1",
    "href": "slides/slides-26-slr-simulation.html#variability-of-coefficient-estimates-1",
    "title": "Inference in SLR",
    "section": "Variability of coefficient estimates",
    "text": "Variability of coefficient estimates"
  },
  {
    "objectID": "slides/slides-26-slr-simulation.html#what-are-we-interested-in",
    "href": "slides/slides-26-slr-simulation.html#what-are-we-interested-in",
    "title": "Inference in SLR",
    "section": "What are we interested in?",
    "text": "What are we interested in?\nRemember: we fit SLR to understand how \\(x\\) is (linearly) related to \\(y\\):\n\\[\ny = \\beta_{0} + \\beta_{1} x + \\epsilon\n\\]\n\n\nWhat would a value of \\(\\beta_{1} = 0\\) mean?\n\n\nIf \\(\\beta_{1} = 0\\), then the effect of \\(x\\) disappears and there is in fact no linear relationship between \\(x\\) and \\(y\\)\n\nWe don’t know \\(\\beta_{1}\\), so let’s look at estimate \\(b_{1}\\):\n\n\nIs an estimate of \\(b_{1}= 0\\) a convincing evidence of no relationship? What if \\(b_{1} = 0.1\\)?\n\nIt depends! We saw that \\(b_{1}\\) varies by sample! So let’s perform inference for \\(\\beta_{1}\\)"
  },
  {
    "objectID": "slides/slides-26-slr-simulation.html#running-example-evals-data",
    "href": "slides/slides-26-slr-simulation.html#running-example-evals-data",
    "title": "Inference in SLR",
    "section": "Running example: evals data",
    "text": "Running example: evals data\nData on 463 courses at UT Austin were obtained to answer the question: “What factors explain differences in instructor teaching evaluation scores?”\n\nOne hypothesis was that more attractive instructors receive better teaching evaluations\nWe will look at the variables:\n\nscore: course instructor’s average teaching score, where average is calculated from all students in that course. Scores ranged from 1-5, with 1 being lowest.\nbty_avg: course instructor’s average “beauty” score, where average is calculated from six student evaluations of “beauty”. Scores ranged from 1-10, with 1 being lowest."
  },
  {
    "objectID": "slides/slides-26-slr-simulation.html#teaching-evaluations-data",
    "href": "slides/slides-26-slr-simulation.html#teaching-evaluations-data",
    "title": "Inference in SLR",
    "section": "Teaching evaluations data",
    "text": "Teaching evaluations data\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nDoes this line really have a non-zero slope?"
  },
  {
    "objectID": "slides/slides-26-slr-simulation.html#hypothesis-test-for-slope",
    "href": "slides/slides-26-slr-simulation.html#hypothesis-test-for-slope",
    "title": "Inference in SLR",
    "section": "Hypothesis test for slope",
    "text": "Hypothesis test for slope\n\nWe have the following hypotheses:\n\n\\(H_{0}: \\beta_{1} = 0\\): the true linear model has slope zero\n\\(H_{A}: \\beta_{1} \\neq 0\\): the true linear model has a non-zero slope. An instructor’s average beauty score is predictive of their average teaching evaluation score.\n\nTo assess, we do what we usually do:\n\nCheck if methods are appropriate\nIf so: obtain an estimate, identify/estimate standard error of the estimate, find an appropriate test statistic, and calculate p-value\n\n\nThe output from lm() actually does all of this for us, but we will see how the test statistic and p-value are calculated!"
  },
  {
    "objectID": "slides/slides-26-slr-simulation.html#teaching-evaluations-model-assessment",
    "href": "slides/slides-26-slr-simulation.html#teaching-evaluations-model-assessment",
    "title": "Inference in SLR",
    "section": "Teaching evaluations: model assessment",
    "text": "Teaching evaluations: model assessment\nWe fit the model in R, and obtain the following plots.\n\nAre all conditions of LINE met?\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/slides-26-slr-simulation.html#looking-at-lm-output",
    "href": "slides/slides-26-slr-simulation.html#looking-at-lm-output",
    "title": "Inference in SLR",
    "section": "Looking at lm() output",
    "text": "Looking at lm() output\n\nlibrary(broom)\neval_mod <- lm(score ~ bty_avg, data = evals)\neval_mod |>\n  tidy()\n\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    3.880338 \n    0.0761430 \n    50.961213 \n    0.00e+00 \n  \n  \n    bty_avg \n    0.066637 \n    0.0162912 \n    4.090382 \n    5.08e-05 \n  \n\n\n\n\n\n\nAssuming the linear model is appropriate, interpret the coefficients!\n\n\nIntercept: an instructor with an average beauty score of 0 would be expected to have an average evaluation score of 3.88\nSlope: for every one point increase in average beauty score an instructor receives, their evaluation score is expected to incrase by 0.067 points"
  },
  {
    "objectID": "slides/slides-26-slr-simulation.html#looking-at-lm-output-1",
    "href": "slides/slides-26-slr-simulation.html#looking-at-lm-output-1",
    "title": "Inference in SLR",
    "section": "Looking at lm() output",
    "text": "Looking at lm() output\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    3.880338 \n    0.0761430 \n    50.961213 \n    0.00e+00 \n  \n  \n    bty_avg \n    0.066637 \n    0.0162912 \n    4.090382 \n    5.08e-05 \n  \n\n\n\n\n\n\n\n\nestimate: the observed point estimate (\\(b_{0}\\) or \\(b_{1}\\))\nstd.error: the estimated standard error of the estimate\n\n\n\nstatistic: the value of the test statistic\np.value: p-value associated with the two-sided alternative \\(H_{A}: \\beta_{1} \\neq 0\\)\n\n\n\n\nLet’s confirm the test statistic calculation:\n\n\n\\[\nt = \\frac{\\text{observed} - \\text{null}}{\\text{SE}_{0}} =\\frac{b_{1,obs} - \\beta_{1, 0}}{\\widehat{\\text{SE}}_{0}} = \\frac{0.066637 - 0}{0.0162912} = 4.0903823 \\sim t_{df}\n\\]\nwhere \\(df = n-2\\)"
  },
  {
    "objectID": "slides/slides-26-slr-simulation.html#p-value-and-conclusion",
    "href": "slides/slides-26-slr-simulation.html#p-value-and-conclusion",
    "title": "Inference in SLR",
    "section": "p-value and conclusion",
    "text": "p-value and conclusion\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    3.880338 \n    0.0761430 \n    50.961213 \n    0.00e+00 \n  \n  \n    bty_avg \n    0.066637 \n    0.0162912 \n    4.090382 \n    5.08e-05 \n  \n\n\n\n\n\nLet’s confirm the p-value calculation:\n\\[\\text{p-value} = \\text{Pr}(T \\geq 4.09) + \\text{Pr}(T \\leq -4.09)\\] where \\(T \\sim t_{461}\\)\n\nSo our p-value is: 2 * (1 - pt(4.09, 461)) = 5.0827307^{-5}\nAssuming the LINE conditions are met: since our p-value 5.0827307^{-5} is extremely small, we would reject \\(H_{0}\\) at any reasonable significant level. Thus, the data provide convincing evidence that there is a linear relationship between instructor’s beauty score and evaluation score."
  },
  {
    "objectID": "slides/slides-26-slr-simulation.html#different-h_a",
    "href": "slides/slides-26-slr-simulation.html#different-h_a",
    "title": "Inference in SLR",
    "section": "Different \\(H_{A}\\)",
    "text": "Different \\(H_{A}\\)\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    3.880338 \n    0.0761430 \n    50.961213 \n    0.00e+00 \n  \n  \n    bty_avg \n    0.066637 \n    0.0162912 \n    4.090382 \n    5.08e-05 \n  \n\n\n\n\n\n\n\n\nWhat would your p-value be if your alternative was \\(H_{A}: \\beta_{1} > 0\\)? What would your conclusion be?\n\n\n\\(\\text{Pr}(T \\geq 4.09)\\) = (1-pt(4.09, 461) = 2.5413653^{-5}\nThe data provide convincing evidence that there is a positive relationship between instructor’s beauty score and evaluation score.\n\n\n\nWhat would your p-value be if your alternative was \\(H_{A}: \\beta_{1} < 0\\)? What would your conclusion be?\n\n\n\\(\\text{Pr}(T \\leq 4.09)\\) = pt(4.09, 461) = 0.9999745\nThe data do not provide convincing evidence that there is a negative relationship between instructor’s beauty score and evaluation score."
  },
  {
    "objectID": "slides/slides-26-slr-simulation.html#confidence-intervals",
    "href": "slides/slides-26-slr-simulation.html#confidence-intervals",
    "title": "Simulation-based inference in SLR",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nCompare to our 95% CI for \\(\\beta_{1}\\) using mathematical model: (0.035, 0.099)"
  },
  {
    "objectID": "slides/slides-26-slr-simulation.html#remarks",
    "href": "slides/slides-26-slr-simulation.html#remarks",
    "title": "Inference in SLR",
    "section": "Remarks",
    "text": "Remarks\n\nNote: for \\(\\beta_{1}\\), the null hypothesis is always of the form \\(H_{0}: \\beta_{1} = 0\\)\nLINE conditions must be met for underlying mathematical and probability theory to hold here! If not met, simulation-based methods would be a better choice\nHere, the Independence conditions did not seem to be met\n\nTake STAT 412 or other course to learn how to incorporate dependencies between observations!\n\nSo what can we say?\n\nThe results suggested by our inference should be viewed as preliminary, and not conclusive\nFurther investigation is certainly warranted!\nChecking LINE can be very subjective, but that’s how real-world analysis will be!"
  },
  {
    "objectID": "live_code/slr_bootstrap.html#bootstrap-distribution-of-b_1",
    "href": "live_code/slr_bootstrap.html#bootstrap-distribution-of-b_1",
    "title": "Bootstrapping for slope",
    "section": "Bootstrap distribution of \\(b_{1}\\)",
    "text": "Bootstrap distribution of \\(b_{1}\\)\n\nset.seed(311)\nn <- nrow(evals)\nB <- 1000\nb1_boot <- rep(NA, B)\n\n# option 1\nfor(b in 1:B){\n  # create a vector of indices for sampling row-by-row:\n  resamp_inds <- sample(1:n, size = n, replace = T)\n  \n  # create new data frame by grabbing the resampled rows\n  evals_resamp <- evals[resamp_inds,]\n  \n  # fit lm on new model and grab the corresponding coefficient\n  coefs <- coef(lm(score ~ bty_avg, data = evals_resamp))\n  b1_boot[b] <- coefs[2]\n}\n\n\n# option 2: dplyr (slower)\nset.seed(311)\nb1_boot <- rep(NA, B)\nfor(b in 1:B){\n  # create new data frame using sample_n() to sample rows\n  evals_resamp <- evals |>\n    sample_n(size = n, replace = T)\n  \n  # get coefficient using broom functions\n  b1_boot[b] <- lm(score ~ bty_avg, data = evals_resamp) |>\n    tidy() |>\n    slice(2) |>\n    pull(estimate)\n}\n\nVisualize the bootstrap distribution:"
  },
  {
    "objectID": "live_code/slr_bootstrap.html#bootstrap-ci-for-beta_1",
    "href": "live_code/slr_bootstrap.html#bootstrap-ci-for-beta_1",
    "title": "Bootstrapping for slope",
    "section": "Bootstrap CI for \\(\\beta_{1}\\)",
    "text": "Bootstrap CI for \\(\\beta_{1}\\)\n\n\n\nWe will obtain the 95% bootstrap interval:\n\nboot_ci <- quantile(b1_boot, c(0.025, 0.975))\nboot_ci\n\n     2.5%     97.5% \n0.0329399 0.1012776 \n\n\nVisualize bootstrap distribution and CI, and compare to 95% CI obtained via mathematical model:"
  },
  {
    "objectID": "slides/slides-26-slr-simulation.html#housekeeping",
    "href": "slides/slides-26-slr-simulation.html#housekeeping",
    "title": "Simulation-based inference in SLR",
    "section": "Housekeeping",
    "text": "Housekeeping\n\n\n\n\nLast set of homework problems are released today!\nOffice hours changed this week:\n\nWednesday (today!) 4-5pm\nFriday: cancelled, moved to next week before midterm"
  },
  {
    "objectID": "slides/slides-26-slr-simulation.html#bootstrapping",
    "href": "slides/slides-26-slr-simulation.html#bootstrapping",
    "title": "Simulation-based inference in SLR",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nRepeat \\(B\\) times:\n\nSample with replacement from the original data, of the same sample size as the original data\nCalculate the quantity of interest using the resampled data\n\n\nIn the case of SLR: what exactly should we be “resampling”? What is the quantity/quantities of interest?"
  },
  {
    "objectID": "slides/slides-26-slr-simulation.html#bootstrapping-for-slr",
    "href": "slides/slides-26-slr-simulation.html#bootstrapping-for-slr",
    "title": "Simulation-based inference in SLR",
    "section": "Bootstrapping for SLR",
    "text": "Bootstrapping for SLR\n\nFor a given observation \\(i\\), we need to keep \\((x_{i}, y_{i})\\) together\n\nWant to keep pairs of score and bty_avg together, but different pairs may be re-sampled\nWe will re-sample with replacement row-by-row\n\nFor each re-sampled dataset, fit a linear regression model and record \\(b_{1}\\)\n\n\nThis yields bootstrap distribution of estimated slope coefficients!"
  },
  {
    "objectID": "slides/slides-26-slr-simulation.html#live-code-for-bootstrapped-slope",
    "href": "slides/slides-26-slr-simulation.html#live-code-for-bootstrapped-slope",
    "title": "Simulation-based inference in SLR",
    "section": "Live code for bootstrapped slope",
    "text": "Live code for bootstrapped slope\n\n\n\n\n\nBootstrap distribution of \\(b_{1}\\):\n\n\n\n\n\n\n95% bootstrap CI for \\(\\beta_{1}\\): (0.033, 0.101):"
  },
  {
    "objectID": "slides/slides-26-slr-simulation.html#bootstrap-dist.",
    "href": "slides/slides-26-slr-simulation.html#bootstrap-dist.",
    "title": "Simulation-based inference in SLR",
    "section": "Bootstrap dist.",
    "text": "Bootstrap dist."
  },
  {
    "objectID": "slides/slides-26-slr-simulation.html#hypotheses",
    "href": "slides/slides-26-slr-simulation.html#hypotheses",
    "title": "Simulation-based inference in SLR",
    "section": "Hypotheses",
    "text": "Hypotheses\nRecall our hypotheses:\n\\[\nH_{0}: \\beta_{1} = 0 \\qquad \\text{vs.} \\qquad H_{A}: \\beta_{1} \\neq 0 \\text{ (or >, <)}\n\\]"
  },
  {
    "objectID": "slides/slides-26-slr-bootstrap-ci.html",
    "href": "slides/slides-26-slr-bootstrap-ci.html",
    "title": "Simulation-based CIs for SLR",
    "section": "",
    "text": "Last set of homework problems are released today!\nOffice hours changed this week:\n\nWednesday (today!) 4-5pm\nFriday: cancelled, moved to next week before midterm"
  },
  {
    "objectID": "slides/slides-26-slr-bootstrap-ci.html#recap",
    "href": "slides/slides-26-slr-bootstrap-ci.html#recap",
    "title": "Simulation-based CIs for SLR",
    "section": "Recap",
    "text": "Recap\n\nPoint estimates \\((b_{0}, b_{1})\\) also have variability as their specific values depend on a given set of data\nWe saw how to use output from lm() to test hypotheses about and create confidence intervals for \\(\\beta_{0}\\) and \\(\\beta_{1}\\)\n\nRelies on LINE conditions being met\n\nLet’s turn to simulation-based techniques (good refresher before midterm!)"
  },
  {
    "objectID": "slides/slides-26-slr-bootstrap-ci.html#bootstrapping",
    "href": "slides/slides-26-slr-bootstrap-ci.html#bootstrapping",
    "title": "Simulation-based CIs for SLR",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nRepeat \\(B\\) times:\n\nSample with replacement from the original data, of the same sample size as the original data\nCalculate the quantity of interest using the resampled data\n\n\nIn the case of SLR: what exactly should we be “resampling”? What is the quantity/quantities of interest?"
  },
  {
    "objectID": "slides/slides-26-slr-bootstrap-ci.html#bootstrapping-for-slr",
    "href": "slides/slides-26-slr-bootstrap-ci.html#bootstrapping-for-slr",
    "title": "Simulation-based CIs for SLR",
    "section": "Bootstrapping for SLR",
    "text": "Bootstrapping for SLR\n\nFor a given observation \\(i\\), we need to keep \\((x_{i}, y_{i})\\) together\n\nWant to keep pairs of score and bty_avg together, but different pairs may be re-sampled\nWe will re-sample with replacement row-by-row\n\nFor each re-sampled dataset, fit a linear regression model and record \\(b_{1}\\)\n\n\nThis yields bootstrap distribution of estimated slope coefficients!"
  },
  {
    "objectID": "slides/slides-26-slr-bootstrap-ci.html#live-code-for-bootstrapped-slope",
    "href": "slides/slides-26-slr-bootstrap-ci.html#live-code-for-bootstrapped-slope",
    "title": "Simulation-based CIs for SLR",
    "section": "Live code for bootstrapped slope",
    "text": "Live code for bootstrapped slope\n\n\n\n\n\nBootstrap distribution of \\(b_{1}\\):\n\n\n\n\n\n\n95% bootstrap CI for \\(\\beta_{1}\\): (0.033, 0.101):"
  },
  {
    "objectID": "slides/slides-26-slr-bootstrap-ci.html#confidence-intervals",
    "href": "slides/slides-26-slr-bootstrap-ci.html#confidence-intervals",
    "title": "Simulation-based CIs for SLR",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    3.880338 \n    0.0761430 \n    50.961213 \n    0.00e+00 \n  \n  \n    bty_avg \n    0.066637 \n    0.0162912 \n    4.090382 \n    5.08e-05 \n  \n\n\n\n\n\nCompare to our 95% CI for \\(\\beta_{1}\\) using mathematical model: (0.035, 0.099)"
  },
  {
    "objectID": "slides/slides-26-slr-bootstrap-ci.html#hypotheses",
    "href": "slides/slides-26-slr-bootstrap-ci.html#hypotheses",
    "title": "Simulation-based inference in SLR",
    "section": "Hypotheses",
    "text": "Hypotheses\nRecall our hypotheses:\n\\[\nH_{0}: \\beta_{1} = 0 \\qquad \\text{vs.} \\qquad H_{A}: \\beta_{1} \\neq 0 \\text{ (or >, <)}\n\\]"
  },
  {
    "objectID": "slides/slides-26-slr-bootstrap-ci.html#housekeeping",
    "href": "slides/slides-26-slr-bootstrap-ci.html#housekeeping",
    "title": "Simulation-based CIs for SLR",
    "section": "Housekeeping",
    "text": "Housekeeping\n\n\n\n\nOffice hours changed this week:\n\nWednesday (today!) 4-5pm\nFriday: cancelled, moved to next week before midterm\n\nCoding practice due tonight"
  },
  {
    "objectID": "slides/slides-26-slr-bootstrap-ci.html#looking-towards-testing",
    "href": "slides/slides-26-slr-bootstrap-ci.html#looking-towards-testing",
    "title": "Simulation-based CIs for SLR",
    "section": "Looking towards testing",
    "text": "Looking towards testing\nRecall our hypotheses for the slope: \\(H_{0}: \\beta_{1} = 0\\) versus \\(H_{A}: \\beta_{1} \\neq 0\\)\n\nHow might we use simulation to test these hypotheses? (i.e. how can we simulate “null world”?)"
  },
  {
    "objectID": "slides/slides-26-slr-bootstrap-ci.html#evals-data",
    "href": "slides/slides-26-slr-bootstrap-ci.html#evals-data",
    "title": "Simulation-based CIs for SLR",
    "section": "evals data",
    "text": "evals data\nFirst six observations:\n\n\n\n\n \n  \n    course_id \n    prof_id \n    score \n    bty_avg \n  \n \n\n  \n    1 \n    1 \n    4.7 \n    5 \n  \n  \n    2 \n    1 \n    4.1 \n    5 \n  \n  \n    3 \n    1 \n    3.9 \n    5 \n  \n  \n    4 \n    1 \n    4.8 \n    5 \n  \n  \n    5 \n    2 \n    4.6 \n    3 \n  \n  \n    6 \n    2 \n    4.3 \n    3 \n  \n\n\n\n\n\n\nRecall our model:\n\\[\\underbrace{\\text{score}}_{y} = \\beta_{0} + \\beta_{1} \\underbrace{\\text{bty_avg}}_{x} + \\epsilon\\]\n\n\nWe can index to denote specific row/observation pairs \\((x_{i}, y_{i})\\)\n\ne.g. \\((x_{1}, y_{1}) = (5, 4.7 )\\)"
  },
  {
    "objectID": "slides/slides-27-slr-randomization.html#housekeeping",
    "href": "slides/slides-27-slr-randomization.html#housekeeping",
    "title": "Simulation-based HTs for SLR",
    "section": "Housekeeping",
    "text": "Housekeeping\n\n\n\nData for end of class (copy when needed):\n\nlibrary(readr)\nbirthwt <- read.csv(\"https://raw.githubusercontent.com/midd-stat201-fall2024/midd-stat201-fall2024.github.io/refs/heads/main/data/birthwt.csv\")\n\n\nOffice hours changed this week:\n\nFriday: cancelled, moved to next week before midterm\n\nHomework 8 due tonight\nOffice hours next week:\n\nMonday 2:00-4:00pm\nTuesday: 11am-12:00pm, 4:00-5:00pm"
  },
  {
    "objectID": "slides/slides-27-slr-randomization.html#simulation-based-ht-for-slope",
    "href": "slides/slides-27-slr-randomization.html#simulation-based-ht-for-slope",
    "title": "Simulation-based HTs for SLR",
    "section": "Simulation-based HT for slope",
    "text": "Simulation-based HT for slope\nRecall our hypotheses for the slope: \\(H_{0}: \\beta_{1} = 0\\) versus \\(H_{A}: \\beta_{1} \\neq 0\\)\n\nHow might we use simulation to test these hypotheses? (i.e. how can we simulate “null world”?)\n\n\nUnder \\(H_{0}\\), there is no relationship between \\(x\\) and \\(y\\), so we can shuffle/permute/break up the \\((x_{i}, y_{i})\\) under \\(H_{0}\\)\n\ni.e. there is no special correspondence between \\(x_{i}\\) and \\(y_{i}\\)"
  },
  {
    "objectID": "slides/slides-27-slr-randomization.html#randomization-test-demonstration",
    "href": "slides/slides-27-slr-randomization.html#randomization-test-demonstration",
    "title": "Simulation-based HTs for SLR",
    "section": "Randomization test (demonstration)",
    "text": "Randomization test (demonstration)\nHere’s how it would look like using cards. Repeat the following \\(B\\) times:\n\nWrite down all \\(x_1,\\ldots, x_{n}\\) values and all \\(y_{1},\\ldots, y_{n}\\) values on cards.\nShuffle the response variable cards to get \\(y_{1}^{shuff}, \\ldots, y_{n}^{shuff}\\)\nDeal out the shuffled responses to pair with an explanatory: \\((x_{1}, y_{1}^{shuff}),\\ldots, (x_{n}, y_{n}^{shuff})\\)\nFit linear regression model to these shuffled data and record \\(b_{1}\\)\n\n\n\nConvince yourself this corresponds to \\(H_0: \\beta_{1} = 0\\)!\nWe are not sampling with replacement"
  },
  {
    "objectID": "slides/slides-27-slr-randomization.html#evals",
    "href": "slides/slides-27-slr-randomization.html#evals",
    "title": "Simulation-based HTs for SLR",
    "section": "evals",
    "text": "evals\nLet’s return to our evals data and model: \\(\\text{score} = \\beta_{0} + \\beta_{1} \\text{bty_avg} + \\epsilon\\)\n\n\n\nFirst six rows of original data:\n\n\n\n\n \n  \n    course_id \n    bty_avg \n    score \n  \n \n\n  \n    1 \n    5 \n    4.7 \n  \n  \n    2 \n    5 \n    4.1 \n  \n  \n    3 \n    5 \n    3.9 \n  \n  \n    4 \n    5 \n    4.8 \n  \n  \n    5 \n    3 \n    4.6 \n  \n  \n    6 \n    3 \n    4.3 \n  \n\n\n\n\n\n\n\n\nFirst six rows of one iteration of shuffled data:\n\n\n\n\n \n  \n    course_id \n    bty_avg \n    score \n    score_shuff \n  \n \n\n  \n    1 \n    5 \n    4.7 \n    4.5 \n  \n  \n    2 \n    5 \n    4.1 \n    3.9 \n  \n  \n    3 \n    5 \n    3.9 \n    3.7 \n  \n  \n    4 \n    5 \n    4.8 \n    3.8 \n  \n  \n    5 \n    3 \n    4.6 \n    4.7 \n  \n  \n    6 \n    3 \n    4.3 \n    4.6"
  },
  {
    "objectID": "slides/slides-27-slr-randomization.html#evals-null-distribution",
    "href": "slides/slides-27-slr-randomization.html#evals-null-distribution",
    "title": "Simulation-based HTs for SLR",
    "section": "evals null distribution",
    "text": "evals null distribution\n\\(H_{0}: \\beta_{1} = 0\\) (there is no linear relationship between score and bty_avg)\n\\(H_{A}: \\beta_{1} > 0\\) (there is a positive linear relationship between score and bty_avg)"
  },
  {
    "objectID": "slides/slides-27-slr-randomization.html#p-value",
    "href": "slides/slides-27-slr-randomization.html#p-value",
    "title": "Simulation-based HTs for SLR",
    "section": "p-value",
    "text": "p-value\n\nscore_lm <- lm(score ~ bty_avg, data = evals)\ntidy(score_lm)\n\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    3.880338 \n    0.0761430 \n    50.961213 \n    0.00e+00 \n  \n  \n    bty_avg \n    0.066637 \n    0.0162912 \n    4.090382 \n    5.08e-05 \n  \n\n\n\n\n\n\nCompare our observed fitted \\(b_{1,obs} = 0.067\\) coefficient to null distribution:\n\n\n\n\n\n\n\n\n\n\n\n\nRecall: \\(H_{A}: \\beta_{1} > 0\\)\np-value is calculated as \\(\\frac{\\text{number of simulated } b_{1} > 0.067}{B} = 0\\)"
  },
  {
    "objectID": "slides/slides-27-slr-randomization.html#your-turn",
    "href": "slides/slides-27-slr-randomization.html#your-turn",
    "title": "Simulation-based HTs for SLR",
    "section": "Your turn!!",
    "text": "Your turn!!\n\\[\\text{score} = \\beta_{0} + \\beta_{1} \\text{bty_avg} + \\epsilon\\]\nIn groups, obtain the null distribution for this \\(H_{0}\\) via simulation and visualize it using ggplot with informative title and axis labels. (You could be expected to do something like this for your midterm…)\n\nSuggestion: in a relevant place in your code, use the data frame evals to create a new data frame called evals_null\n\nevals_null should have a variable called score_shuffle that represents the shuffled scores"
  },
  {
    "objectID": "live_code/slr-ht-randomization.html",
    "href": "live_code/slr-ht-randomization.html",
    "title": "Hypothesis test for SLR with simulation",
    "section": "",
    "text": "library(tidyverse)\nlibrary(openintro)\n\n\nset.seed(116)\nB <- 1000\nb1_null <- rep(NA, B)\nscores <- evals |>\n  pull(score)\nfor(b in 1:B){\n  score_shuff <- sample(scores)\n  evals_null <- evals |>\n    mutate(score_shuffle = score_shuff)\n  null_lm <- lm(score_shuffle ~ bty_avg, data = evals_null)\n  b1_null[b] <- coef(null_lm)[2]\n}"
  },
  {
    "objectID": "coding_practice/coding-practice-27-slr-randomization.html",
    "href": "coding_practice/coding-practice-27-slr-randomization.html",
    "title": "Hypothesis test for SLR with simulation",
    "section": "",
    "text": "library(tidyverse)\nlibrary(openintro)\nFeel free to type View(evals) in your console to look at the data!"
  },
  {
    "objectID": "coding_practice/coding-practice-27-slr-randomization.html#obtain-null-distribution",
    "href": "coding_practice/coding-practice-27-slr-randomization.html#obtain-null-distribution",
    "title": "Hypothesis test for SLR with simulation",
    "section": "Obtain null distribution",
    "text": "Obtain null distribution\n\n# set a seed\n\n\n# define/create necessary quantities\n\n\n# code for null distribution"
  },
  {
    "objectID": "coding_practice/coding-practice-27-slr-randomization.html#visualize-your-null-distribution",
    "href": "coding_practice/coding-practice-27-slr-randomization.html#visualize-your-null-distribution",
    "title": "Hypothesis test for SLR with simulation",
    "section": "Visualize your null distribution",
    "text": "Visualize your null distribution"
  },
  {
    "objectID": "live_code/mlr-intro.html",
    "href": "live_code/mlr-intro.html",
    "title": "Introduction to MLR",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)"
  },
  {
    "objectID": "live_code/mlr-intro.html#birth-weight-data",
    "href": "live_code/mlr-intro.html#birth-weight-data",
    "title": "Introduction to MLR",
    "section": "Birth weight data",
    "text": "Birth weight data\nBaystate Medical Center, Springfield, MA during 1986 on the birth weights of 189 babies, along with descriptive information about the mother\n\nlibrary(readr)\nbirthwt <- read.csv(\"https://raw.githubusercontent.com/midd-stat201-fall2024/midd-stat201-fall2024.github.io/refs/heads/main/data/birthwt.csv\")\n\nWe want to understand risk factors for a baby’s birth weight (bwt). Homework 8 explores the effect of mother’s smoke status on birth weight of baby.\nLet’s look at a different variable: race of mother\n\nVariable race is numerical where 1 = white, 2 = black, 3 = other\n\n\n# wrong! This is bad!!\nlm(bwt ~ race, data = birthwt) |>\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    3230.     117.      27.5  1.17e-67\n2 race           -155.      57.0     -2.71 7.26e- 3"
  },
  {
    "objectID": "live_code/mlr-intro.html#converting-to-factor",
    "href": "live_code/mlr-intro.html#converting-to-factor",
    "title": "Introduction to MLR",
    "section": "Converting to factor",
    "text": "Converting to factor\nWe need to convert race variable to categorical!\n\nbirthwt2 <- birthwt |>\n  mutate(race = case_when(race == 1 ~ \"white\",\n                          race == 2 ~ \"black\",\n                          race == 3 ~ \"other\")) |>\n  mutate(race = factor(race, levels = c(\"white\", \"black\", \"other\")))\n\nstr(birthwt2$race)\n\n Factor w/ 3 levels \"white\",\"black\",..: 2 3 1 1 1 3 1 3 1 1 ...\n\n\nLet’s go ahead and fit this model using lm() as we usually would and look at the tidy output:\n\nbwt_lm <- lm(bwt ~ race, data = birthwt2)\ntidy(bwt_lm)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    3103.      72.9     42.5  8.53e-98\n2 raceblack      -383.     158.      -2.42 1.63e- 2\n3 raceother      -297.     114.      -2.61 9.65e- 3\n\n\nLet’s write out the form of the linear regression model:"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html",
    "href": "slides/slides-28-mlr-intro.html",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Study for midterm!"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#multiple-linear-regression",
    "href": "slides/slides-28-mlr-intro.html#multiple-linear-regression",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nWe have seen simple linear regression, which is where we have one explanatory variableLinear regression can naturally extend to include more than one explanatory variable\n\nSeems natural: usually several factors affect behavior of phenomena\n\nMultiple linear regression takes the form: \\[y = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} + \\ldots + \\beta_{p} x_{p} + \\epsilon\\]\n\nNow there are \\(p\\) different explanatory variables \\(x_{1},\\ldots, x_{p}\\) per observation\nStill one response \\(y\\) and error \\(\\epsilon\\) per observation\n\nRepresents a holistic approach for modeling all of the variables simultaneously"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#birth-weight-data",
    "href": "slides/slides-28-mlr-intro.html#birth-weight-data",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Birth weight data",
    "text": "Birth weight data\nBaystate Medical Center, Springfield, MA during 1986 on the birth weights of 189 babies, along with descriptive information about the mother\n\n\n\n\nWant to understand risk factors for a baby’s birth weight (bwt)\nHomework 8 explores the effect of mother’s smoke status on birth weight of baby\nLet’s look at a different variable: race of mother\n\nVariable race numerical where 1 = white, 2 = black, 3 = other\n\n\n\n\n\n'data.frame':   189 obs. of  10 variables:\n $ low  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ age  : int  19 33 20 21 18 21 22 17 29 26 ...\n $ lwt  : int  182 155 105 108 107 124 118 103 123 113 ...\n $ race : int  2 3 1 1 1 3 1 3 1 1 ...\n $ smoke: chr  \"no\" \"no\" \"yes\" \"yes\" ...\n $ ptl  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ht   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ui   : int  1 0 0 1 1 0 0 0 0 0 ...\n $ ftv  : int  0 3 1 2 0 0 1 1 1 0 ...\n $ bwt  : int  2523 2551 2557 2594 2600 2622 2637 2637 2663 2665 ..."
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#converting-to-factor",
    "href": "slides/slides-28-mlr-intro.html#converting-to-factor",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Converting to factor",
    "text": "Converting to factor\nWe need to convert variable race to categorical! Does not make sense to do “math” on variable:\n\nbirthwt2 <- birthwt |>\n  mutate(race = case_when(race == 1 ~ \"white\",\n                          race == 2 ~ \"black\",\n                          race == 3 ~ \"other\")) |>\n  mutate(race = factor(race, levels = c(\"white\", \"black\", \"other\")))\n\nstr(birthwt2)\n\n'data.frame':   189 obs. of  10 variables:\n $ low  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ age  : int  19 33 20 21 18 21 22 17 29 26 ...\n $ lwt  : int  182 155 105 108 107 124 118 103 123 113 ...\n $ race : Factor w/ 3 levels \"white\",\"black\",..: 2 3 1 1 1 3 1 3 1 1 ...\n $ smoke: chr  \"no\" \"no\" \"yes\" \"yes\" ...\n $ ptl  : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ht   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ ui   : int  1 0 0 1 1 0 0 0 0 0 ...\n $ ftv  : int  0 3 1 2 0 0 1 1 1 0 ...\n $ bwt  : int  2523 2551 2557 2594 2600 2622 2637 2637 2663 2665 ..."
  },
  {
    "objectID": "practice_probs/practice-midterm2-coding.html",
    "href": "practice_probs/practice-midterm2-coding.html",
    "title": "Extra practice problems: coding",
    "section": "",
    "text": "Almost all of following problems should require to use R in some way.\n\nAn apple farmer has historically lost an average of 4% of his trees each year. He believes that he has been losing more trees lately. In a random sample of 200 trees, 12 have died.\n\nUsing an appropriate method, test the farmer’s claim at the 0.01 level.\nUsing the data from his sample, obtain a 90% confidence interval for the farmer’s loss rate of trees.\n\n\n\n\n\n\n\n\n\nRecall that the starbucks data from openintro has several different types of food items. We’d like to know if the average calories in hot breakfast items are different from the average calories of sandwich items. Answer this two ways: 1) using an appropriate hypothesis test and 2) using an appropriate confidence interval. Try to do one via mathematical model (if appropriate) and another via simulation.\nWorking with the starbucks data again: Using an appropriate method, obtain a 95% confidence interval for the mean calorie per carbohydrate of bakery type items.\nTake a look at the Help file of the satgpa data from openintro. Fit a linear model where we use math SAT percentiles to estimate the first year college GPA. Check if your model is appropriate. If so, is a student’s performance on the math section of the SAT predictive of their first-year GPA?\nYawning. Take a look at the Help file for the yawn data from openintro. Write down null and alternative hypotheses (in words or in notation is) that correspond to the research question implied in the Help file Description. Make a plot of the data that would be appropriate/helpful exploratory analysis for the researchers. Then using simulation, test your hypotheses at the 0.05 significance level. Make a conclusion in context. Optional but good practice: describe in words how you would implement the simulation using props/cards."
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#housekeeping",
    "href": "slides/slides-28-mlr-intro.html#housekeeping",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Housekeeping",
    "text": "Housekeeping\n\n\n\n\nStudy for midterm!"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#interpreting-coefficients",
    "href": "slides/slides-28-mlr-intro.html#interpreting-coefficients",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Interpreting coefficients",
    "text": "Interpreting coefficients\n\\[\n\\widehat{\\text{birth_wt}} = 3102.72 + -383.03 \\text{raceBlack} + -297.44 \\text{raceOther}\n\\]\n\n\\(\\widehat{\\text{birth_wt}} = 3102.72 -383.03 \\times \\color{orange}{0} -297.44 \\times \\color{orange}{0}\\)\nThe estimated birth weight of babies whose mothers are White is 3102.72 grams\n\nMore generally: \\(b_{0}\\) is the estimated value of the response variable for the base level\n\n\nWhat is the interpretation of \\(b_{1}\\) = -383.03? Of \\(b_{2}\\) = -297.44?\n\n\nBabies whose mothers are Black have an estimated birth weight about 383.03 grams less than babies whose mothers are White\nBabies whose mothers are race “Other” (i.e. not Black or White) have an estimated birth weight about 297.44 grams less than babies whose mothers are White"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#general-interpretation",
    "href": "slides/slides-28-mlr-intro.html#general-interpretation",
    "title": "Introduction to Multiple Linear Regression",
    "section": "General interpretation",
    "text": "General interpretation\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n3102.7188\n72.92298\n42.547890\n0.0000000\n\n\nraceblack\n-383.0264\n157.96382\n-2.424773\n0.0162741\n\n\nraceother\n-297.4352\n113.74198\n-2.614999\n0.0096546\n\n\n\n\n\n\nWhen fitting a regression model with a categorical variable with \\(k > 2\\) levels, the software will always provide a coefficient for \\(k-1\\) of the levels\n\n\nThe base level does not receive a coefficient\n\nInterpretation of the coefficient associated with a non-base level is the expected change in the response relative to the base level\n\nNote: the fitted model has more than one “slope” coefficient, but the race variable is still a single explanatory variable\nWhat happens if we explicitly want to include more than one explanatory variable?"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#birthweight-data-cont.",
    "href": "slides/slides-28-mlr-intro.html#birthweight-data-cont.",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Birthweight data (cont.)",
    "text": "Birthweight data (cont.)\nSuppose we would also like to include the mother’s age (age) and weight at last period (lwt) into the model:\n\\[\\text{birth_wt} = \\beta_{0} + \\beta_{1} \\text{raceBlack} + \\beta_{2} \\text{raceOther} + \\beta_{3} \\text{age} + \\beta_4 \\text{lwt} + \\epsilon\\]\n\nJust as in the case of SLR, the estimates of \\(\\beta_{0},\\ldots, \\beta_{4}\\) parameters are chosen via the squared deviation criterion"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#multiple-regression-in-r",
    "href": "slides/slides-28-mlr-intro.html#multiple-regression-in-r",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Multiple regression in R",
    "text": "Multiple regression in R\nVery easy to code:\n\nbwt_mlr <- lm(bwt ~ race + age + lwt, data = birthwt2)\n\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    2461.147482 \n    314.722327 \n    7.8200600 \n    0.0000000 \n  \n  \n    raceblack \n    -447.614691 \n    161.369310 \n    -2.7738527 \n    0.0061108 \n  \n  \n    raceother \n    -239.356515 \n    115.188920 \n    -2.0779474 \n    0.0391022 \n  \n  \n    age \n    1.298831 \n    10.107701 \n    0.1284991 \n    0.8978943 \n  \n  \n    lwt \n    4.619545 \n    1.787729 \n    2.5840294 \n    0.0105407 \n  \n\n\n\n\n\n\nSimply identify the estimated coefficients from the output to obtain fitted model\n\n\n\\[\n\\begin{align*}\n\\widehat{\\text{birth_wt}} &= 2461.15  -447.61 \\text{raceBlack} -239.36 \\text{raceOther} +  1.3 \\text{age}  \\\\\n& \\quad + 4.62 \\text{lwt}\n\\end{align*}\n\\]\n\n\nNote that the number of explanatory variables need not equal the number of parameters in the model!"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#interpretation",
    "href": "slides/slides-28-mlr-intro.html#interpretation",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\nWhen we have more than one predictor variable, interpretation of the coefficients requires a bit of care\n\nMultiple moving parts\n\nInterpretation of a particular coefficient \\(b_{m}\\) relies on “holding the other variables fixed/constant” (assuming the model is appropriate)\n\n\n\\[\n\\begin{align*}\n\\widehat{\\text{birth_wt}} &= 2461.15  -447.61 \\text{raceBlack} -239.36 \\text{raceOther} + \\color{orange}{1.3} \\text{age}  \\\\\n& \\quad + 4.62 \\text{lwt}\n\\end{align*}\n\\]\n\n\nFor every one year older the mother is, the baby’s birth weight is expected to increase by \\(\\color{orange}{1.3}\\) grams, holding all other variables constant\n\nInterpret the coefficient associated with the mother’s weight (lwt)"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#interpretation-cont.",
    "href": "slides/slides-28-mlr-intro.html#interpretation-cont.",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Interpretation (cont.)",
    "text": "Interpretation (cont.)\n\\[\n\\begin{align*}\n\\widehat{\\text{birth_wt}} &= 2461.15  -447.61 \\text{raceBlack} -239.36 \\text{raceOther} +  1.3 \\text{age}  \\\\\n& \\quad + \\color{orange}{4.62} \\text{lwt}\n\\end{align*}\n\\]\n\nFor every one pound heavier the mother’s weight at last period was, the baby’s birth weight is expected to increase by \\(\\color{orange}{4.62}\\) grams, holding all other variables constant"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#more-isnt-always-better",
    "href": "slides/slides-28-mlr-intro.html#more-isnt-always-better",
    "title": "Introduction to Multiple Linear Regression",
    "section": "More isn’t always better",
    "text": "More isn’t always better\n\nYou might be tempted to throw in all available predictors into your model! Don’t fall into temptation!\nQuality over quantity\nFor SLR, we used the coefficient of determination \\(R^2\\) to assess how good the model was\n\n\\(R^2\\) is less helpful when there are many variables\nWhy? The \\(R^2\\) will never decrease (and will almost always increase) when we include an additional predictor"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#adjusted-r2",
    "href": "slides/slides-28-mlr-intro.html#adjusted-r2",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nFor multiple linear regression, we use the adjusted \\(R^2\\) to assess the quality of model fit\n\n“Adjusted” for the presence of additional predictors\nTake STAT 211 to learn the formula and intuition behind it!\n\nAdjusted \\(R^2\\) is always less than \\(R^2\\)"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#adjusted-r2-cont.",
    "href": "slides/slides-28-mlr-intro.html#adjusted-r2-cont.",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Adjusted \\(R^2\\) (cont.)",
    "text": "Adjusted \\(R^2\\) (cont.)\n\n\n\nsummary(bwt_mlr)\n\n\nCall:\nlm(formula = bwt ~ race + age + lwt, data = birthwt2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2103.50  -429.68    41.74   486.10  1902.20 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 2461.147    314.722   7.820 3.97e-13 ***\nraceblack   -447.615    161.369  -2.774  0.00611 ** \nraceother   -239.357    115.189  -2.078  0.03910 *  \nage            1.299     10.108   0.128  0.89789    \nlwt            4.620      1.788   2.584  0.01054 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 704.9 on 184 degrees of freedom\nMultiple R-squared:  0.08536,   Adjusted R-squared:  0.06548 \nF-statistic: 4.293 on 4 and 184 DF,  p-value: 0.00241\n\n\n\n\nglance(bwt_mlr)\n\n\n\n\n\n \n  \n    r.squared \n    adj.r.squared \n    sigma \n    statistic \n    p.value \n    df \n    logLik \n    AIC \n    BIC \n    deviance \n    df.residual \n    nobs \n  \n \n\n  \n    0.0854 \n    0.0655 \n    704.9368 \n    4.293 \n    0.0024 \n    4 \n    -1505.128 \n    3022.256 \n    3041.707 \n    91436202 \n    184 \n    189"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#simpler-model",
    "href": "slides/slides-28-mlr-intro.html#simpler-model",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Simpler model",
    "text": "Simpler model\nLet’s see the model that does not include mother’s age in the model:\n\nbwt_mlr_no_age <- lm(bwt ~ race + lwt, data = birthwt2)\ntidy(bwt_mlr_no_age)\n\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    2486.9039 \n    241.9933 \n    10.2767 \n    0.0000 \n  \n  \n    raceblack \n    -451.8381 \n    157.5662 \n    -2.8676 \n    0.0046 \n  \n  \n    raceother \n    -241.3008 \n    113.8869 \n    -2.1188 \n    0.0354 \n  \n  \n    lwt \n    4.6634 \n    1.7501 \n    2.6646 \n    0.0084 \n  \n\n\n\n\n\n\n\nWrite out the fitted model. Interpret the intercept and the coefficient for lwt in context"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#simpler-model-cont.",
    "href": "slides/slides-28-mlr-intro.html#simpler-model-cont.",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Simpler model (cont.)",
    "text": "Simpler model (cont.)\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    2486.9039 \n    241.9933 \n    10.2767 \n    0.0000 \n  \n  \n    raceblack \n    -451.8381 \n    157.5662 \n    -2.8676 \n    0.0046 \n  \n  \n    raceother \n    -241.3008 \n    113.8869 \n    -2.1188 \n    0.0354 \n  \n  \n    lwt \n    4.6634 \n    1.7501 \n    2.6646 \n    0.0084 \n  \n\n\n\n\n\n\nIntercept: the birth weight of babies whose mothers are White and weigh 0 lbs have an estimated birth weight of 2486.9 grams\nCoefficient for lwt: for every one pound increase in the mother’s weight at last period, the birth weight of the baby is expected to increase by 4.66 grams, holding all other variables (i.e. race) constant"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#simpler-model-model-assessment",
    "href": "slides/slides-28-mlr-intro.html#simpler-model-model-assessment",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Simpler model: model assessment",
    "text": "Simpler model: model assessment\nLet’s compare the two models: one with age and one without age\n\n\n\ntidy(bwt_mlr)\n\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    2461.147482 \n    314.722327 \n    7.8200600 \n    0.0000000 \n  \n  \n    raceblack \n    -447.614691 \n    161.369310 \n    -2.7738527 \n    0.0061108 \n  \n  \n    raceother \n    -239.356515 \n    115.188920 \n    -2.0779474 \n    0.0391022 \n  \n  \n    age \n    1.298831 \n    10.107701 \n    0.1284991 \n    0.8978943 \n  \n  \n    lwt \n    4.619545 \n    1.787729 \n    2.5840294 \n    0.0105407 \n  \n\n\n\n\n\n\nglance(bwt_mlr)\n\n\n\n\n\n \n  \n    r.squared \n    adj.r.squared \n    sigma \n    statistic \n    p.value \n    df \n    logLik \n    AIC \n    BIC \n    deviance \n    df.residual \n    nobs \n  \n \n\n  \n    0.0853604 \n    0.065477 \n    704.9368 \n    4.293036 \n    0.0024104 \n    4 \n    -1505.128 \n    3022.256 \n    3041.707 \n    91436202 \n    184 \n    189 \n  \n\n\n\n\n\n\n\ntidy(bwt_mlr_no_age)\n\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    2486.9039 \n    241.9933 \n    10.2767 \n    0.0000 \n  \n  \n    raceblack \n    -451.8381 \n    157.5662 \n    -2.8676 \n    0.0046 \n  \n  \n    raceother \n    -241.3008 \n    113.8869 \n    -2.1188 \n    0.0354 \n  \n  \n    lwt \n    4.6634 \n    1.7501 \n    2.6646 \n    0.0084 \n  \n\n\n\n\n\n\nglance(bwt_mlr_no_age)\n\n\n\n\n\n \n  \n    r.squared \n    adj.r.squared \n    sigma \n    statistic \n    p.value \n    df \n    logLik \n    AIC \n    BIC \n    deviance \n    df.residual \n    nobs \n  \n \n\n  \n    0.0853 \n    0.0704 \n    703.0605 \n    5.7491 \n    9e-04 \n    3 \n    -1505.137 \n    3020.273 \n    3036.482 \n    91444408 \n    185 \n    189"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#comparing-models",
    "href": "slides/slides-28-mlr-intro.html#comparing-models",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Comparing models",
    "text": "Comparing models\nLet’s compare the model that includes age to the model without age:\n\n\n\ntidy(bwt_mlr) |>\n  select(term, estimate, p.value)\n\n\n\n\n\n \n  \n    term \n    estimate \n    p.value \n  \n \n\n  \n    (Intercept) \n    2461.1475 \n    0.0000 \n  \n  \n    raceblack \n    -447.6147 \n    0.0061 \n  \n  \n    raceother \n    -239.3565 \n    0.0391 \n  \n  \n    age \n    1.2988 \n    0.8979 \n  \n  \n    lwt \n    4.6195 \n    0.0105 \n  \n\n\n\n\n\n\nglance(bwt_mlr)\n\n\n\n\n\n \n  \n    r.squared \n    adj.r.squared \n    sigma \n    statistic \n    p.value \n    df \n    logLik \n    AIC \n    BIC \n    deviance \n    df.residual \n    nobs \n  \n \n\n  \n    0.0854 \n    0.0655 \n    704.9368 \n    4.293 \n    0.0024 \n    4 \n    -1505.128 \n    3022.256 \n    3041.707 \n    91436202 \n    184 \n    189 \n  \n\n\n\n\n\n\n\ntidy(bwt_mlr_no_age) |>\n  select(term, estimate, p.value)\n\n\n\n\n\n \n  \n    term \n    estimate \n    p.value \n  \n \n\n  \n    (Intercept) \n    2486.9039 \n    0.0000 \n  \n  \n    raceblack \n    -451.8381 \n    0.0046 \n  \n  \n    raceother \n    -241.3008 \n    0.0354 \n  \n  \n    lwt \n    4.6634 \n    0.0084 \n  \n\n\n\n\n\n\nglance(bwt_mlr_no_age)\n\n\n\n\n\n \n  \n    r.squared \n    adj.r.squared \n    sigma \n    statistic \n    p.value \n    df \n    logLik \n    AIC \n    BIC \n    deviance \n    df.residual \n    nobs \n  \n \n\n  \n    0.0853 \n    0.0704 \n    703.0605 \n    5.7491 \n    9e-04 \n    3 \n    -1505.137 \n    3020.273 \n    3036.482 \n    91444408 \n    185 \n    189 \n  \n\n\n\n\n\n\n\n\nWhat do you notice about the estimated coefficients, \\(R^2\\), and adjusted \\(R^2\\)?"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#fit-model",
    "href": "slides/slides-28-mlr-intro.html#fit-model",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Fit model",
    "text": "Fit model\n\nbwt_lm <- lm(bwt ~ race, data = birthwt2)\ntidy(bwt_lm)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n3102.7188\n72.92298\n42.547890\n0.0000000\n\n\nraceblack\n-383.0264\n157.96382\n-2.424773\n0.0162741\n\n\nraceother\n-297.4352\n113.74198\n-2.614999\n0.0096546\n\n\n\n\n\n\nFitted model:\n\\[\n\\widehat{\\text{birth_wt}} = 3102.72  -383.03 \\text{raceBlack}  -297.44 \\text{raceOther}\n\\]\n\\[\\text{raceBlack} = \\begin{cases}1 & \\text{if race = Black}  \\\\ 0 & \\text{otherwise} \\end{cases} \\qquad \\text{raceOther} = \\begin{cases}1 & \\text{if race = Other}  \\\\ 0 & \\text{otherwise} \\end{cases}\\]\n\n\n\nEstimate the birth weight for babies whose mothers are White"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#multiple-linear-regression-1",
    "href": "slides/slides-28-mlr-intro.html#multiple-linear-regression-1",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nWe have seen simple linear regression, where we had one explanatory variable\nExtend to include multiple explanatory variables\n\nSeems natural: usually several factors affect behavior of phenomena\n\nMultiple linear regression takes the form: \\[y = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} + \\ldots + \\beta_{p} x_{p} + \\epsilon\\]\n\nNow there are \\(p\\) different explanatory variables \\(x_{1},\\ldots, x_{p}\\) per observation\nStill one response \\(y\\) and error \\(\\epsilon\\) per observation\n\nRepresents a holistic approach for modeling all of the variables simultaneously"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#conditions",
    "href": "slides/slides-28-mlr-intro.html#conditions",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Conditions",
    "text": "Conditions\nWe still need LINE to hold\n\nLinearity: harder to assess now that multiple predictors are involved. Good idea to make several scatter plots\nIndependence: same as before\nNearly normal residuals: same as before\nEqual variance: residual plot has fitted values on the x-axis, instead of an explanatory variable"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#hypothesis-testing-in-mlr",
    "href": "slides/slides-28-mlr-intro.html#hypothesis-testing-in-mlr",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Hypothesis testing in MLR",
    "text": "Hypothesis testing in MLR\n\nIn MLR, we are interested in the effect of a variable \\(m\\) on the response \\(y\\).\n\nNeed to account for presence of other predictors in the model\n\n\\(H_{0}: \\beta_m = 0\\), given other predictors in the model\n\\(H_{A}: \\beta_m \\neq 0\\), given other predictors in the model (or \\(>, <\\))\nWe can write down one null hypothesis for each coefficient in the model"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#hypothesis-tests-from-lm",
    "href": "slides/slides-28-mlr-intro.html#hypothesis-tests-from-lm",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Hypothesis tests from lm()",
    "text": "Hypothesis tests from lm()\nReturning to the larger model:\n\\[\\text{birth_wt} = \\beta_{0} + \\beta_{1} \\text{raceBlack} + \\beta_{2} \\text{raceOther} + \\beta_{3} \\text{age} + \\beta_4 \\text{lwt} + \\epsilon\\]\n\nWe can test the following null hypotheses (no need to write down):\n\n\\(H_{0}: \\beta_{1} = 0\\), given age and lwt are included in the model\n\\(H_{0}: \\beta_{2} = 0\\), given age and lwt are included in the model\n\\(H_{0}: \\beta_{3} = 0\\), given race and lwt are included in the model\n\\(H_{0}: \\beta_{4} = 0\\), given race and age are included in the model"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#hypothesis-tests-from-lm-1",
    "href": "slides/slides-28-mlr-intro.html#hypothesis-tests-from-lm-1",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Hypothesis tests from lm()",
    "text": "Hypothesis tests from lm()\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    2461.1475 \n    314.7223 \n    7.8201 \n    0.0000 \n  \n  \n    raceblack \n    -447.6147 \n    161.3693 \n    -2.7739 \n    0.0061 \n  \n  \n    raceother \n    -239.3565 \n    115.1889 \n    -2.0779 \n    0.0391 \n  \n  \n    age \n    1.2988 \n    10.1077 \n    0.1285 \n    0.8979 \n  \n  \n    lwt \n    4.6195 \n    1.7877 \n    2.5840 \n    0.0105 \n  \n\n\n\n\n\n\nOutput from lm() provides:\n\nTest statistic, which follows \\(t_{n-p}\\) where \\(p =\\) total number of unknown parameters (i.e. \\(\\beta\\) terms)\np-values for testing two-sided \\(H_{A}\\) provided\n\n\n\n\nBased on the model fit, which variables seem to be important predictors of birth weight of a baby? Why?"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#hypothesis-tests-from-lm-cont.",
    "href": "slides/slides-28-mlr-intro.html#hypothesis-tests-from-lm-cont.",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Hypothesis tests from lm() (cont.)",
    "text": "Hypothesis tests from lm() (cont.)\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    2461.1475 \n    314.7223 \n    7.8201 \n    0.0000 \n  \n  \n    raceblack \n    -447.6147 \n    161.3693 \n    -2.7739 \n    0.0061 \n  \n  \n    raceother \n    -239.3565 \n    115.1889 \n    -2.0779 \n    0.0391 \n  \n  \n    age \n    1.2988 \n    10.1077 \n    0.1285 \n    0.8979 \n  \n  \n    lwt \n    4.6195 \n    1.7877 \n    2.5840 \n    0.0105 \n  \n\n\n\n\n\n\nlwt does seem to be an important predictor for birth weight, despite inclusion of race and age in the model\n\nLow p-value suggests it would be extremely unlikely to see data that produce \\(b_{4} = 4.62\\) if the true relationship between lwtand bwt was non-existent (i.e., if \\(\\beta_{4} = 0\\)) and the model also included age and race\n\nrace does seem to be an important predictor, despite inclusion of age and lwt\nage does not seem to be an important predictor after including race and lwt"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#remarks",
    "href": "slides/slides-28-mlr-intro.html#remarks",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Remarks",
    "text": "Remarks\n\nWe have only scratched the surface of MLR\nThings to consider:\n\nMulticollinearity (when the predictor variables are correlated with each other)\nModel selection\nMore than one categorical variable\nInteraction effects"
  },
  {
    "objectID": "slides/slides-28-mlr-intro.html#conditions-for-inference",
    "href": "slides/slides-28-mlr-intro.html#conditions-for-inference",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Conditions for inference",
    "text": "Conditions for inference\nWe still need LINE to hold\n\nLinearity: harder to assess now that multiple predictors are involved. Good idea to make several scatter plots\nIndependence: same as before\nNearly normal residuals: same as before\nEqual variance: residual plot has fitted values on the x-axis, instead of an explanatory variable"
  },
  {
    "objectID": "slides/worksheet-28-mlr-intro.html",
    "href": "slides/worksheet-28-mlr-intro.html",
    "title": "MLR Intro",
    "section": "",
    "text": "bwt_mlr <- lm(bwt ~ race + age + lwt, data = birthwt2)\n\n\n\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    2461.147482 \n    314.722327 \n    7.8200600 \n    0.0000000 \n  \n  \n    raceblack \n    -447.614691 \n    161.369310 \n    -2.7738527 \n    0.0061108 \n  \n  \n    raceother \n    -239.356515 \n    115.188920 \n    -2.0779474 \n    0.0391022 \n  \n  \n    age \n    1.298831 \n    10.107701 \n    0.1284991 \n    0.8978943 \n  \n  \n    lwt \n    4.619545 \n    1.787729 \n    2.5840294 \n    0.0105407 \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.0853604\n0.065477\n704.9368\n4.293036\n0.0024104\n4\n-1505.128\n3022.256\n3041.707\n91436202\n184\n189\n\n\n\n\n\n\n\n\n\nCall:\nlm(formula = bwt ~ race + age + lwt, data = birthwt2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2103.50  -429.68    41.74   486.10  1902.20 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 2461.147    314.722   7.820 3.97e-13 ***\nraceblack   -447.615    161.369  -2.774  0.00611 ** \nraceother   -239.357    115.189  -2.078  0.03910 *  \nage            1.299     10.108   0.128  0.89789    \nlwt            4.620      1.788   2.584  0.01054 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 704.9 on 184 degrees of freedom\nMultiple R-squared:  0.08536,   Adjusted R-squared:  0.06548 \nF-statistic: 4.293 on 4 and 184 DF,  p-value: 0.00241"
  },
  {
    "objectID": "project/project_description.html#report",
    "href": "project/project_description.html#report",
    "title": "Final project",
    "section": "Report",
    "text": "Report\nKnitting more beautiful tables.\nEach group will write a final report created in an .Rmd file. Ideally, the report will be submitted as a PDF (we will discuss how to do this as a class). The code in your report should be as reproducible as possible (e.g. not hard coding specific values, setting a seed when appropriate).\nAdditionally, only the output/execution from the code relevant for the report should be visible to the reader, not the code itself. This means that as the viewer, I should not see any code or extraneous output. The former is achieved by setting the chunk header option echo = F. That being said, if there is some code that you believe is useful/important for the reader to see, you are welcome to make that code visible by setting echo = T.\nThe report does not have a minimum length, but should contain each of the following sections. The report should also have an informative, academically appropriate title along with each group member’s name.\n\nIntroduction\n\nThe introduction should introduce your research questions and the population(s) of interest. Your report should have at least one research question per team member. In the introduction, you should briefly motivate the research questions (i.e. explain why they were of interest to your group)!\n\nIt could be helpful to identify/label your research questions as “Question 1”, “Question 2”, etc. for easier reference moving forward.\n\n\n\n\nData collection\nThis section should describe your data and the sampling process: how the data were collected and a summary description of the variables you collected.\n\n\nMethods\nFor each research question, you should:\n\nIdentify the specific variables used to address the research question. Also describe if you had to create/mutate new variables or filter out/subset your analysis to focus on certain observations for this research question.\nInclude at least one visualization or table of summary statistics that will be useful/informative for answering the research question. Briefly interpret the visualization/summary table.\n\nIt is okay if if one visualization is complex enough to address multiple research questions!\n\nDescribe and justify the statistical method(s) that you will use to answer your research questions. Part of the “justification” component includes conditions being met, if applicable. For linear regression, you should only verify the “L” and the “I” at this point.\n\nBe specific about different groups/populations, direction of differences, explanatory vs response variable, etc.\nIf you are conducting a hypothesis test, don’t forget to define your hypotheses and set the significance level. If you are creating a confidence interval, be sure to specify the level of confidence.\n\n\n\n\nResults\n\nUsing appropriate code, conduct the statistical methods you described in the Methods section for each research question. Then interpret your results in context (i.e. use your interval/p-value/model to actually answer the question).\n\nIf you do linear regression, you should check the “N” and “E” conditions here.\n\nThe goal is not to do an exhaustive data analysis (i.e., do not calculate every statistic and procedure you have learned for every variable), but rather let me know that you are proficient at asking meaningful questions and answering them with results of data analysis, that you are proficient in using R, and that you are proficient at interpreting and presenting the results.\n\n\n\nDiscussion\nThis section is a conclusion and discussion. This will require:\n\nA summary of what you have learned about your research questions along with statistical arguments supporting your conclusions.\nA critique your own methods and provide suggestions for improving your analysis. Issues pertaining to the reliability and validity of your data/data collection and appropriateness of the statistical analysis should also be discussed here.\nA paragraph on what your group would do differently if you were able to start over with the project or what you would do next if you were going to continue work on the project should also be included."
  },
  {
    "objectID": "project/project_description.html#presentation",
    "href": "project/project_description.html#presentation",
    "title": "Final project",
    "section": "Presentation",
    "text": "Presentation\nEach group will present their work during the exam period. Presentation duration lengths (before questions) are:\n\nGroups of two: 6-7 minutes\nGroups of three: 9-10 minutes\nGroups of four: 12-13 minutes\n\nPart of your presentation grade will be sticking to the presentation duration. Your presentation will be followed by a few minutes for questions from the audience.\nYour presentation should be accompanied by slides that summarize and showcase your project. Introduce your research questions and data/data collection, showcase visualizations, and provide some conclusions. These slides should serve as a brief visual accompaniment and will be graded for content and quality.\nThe slides are due to Canvas one hour before our final exam/presentation period.\n\nYou do not need to discuss all of your research questions if you don’t want to! You are not expected to have finished all your analyses by the time you present, but you should have some results (whether preliminary or finalized).\nIt’s most important that your audience understands the research questions you present, how you obtained your data, and how you plan to answer the research questions.\nPictures and summary tables are always preferred over sentences!\nA general recommendation is to have one slide per minute of the presentation.\nAll presenters should speak roughly an equal amount of time.\nMake sure the font is large enough. Also, less is more!\n\nPart of the presentation component is participation. You should be an engaged audience member. You will be asked to give feedback to a few groups and you are expected to ask questions.\n\nPresentation order\nSection AZ:\n\nDanny, Mason, Jason\nCharlie, Tanmay, Gideon\nLily, Emma\nHino, Zhanning, Thehan\nMarli, Parker\nTessa, Julia\nHarper, Analiese\nRichard, Lucas, Trevor\n\nSection BY\n\nAidan, Teddy\nSage, Ryan, Izzy\nSebastian, Jackson\nAthena, Remi, Krysta\nGigi, Kaia, Keila\nAndrew, Wills, Ye, Soe\nGabe, Ronin\nMadelyn, Phoebe, Ruth\nAdam, Grace, Ivy"
  },
  {
    "objectID": "project/project_description.html#grading",
    "href": "project/project_description.html#grading",
    "title": "Final project",
    "section": "Grading",
    "text": "Grading\n\nGeneral grading structure\nGrading of the project will take into account the following:\n\nContent - What is the quality of research and/or policy question and relevancy of data to those questions?\nCorrectness - Are statistical procedures carried out and explained correctly?\nWriting and Presentation - What is the quality of the statistical presentation, writing, and explanations?\nCreativity and Critical Thought - Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\nContribution to group – Did you contribute meaningfully to all aspects of the project?\n\nA general breakdown of scoring for the final report and presentation is as follows:\n\n90%-100%: Outstanding effort. Student understands how to apply all statistical concepts, can put the results into a cogent argument, can identify weaknesses in the argument, and can clearly communicate the results to others.\n80%-89%: Good effort. Student understands most of the concepts, puts together an adequate argument, identifies some weaknesses of their argument, and communicates most results clearly to others.\n70%-79%: Passing effort. Student has misunderstanding of concepts in several areas, has some trouble putting results together in a cogent argument, and communication of results is sometimes unclear.\n60%-69%: Struggling effort. Student is making some effort, but has misunderstanding of many concepts and is unable to put together a cogent argument. Communication of results is unclear.\nBelow 60%: Student is not making a sufficient effort.\n\n\n\n\n\n\n\n\n\nComponent\nSection\nPoints\n\n\n\n\nProject proposal\n\n15 points\n\n\nInitial data set submission\n\n1 point\n\n\nRough draft\nPick two sections you’d like Prof. Tang to give feedback on!\n5 points\n\n\nPresentation\nPresentation style: well-practiced, equal distribution of speaking time, within time limit\n8 points\n\n\n\nSlides: content, quality\n5 points\n\n\n\nParticipation\n2 points\n\n\nFinal report\nIntroduction\n5 points\n\n\n\nData collection description\n5 points\n\n\n\nMethods\n25 points\n\n\n\nResults\n20 points\n\n\n\nDiscussion\n10 points\n\n\n\nClean code + reproducibility\n3 points\n\n\n\nSubmission of both PDF and .Rmd report\n1 point\n\n\nReflection\n\n3 points\n\n\nMisc. items\nFinalized data set with accompanying dictionary (12/15)\n2 point\n\n\n\nContribution to group\n10 points\n\n\nTotal\n\n120 points"
  },
  {
    "objectID": "slides/slides-29-project.html#housekeeping",
    "href": "slides/slides-29-project.html#housekeeping",
    "title": "Final project",
    "section": "Housekeeping",
    "text": "Housekeeping\n\n\n\n\nPlease sit with your final project group this week!\nOffice hours + additional 1:1 meetings available"
  },
  {
    "objectID": "slides/slides-29-project.html#final-report",
    "href": "slides/slides-29-project.html#final-report",
    "title": "Final project",
    "section": "Final report",
    "text": "Final report\n\nCollaborative working styles:\n\nPhysically work together on the same .Rmd document\nWork on separate .Rmd documents and then share code with each other (e.g. via a Googledoc) to compile one master .Rmd\nA combination of the above two as the project comes together\n\nRecommendations:\n\nClear your environment before running new, shared code\nKnit often"
  },
  {
    "objectID": "slides/slides-29-project.html#r-chunk-headers",
    "href": "slides/slides-29-project.html#r-chunk-headers",
    "title": "Final project",
    "section": "R chunk headers",
    "text": "R chunk headers\n\n\nIn .Rmd documents, we can control the behavior of specific code chunks by changing the settings in the chunk header:\n\n\n\n\n\n\n\nFor example, setting the option eval = F in the code chunk header means “do not evaluate the code in this chunk”.\n\n\n\n\nCode:\n\n\n\n\nOutput:"
  },
  {
    "objectID": "slides/slides-29-project.html#hiding-code",
    "href": "slides/slides-29-project.html#hiding-code",
    "title": "Final project",
    "section": "Hiding code",
    "text": "Hiding code\nWe can hide code from the viewer but still have it execute using the echo setting.\n\n\nCode:\n\n\n\nOutput:\n\n\n\n\n\n\n\nCode:\n\n\n\n\nOutput:"
  },
  {
    "objectID": "slides/slides-29-project.html#final-report-requirements",
    "href": "slides/slides-29-project.html#final-report-requirements",
    "title": "Final project",
    "section": "Final report: Requirements",
    "text": "Final report: Requirements\nReview the Report components on the Project description page."
  },
  {
    "objectID": "slides/slides-29-project.html#working-methods",
    "href": "slides/slides-29-project.html#working-methods",
    "title": "Final project",
    "section": "Working methods",
    "text": "Working methods\n\nCollaborative working styles:\n\nPhysically work together on the same .Rmd document\nWork on separate .Rmd documents and then share code with each other (e.g. via a Googledoc) to compile one master .Rmd\nA combination of the above two as the project comes together\n\nRecommendations:\n\nClear your environment before running new, shared code\nKnit often"
  },
  {
    "objectID": "slides/slides-29-project.html#rmd-template",
    "href": "slides/slides-29-project.html#rmd-template",
    "title": "Final project",
    "section": ".Rmd template",
    "text": ".Rmd template\nI have provided you with a .Rmd template for the final project. Feel free to add to/modify the template as your group sees fit!\n\nPlease ensure your .Rmd is in your STAT 201 folder\nThen, have a copy of your working dataset as a .csv file located in the same STAT 201 folder"
  },
  {
    "objectID": "slides/slides-29-project.html#data-file",
    "href": "slides/slides-29-project.html#data-file",
    "title": "Final project",
    "section": "Data file",
    "text": "Data file\n\nIn your group, everyone should have the same file name for the data.\nNow modify the second code chunk in the template to read in your data:\n\n\n\nNote: name the variable whatever you like (i.e. not need to use my_data, so long as your group is consistent.\n\n\nIf you prefer to use a .xlsx file, you will need to load the readxl package at the top of the document (may need to install first) and then modify the code to\n\n\n\nmy_data <- read_xlsx(\"data_file_name.xlsx\")"
  },
  {
    "objectID": "slides/slides-29-project.html#cleaning-your-data",
    "href": "slides/slides-29-project.html#cleaning-your-data",
    "title": "Final project",
    "section": "Cleaning your data",
    "text": "Cleaning your data\n\nYou might need to clean your data before working with it. This is probably most easily achieved within GoogleSheets or Excel rather than in R.\n\nE.g. a data entry is “6.4 hours” instead of 6.4, or there is inconsistent capitalization\n\nOnce/if your group has cleaned the data, you should replace the old data file in your STAT 201 folder with the cleaned version."
  },
  {
    "objectID": "project/final_project_template.html#introduction",
    "href": "project/final_project_template.html#introduction",
    "title": "STAT 201: Final Project Report Template",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "project/final_project_template.html#data-collection",
    "href": "project/final_project_template.html#data-collection",
    "title": "STAT 201: Final Project Report Template",
    "section": "Data Collection",
    "text": "Data Collection"
  },
  {
    "objectID": "project/final_project_template.html#methods",
    "href": "project/final_project_template.html#methods",
    "title": "STAT 201: Final Project Report Template",
    "section": "Methods",
    "text": "Methods"
  },
  {
    "objectID": "project/final_project_template.html#results",
    "href": "project/final_project_template.html#results",
    "title": "STAT 201: Final Project Report Template",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "project/final_project_template.html#discussion",
    "href": "project/final_project_template.html#discussion",
    "title": "STAT 201: Final Project Report Template",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "project/project_description.html#misc.-submission-items",
    "href": "project/project_description.html#misc.-submission-items",
    "title": "Final project",
    "section": "Misc. submission items",
    "text": "Misc. submission items\n\nFinalized data set + dictionary\nYou will be asked to submit your finalized, clean dataset. This may take the form of a .csv file or a link to a GoogleSheet. Please give each variable an informative one-word name. Alongside the submission, please include a separate file for the data dictionary: a bulleted list or table containing each variable name, a brief description of the variable, and the variable type. Include units if appropriate. If the variable is categorical, please provide all the levels as they appear in the data.\nThe purpose of the data dictionary is to enable future use of your data.\n\n\nReflection\nYou will be asked to write a brief reflection about the final project. There will be a few questions/prompts pertaining to your specific contribution, learning, and experience, along with an opportunity to comment on the group dynamics and working methods. The prompts/questions will be provided on the associated Canvas assignment."
  },
  {
    "objectID": "live_code/knitr.html",
    "href": "live_code/knitr.html",
    "title": "Knitting beautiful tables",
    "section": "",
    "text": "You may have noticed that the data frames are not the most beautiful in the knitted output. With the help of the knitr package, we can make the tables knit beautifully. When you knit for the first time, it may take a minute or two as more things get installed. Don’t panic! It will knit!\n\nAt the top of your document, load in the knitr package: library(knitr).\n\nYou may need to install this package first. Either do this in the “Packages” tab on the right, or in the Console by typing install.packages(\"knitr\").\n\nThen for any data frame/table you’d like to knit beautifully, just pipe directly to the kable() function. For example:\n\n\nlibrary(knitr)\nmod <- lm(volume ~ diam, data = cherry)\n## equivalently: kable(tidy(mod))\ntidy(mod) |>\n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-36.943459\n3.365145\n-10.97827\n0\n\n\ndiam\n5.065856\n0.247377\n20.47829\n0\n\n\n\n\n\nCompare this to when we don’t use kable():\n\ntidy(mod)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -36.9      3.37      -11.0 7.62e-12\n2 diam            5.07     0.247      20.5 8.64e-19\n\n\nYou can control the number of digits shown in the table:\n\ntidy(mod) |>\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-36.943\n3.365\n-10.978\n0\n\n\ndiam\n5.066\n0.247\n20.478\n0\n\n\n\n\n\nLook at the Help file for more customization!"
  }
]