{
  "hash": "e9bffffde66becbc37f7fd2373b105ca",
  "result": {
    "markdown": "---\ntitle: \"Introduction to Simple Linear Regression\"\ndate: \"November 6, 2024\"\ntitle-slide-attributes:\n    data-background-image: \"figs/bikeshare-plots.png\"\n    data-background-size: contain\n    data-background-opacity: \"0.2\"\nformat: \n  revealjs:\n    theme: custom.scss\n    transition: none\n    incremental: true\n    scrollable: true\neditor: visual\neditor_options: \n  chunk_output_type: console\ndraft: false\n---\n\n\n## Housekeeping\n\n\n::: {.cell}\n\n:::\n\n\n-   Homework 7 due tonight!\n\n-   Last problem set is assigned today\n\n# Linear regression\n\nCrash course; take STAT 211 for more depth!\n\n## Fitting a line to data\n\n-   Hopefully we are all familiar with the equation of a line: $y = mx + b$\n\n    -   Intercept $b$ and slope $m$ determine specific line\n\n    -   This function is *deterministic*: as long as we know $x$, we know value of $y$ exactly\n\n-   **Linear regression**: statistical method where the relationship between variable $x$ and variable $y$ is modeled as a **line + error:**\n\n::: fragment\n$$\ny = \\underbrace{\\beta_{0} + \\beta_{1} x}_{\\text{line}} + \\underbrace{\\epsilon}_{\\text{error}}\n$$\n:::\n\n## Linear regression model\n\n$$\ny = \\beta_{0} + \\beta_{1} x + \\epsilon\n$$\n\n-   We have two variables:\n\n    1.  $y$ is response variable\n    2.  $x$ is explanatory variable, also called the **predictor** variable\n\n-   $\\beta_{0}$ and $\\beta_{1}$ are the model **parameters** (intercept and slope)\n\n    -   Estimated using the data, with point estimates $b_{0}$ and $b_{1}$\n\n-   $\\epsilon$ (epsilon) represents the **error**\n\n    -   Accounts for variability: we do not expect all data to fall perfectly on the line!\n\n    -   Sometimes we drop the $\\epsilon$ term for convenience\n\n## Linear relationship\n\nSuppose we have the following data:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides-23-slr-intro_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n-   Observations won't fall exactly on a line, but do fall around a straight line, so maybe a linear relationship makes sense!\n\n\n\n## Fitted values\n\nSuppose we have some specific estimates $b_0$ and $b_{1}$. We could fit the linear relationship using these values as:\n\n$$\n\\hat{y} = b_{0} + b_{1} x\n$$\n\n-   The hat on $y$ signifies that this is an estimate: the estimated/**fitted** value of $y$ given these specific values of $x$, $b_{0}$ and $b_{1}$\n\n    -   We observe $y$, but can obtain a corresponding estimate $\\hat{y}$\n\n-   Note that the fitted value is obtained *without* the error\n\n## Fitted values (cont.)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides-23-slr-intro_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n-   Every observed value has a corresponding fitted value; the above plot just shows three specific examples\n\n## Residual\n\n**Residuals** are the remaining variation in the data after fitting a model.\n\n$$\n\\text{data} = \\text{fit} + \\text{residual}\n$$\n\n-   For each observation $i$, we obtain residual $e_{i}$ via:\n\n$$y_{i} = \\hat{y}_{i} + e_{i} \\quad \\Rightarrow \\quad e_{i} = \\hat{y}_{i} - y_{i}$$\n\n-   Residual = difference between observed and expected\n\n-   Since each observation has a fitted value, each observation has a residual\n\n    -   In the linear regression case, the residual is indicated by the vertical dashed line\n\n    -   ::: discuss\n        What is the ideal value for a residual?\n        :::\n\n## Residual plot\n\n-   Residuals are very helpful in evaluating how well a model fits a set of data\n\n-   **Residual plot** shows residuals plotted at the original $x$ locations, but now $y$-axis represents the residual\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides-23-slr-intro_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n|      x|      y|  y_hat| residual|\n|------:|------:|------:|--------:|\n| -2.991|  2.481| -0.130|   -2.611|\n| -1.005| -1.302|  0.691|    1.994|\n|  3.990|  3.929|  2.757|   -1.172|\n:::\n:::\n\n\n## Residual plot (cont.)\n\nResidual plots can be useful for identifying characteristics/patterns that remain in the data even after fitting a model.\n\n-   ::: {style=\"color: maroon\"}\n    Just because you fit a model to data, does not mean the model is a good fit!\n    :::\n\n::: fragment\n![](figs/23-residual-plot.png){fig-align=\"center\"}\n:::\n\n::: discuss\nCan you identify any patterns remaining in the residuals?\n:::\n\n## Describing linear relationships\n\nDifferent data may exhibit different strength of linear relationships:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides-23-slr-intro_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n-   Can we quantify the strength of the linear relationship?\n\n## Correlation\n\n-   **Correlation** is describes the strength of a *linear* relationship between two variables\n\n    -   Denoted by `R` or $\\rho$\n    -   Formula (not important): $R = \\frac{1}{n-1} \\sum_{i=1}^{n} \\left(\\frac{x_{i} - \\bar{x}}{s_x} \\right)\\left(\\frac{y_{i} - \\bar{y}}{s_y} \\right)$\n\n-   Always takes a value between -1 and 1\n\n    -   -1 = perfectly linear and negative\n\n    -   1 = perfectly linear and positive\n\n    -   0 = no linear relationship\n\n-   Nonlinear trends, even when strong, sometimes produce correlations that do not reflect the strength of the relationship\n\n![](figs/23-correlations.png){fig-align=\"center\" width=\"1610\"}\n\n# Least squares regression\n\nIn Algebra class, there exists a single (intercept, slope) pair because the $(x,y)$ points had no error; all points landed on the line.\n\nNow, we assume there is error. So how do we choose a single \"best\" $(b_{0}, b_{1})$ pair?\n\n## Different lines\n\nThe following display the same set of 50 observations.\n\n::: discuss\nWhich line would you say fits the data the best?\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](slides-23-slr-intro_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n-   There are infinitely many choices of $(b_{0}, b_{1})$ that could be used to create a line for the data\n\n-   We want the BEST choice (i.e. the one that gives us the \"line of best fit\")\n\n    -   How to define \"best\"?\n\n## Line of best fit\n\n-   One way to define a \"line of best fit\" is to choose the specific values of $(b_{0}, b_{1})$ that minimize the total residuals across all $n$ data points\n\n-   Results in following possible criterion:\n\n    -   **Least absolute criterion**: minimize sum of residual magnitudes:\n\n    $$\n    |\\epsilon_{1} | + |\\epsilon_{2}| + \\ldots + |\\epsilon_{n}|\n    $$\n\n    -   ::: {style=\"color: maroon\"}\n        **Least squares criterion**: minimize sum of squared residuals:\n        :::\n\n$$\ne_{1}^2 + e_{2}^2 +\\ldots + e_{n}^2\n$$\n\n-   The choice of $(b_{0}, b_{1})$ that satisfy least squares criterion yields the **least squares line**, and will be our criterion for \"best\"\n\n-   Option 1 (yellow line) is our least squares line, whereas pink line is the least absolute line\n",
    "supporting": [
      "slides-23-slr-intro_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}